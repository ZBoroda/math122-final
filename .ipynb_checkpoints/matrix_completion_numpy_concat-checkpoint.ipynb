{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful packages\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# load the browsing history data\n",
    "user_history = pd.read_csv(\"user_history.csv\")\n",
    "user_history_without_user_ID = user_history.drop(['USER ID'],axis=1)\n",
    "user_history_indexed = user_history.set_index('USER ID')\n",
    "user_ratings = pd.read_csv(\"user_ratings.csv\")\n",
    "\n",
    "# load the ratings data\n",
    "train = pd.read_csv(\"train_rating.csv\")\n",
    "test = pd.read_csv(\"test_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id_to_ind = dict(zip(user_history['USER ID'].to_numpy(), range(len(user_history['USER ID'].to_numpy()))))\n",
    "ind_to_row_id = dict(zip(range(len(user_history['USER ID'].to_numpy())), user_history['USER ID'].to_numpy()))\n",
    "\n",
    "col_id_to_ind = dict(zip(train['PRODUCT'].unique(), range(len(train['PRODUCT'].unique()))))\n",
    "ind_to_col_id = dict(zip(range(len(train['PRODUCT'].unique())), train['PRODUCT'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing PCA (in order to do PCA we must strip the first column then transpose our matrix)\n",
    "X = user_history.loc[:, user_history.columns != 'USER ID'].to_numpy()\n",
    "X_meanzero = (X - np.mean(X, axis = 0)) # subtract the mean \n",
    "X = (X_meanzero / np.std(X, axis = 0)) # divide by the standard deviation\n",
    "U,S,Vt = np.linalg.svd(X, full_matrices=True)\n",
    "\n",
    "#Reduce the dimensionality of the data to 2 dimensions to visualize it \n",
    "user_data_t2 = pd.DataFrame(data = U[:,0:2], index = user_history['USER ID'])\n",
    "\n",
    "# #Visualize the data in 2D\n",
    "# ax1 = user_data_t2.plot.scatter(x=0, y=1)\n",
    "\n",
    "#Use k-means to separate the data into 3 clusters\n",
    "km = KMeans(n_clusters = 3)\n",
    "km.fit(X)\n",
    "prediction = km.predict(X)\n",
    "clusters_df = pd.DataFrame(data = prediction, index = user_history['USER ID'])\n",
    "colors = {0:'red', 1:'green', 2:'blue'}\n",
    "ax1 = user_data_t2.plot.scatter(x=0, y=1, c=clusters_df[0].map(colors))\n",
    "\n",
    "clusters_df = clusters_df.rename(columns={0: 'Cluster'})\n",
    "\n",
    "# separate the user_history df into clusters\n",
    "user_history_clustered = pd.concat([user_history_indexed, clusters_df], axis=1)\n",
    "\n",
    "cluster0 = user_history_clustered[user_history_clustered['Cluster']==0]\n",
    "cluster1 = user_history_clustered[user_history_clustered['Cluster']==1]\n",
    "cluster2 = user_history_clustered[user_history_clustered['Cluster']==2]\n",
    "\n",
    "all_clusters = [cluster0, cluster1, cluster2]\n",
    "\n",
    "num_dimensions_for_each_cluster = []\n",
    "cutoff = 0.9\n",
    "\n",
    "#find the optimal number of dimensions (min that keeps at least 90% of the variation) for each cluster separately\n",
    "for cluster in all_clusters:\n",
    "    X = cluster.loc[:, cluster.columns != 'Cluster'].to_numpy()\n",
    "    X_meanzero = (X - np.mean(X, axis = 0)) # subtract the mean \n",
    "    X = (X_meanzero / np.std(X, axis = 0)) # divide by the standard deviation\n",
    "    U,S,Vt = np.linalg.svd(X, full_matrices=True)\n",
    "    fractions = np.zeros((100))\n",
    "    fractions_greater_than_cutoff = np.zeros((100))\n",
    "    for num_components in range(1,Vt.shape[0]+1):\n",
    "        # Compute the fraction of the total spectrum in the first n singular values\n",
    "        fractions[num_components-1] = sum(S[0:num_components]**2) / sum(S**2)\n",
    "        fractions_greater_than_cutoff[num_components-1] = sum(S[0:num_components]**2) / sum(S**2)\n",
    "    fractions_greater_than_cutoff[fractions_greater_than_cutoff<cutoff]=1\n",
    "    num_dimensions_for_each_cluster.append(np.argmin(fractions_greater_than_cutoff))\n",
    "\n",
    "#do PCA on each cluster with its optimal number of dimensions\n",
    "reduced_cluster_data = []\n",
    "for i in range(len(all_clusters)):\n",
    "    X = all_clusters[i].loc[:, all_clusters[i].columns != 'Cluster'].to_numpy()\n",
    "    X_meanzero = (X - np.mean(X, axis = 0)) # subtract the mean \n",
    "    X = (X_meanzero / np.std(X, axis = 0)) # divide by the standard deviation\n",
    "    U,S,Vt = np.linalg.svd(X, full_matrices=True)\n",
    "    reduced_cluster_data.append(pd.DataFrame(data = U[:,0:num_dimensions_for_each_cluster[i]], \n",
    "                                index=all_clusters[i].reset_index(inplace=False)['USER ID']))\n",
    "reduced_features = pd.concat(reduced_cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history_indexed_normalized = user_history_indexed.copy()\n",
    "user_history_indexed_normalized -= user_history_indexed_normalized.mean()\n",
    "user_history_indexed_normalized /= user_history_indexed_normalized.std()\n",
    "\n",
    "n_cols = len(col_id_to_ind)\n",
    "\n",
    "# added_cols = user_history_indexed_normalized\n",
    "added_cols = reduced_features\n",
    "\n",
    "added_col_id_to_ind = dict(zip(added_cols.columns.to_numpy(), range(n_cols, n_cols+added_cols.shape[1])))\n",
    "ind_to_added_col_id = dict(zip(range(n_cols, n_cols+added_cols.shape[1]), added_cols.columns.to_numpy()))\n",
    "\n",
    "all_cols_id_to_ind = {**col_id_to_ind, **added_col_id_to_ind}\n",
    "ind_to_all_cols_id = {**ind_to_col_id, **ind_to_added_col_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_cols_melted = added_cols.reset_index(inplace=False).melt('USER ID', var_name='PRODUCT', value_name='RATING')\n",
    "\n",
    "train_appended = pd.concat([train, added_cols_melted], axis=0)                #480352 x 3\n",
    "train_appended = train_appended.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "\n",
    "# train_ratings = train['RATING']\n",
    "# train_rows = train['USER ID']\n",
    "# train_cols = train['PRODUCT']\n",
    "\n",
    "train_ratings = train_appended['RATING']\n",
    "train_rows = train_appended['USER ID']\n",
    "train_cols = train_appended['PRODUCT']\n",
    "\n",
    "test_ratings = test['RATING']\n",
    "test_rows = test['USER ID']\n",
    "test_cols = test['PRODUCT']\n",
    "\n",
    "m = len(train_ratings) # the size of the training set                             #30352 for 1st approach, 480352 for 2nd\n",
    "n_rows = user_history['USER ID'].unique().shape[0] # the largest index, plus 1    #4500\n",
    "n_cols = train_cols.unique().shape[0]                                             #75\n",
    "\n",
    "train_rows_inds = np.array([row_id_to_ind[r] for r in train_rows])\n",
    "train_cols_inds = np.array([all_cols_id_to_ind[c] for c in train_cols])\n",
    "\n",
    "test_rows_inds = np.array([row_id_to_ind[r] for r in test_rows])\n",
    "test_cols_inds = np.array([col_id_to_ind[c] for c in test_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_stats = csv.reader(open('train_rating_stats.csv','r'))\n",
    "train_mean_and_std = [row[0] for row in train_data_stats][1:]\n",
    "train_data_mean = float(train_mean_and_std[0])\n",
    "train_data_std = float(train_mean_and_std[1])\n",
    "\n",
    "train_ratings_unnormalized = train_ratings.copy()\n",
    "train_ratings_unnormalized *= train_data_std\n",
    "train_ratings_unnormalized += train_data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(n_rows, n_cols, row_features=np.array([]), col_features=np.array([]), k=15):\n",
    "    \n",
    "    \"\"\"Initalize a random model, and normalize it so that it has sensible mean and variance\"\"\"\n",
    "    # (The normalization helps make sure we start out at a reasonable parameter scale, which speeds up training)\n",
    "    if row_features.size != 0:\n",
    "        k = row_features.shape[1]\n",
    "    elif col_features.size != 0:\n",
    "        k = col_features.shape[1]\n",
    "    if row_features.size == 0:\n",
    "        row_features = np.random.normal(size=(n_rows, k))\n",
    "    else:\n",
    "        row_features -= row_features.mean(axis=1)[:,None]\n",
    "        row_features /= row_features.std(axis=1)[:,None]\n",
    "    if col_features.size == 0:\n",
    "        col_features = np.random.normal(size=(n_cols, k))\n",
    "        \n",
    "    raw_predictions = predict((row_features, col_features))\n",
    "    \n",
    "    s = np.sqrt(2*raw_predictions.std()) # We want to start out with roughly unit variance\n",
    "    b = np.sqrt((train_data_mean - raw_predictions.mean()/s)/k) #We want to start out with average rating 3.5\n",
    "    row_features /= s\n",
    "    row_features += b\n",
    "    col_features /= s\n",
    "    col_features += b\n",
    "    \n",
    "    return (row_features, col_features)\n",
    "\n",
    "def predict(model):\n",
    "    \"\"\"The model's predictions for all row/col pairs\"\"\"\n",
    "    row_features, col_features = model\n",
    "    return row_features @ col_features.T\n",
    "\n",
    "def single_example_step(model, row, col, rating):\n",
    "    \"\"\"Update the model using the gradient at a single training example\"\"\"\n",
    "    row_features, col_features = model\n",
    "    residual = np.dot(row_features[row], col_features[col]) - rating\n",
    "    grad_rows = 2 * residual * col_features[col] # the gradient for the row_features matrix\n",
    "    grad_cols = 2 * residual * row_features[row] # the gradient for the col_features matrix\n",
    "    row_features[row] -= learning_rate*grad_rows\n",
    "    col_features[col] -= learning_rate*grad_cols\n",
    "\n",
    "def train_sgd(model, num_epochs, batch_size):\n",
    "    \"\"\"Train the model for a number of epochs via SGD (batch size=1)\"\"\"\n",
    "    row_features, col_features = model\n",
    "    train_MSEs = []\n",
    "    test_MSEs = []\n",
    "    # It's good practice to shuffle your data before doing batch gradient descent,\n",
    "    # so that each mini-batch peforms like a random sample from the dataset\n",
    "    for epoch in range(num_epochs):\n",
    "        shuffle = np.random.permutation(m) \n",
    "        shuffled_rows = train_rows[shuffle]\n",
    "        shuffled_cols = train_cols[shuffle]\n",
    "        shuffled_ratings = train_ratings[shuffle]\n",
    "        for row, col, rating in zip(shuffled_rows[:batch_size], shuffled_cols[:batch_size], shuffled_ratings[:batch_size]):\n",
    "            # update the model using the gradient at a single example\n",
    "            single_example_step(model, row_id_to_ind[row], all_cols_id_to_ind[col], rating)\n",
    "        # after each Epoch, we'll evaluate our model\n",
    "        predicted = predict(model)        \n",
    "        predicted_unnormalized = predicted.copy()\n",
    "        predicted_unnormalized *= train_data_std\n",
    "        predicted_unnormalized += train_data_mean\n",
    "        train_loss = np.mean((train_ratings_unnormalized - predicted_unnormalized[train_rows_inds, train_cols_inds])**2)\n",
    "        test_loss = np.mean((test_ratings - predicted_unnormalized[test_rows_inds, test_cols_inds])**2)\n",
    "#         print(\"Loss after epoch #{} is: train/{} --- test/{}\".format(epoch+1, train_loss, test_loss))\n",
    "        train_MSEs.append(train_loss)\n",
    "        test_MSEs.append(test_loss)\n",
    "    return predicted_unnormalized, train_MSEs[20:], test_MSEs[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_features = user_history_indexed.to_numpy()\n",
    "sgd_model = initialize(n_rows, n_cols)\n",
    "# sgd_model = initialize(n_rows, n_cols)\n",
    "predictions, train_MSEs, test_MSEs = train_sgd(sgd_model, num_epochs=2000, batch_size=10000)\n",
    "\n",
    "plt.scatter(np.array(range(len(train_MSEs))), np.array(train_MSEs), s=2, c='b', marker='o')\n",
    "plt.scatter(np.array(range(len(test_MSEs))), np.array(test_MSEs), s=2, c='r', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(test_MSEs[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
