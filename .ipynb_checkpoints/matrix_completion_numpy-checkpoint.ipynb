{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful packages\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# load the browsing history data\n",
    "user_history = pd.read_csv(\"user_history.csv\")\n",
    "user_history_without_user_ID = user_history.drop(['USER ID'],axis=1)\n",
    "user_history_indexed = user_history.set_index('USER ID')\n",
    "user_ratings = pd.read_csv(\"user_ratings.csv\")\n",
    "\n",
    "# load the ratings data\n",
    "train_ratings = pd.read_csv(\"train_rating.csv\")\n",
    "test_ratings = pd.read_csv(\"test_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alpine kimono', 'sweden kansas', 'student icon', 'supreme ivan',\n",
       "       'albert charlie', 'heavy trapeze', 'fabric tokyo', 'brother robin',\n",
       "       'tiger catalog', 'explain alex', 'trick bambino', 'lily square',\n",
       "       'oliver ballet', 'segment place', 'patent input', 'exit diet',\n",
       "       'zipper tibet', 'tobacco rubber', 'star drum', 'chariot neon',\n",
       "       'bicycle ambient', 'galileo eric', 'amazon campus',\n",
       "       'montana number', 'welcome visible', 'charm lucky',\n",
       "       'vampire mayday', 'english pinball', 'caesar egypt',\n",
       "       'modern small', 'amigo shampoo', 'talent finland', 'margo alien',\n",
       "       'jason disney', 'final limbo', 'frog command', 'java poem',\n",
       "       'iceberg frame', 'saddle ramirez', 'climax next', 'grand cactus',\n",
       "       'fiction resume', 'audio mango', 'screen silver', 'carmen song',\n",
       "       'armor ocean', 'matrix stuart', 'scoop spider', 'maximum permit',\n",
       "       'adam mimic', 'igloo monaco', 'pattern collect', 'liberal avatar',\n",
       "       'pigment pogo', 'prime common', 'voltage jasmine', 'block betty',\n",
       "       'aztec billy', 'canoe crimson', 'anita tornado', 'chaos paradox',\n",
       "       'zebra dinner', 'private belgium', 'group before', 'picasso cover',\n",
       "       'prelude derby', 'ticket think', 'episode direct', 'reward factor',\n",
       "       'ceramic binary', 'forever snow', 'europe italian',\n",
       "       'meaning popular', 'cliff enrico', 'arsenal greek',\n",
       "       'simple seminar', 'apple invest', 'basic gate', 'deal cafe',\n",
       "       'yoyo margin', 'infant taxi', 'initial bonanza', 'artist omega',\n",
       "       'marco control', 'alfred magnum', 'clinic vortex', 'change escape',\n",
       "       'slow buenos', 'prague wolf', 'sheriff sonata', 'cigar lagoon',\n",
       "       'equal comedy', 'bombay podium', 'helena robot', 'prodigy rhino',\n",
       "       'jumbo gray', 'radius wizard', 'fame quiz', 'bazaar complex',\n",
       "       'glass slogan'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_history_index.columns.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id_to_ind = dict(zip(user_history['USER ID'].to_numpy(), range(len(user_history['USER ID'].to_numpy()))))\n",
    "ind_to_row_id = dict(zip(range(len(user_history['USER ID'].to_numpy())), user_history['USER ID'].to_numpy()))\n",
    "\n",
    "website_id_to_ind = dict(zip(user_history_index.columns.to_numpy(), range(len(user_history_index.columns.to_numpy()))))\n",
    "ind_to_website_id = dict(zip(range(len(user_history_index.columns.to_numpy())), user_history_index.columns.to_numpy()))\n",
    "\n",
    "col_id_to_ind = dict(zip(train['PRODUCT'].unique(), range(len(train['PRODUCT'].unique()))))\n",
    "ind_to_col_id = dict(zip(range(len(train['PRODUCT'].unique())), train['PRODUCT'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "\n",
    "train_ratings = train['RATING']\n",
    "train_rows = train['USER ID']\n",
    "train_cols = train['PRODUCT']\n",
    "test_ratings = test['RATING']\n",
    "test_rows = test['USER ID']\n",
    "test_cols = test['PRODUCT']\n",
    "\n",
    "m = len(train_ratings) # the size of the training set                             #30352\n",
    "n_rows = user_history['USER ID'].unique().shape[0] # the largest index, plus 1    #4500\n",
    "n_cols = train_cols.unique().shape[0]                                             #75\n",
    "\n",
    "train_rows_inds = np.array([row_id_to_ind[r] for r in train_rows])\n",
    "train_cols_inds = np.array([col_id_to_ind[c] for c in train_cols])\n",
    "\n",
    "test_rows_inds = np.array([row_id_to_ind[r] for r in test_rows])\n",
    "test_cols_inds = np.array([col_id_to_ind[c] for c in test_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(n_rows, n_cols, row_features=None, col_features=None, k=15):\n",
    "    \n",
    "    \"\"\"Initalize a random model, and normalize it so that it has sensible mean and variance\"\"\"\n",
    "    # (The normalization helps make sure we start out at a reasonable parameter scale, which speeds up training)\n",
    "    if row_features:\n",
    "        k = row_features.shape[1]\n",
    "    elif col_features:\n",
    "        k = col_features.shape[1]\n",
    "    if not row_features:\n",
    "        row_features = np.random.normal(size=(n_rows, k))\n",
    "    if not col_features:\n",
    "        col_features = np.random.normal(size=(n_cols, k))\n",
    "        \n",
    "    raw_predictions = predict((row_features, col_features))\n",
    "    \n",
    "    s = np.sqrt(2*raw_predictions.std()) # We want to start out with roughly unit variance\n",
    "    b = np.sqrt((3.5 - raw_predictions.mean()/s)/k) #We want to start out with average rating 3.5\n",
    "    row_features /= s\n",
    "    row_features += b\n",
    "    col_features /= s\n",
    "    col_features += b\n",
    "    \n",
    "    return (row_features, col_features)\n",
    "\n",
    "def predict(model):\n",
    "    \"\"\"The model's predictions for all row/col pairs\"\"\"\n",
    "    row_features, col_features = model\n",
    "    return row_features @ col_features.T\n",
    "\n",
    "def single_example_step(model, row, col, rating):\n",
    "    \"\"\"Update the model using the gradient at a single training example\"\"\"\n",
    "    row_features, col_features = model\n",
    "    residual = np.dot(row_features[user], col_features[movie]) - rating\n",
    "    grad_rows = 2 * residual * col_features[col] # the gradient for the row_features matrix\n",
    "    grad_cols = 2 * residual * row_features[row] # the gradient for the col_features matrix\n",
    "    row_features[row] -= learning_rate*grad_rows\n",
    "    col_features[col] -= learning_rate*grad_cols\n",
    "\n",
    "def train_sgd(model, epochs):\n",
    "    \"\"\"Train the model for a number of epochs via SGD (batch size=1)\"\"\"\n",
    "    row_features, col_features = model\n",
    "    # It's good practice to shuffle your data before doing batch gradient descent,\n",
    "    # so that each mini-batch peforms like a random sample from the dataset\n",
    "    shuffle = np.random.permutation(m) \n",
    "    shuffled_rows = train_rows[shuffle]\n",
    "    shuffled_cols = train_cols[shuffle]\n",
    "    shuffled_ratings = train_ratings[shuffle]\n",
    "    for epoch in range(epochs):\n",
    "        for row, col, rating in zip(shuffled_rows[:100], shuffled_cols[:100], shuffled_ratings[:100]):\n",
    "            # update the model using the gradient at a single example\n",
    "            print(row, col, rating)\n",
    "            single_example_step(model, row, col, rating)\n",
    "        # after each Epoch, we'll evaluate our model\n",
    "        predicted = predict(model)\n",
    "        train_loss = np.mean((train_ratings - predicted[train_rows, train_cols])**2)\n",
    "        test_loss = np.mean((test_ratings - predicted[train_rows, train_cols])**2)\n",
    "        print(\"Loss after epoch #{} is: train/{} --- test/{}\".format(epoch+1, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'P' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-f6020908e2ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msgd_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_history_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_sgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msgd_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-95-7011b32786bf>\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(n_rows, n_cols, row_features, col_features, k)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m\"\"\"Initalize a random model, and normalize it so that it has sensible mean and variance\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# (The normalization helps make sure we start out at a reasonable parameter scale, which speeds up training)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'P' is not defined"
     ]
    }
   ],
   "source": [
    "sgd_model = initialize(n_rows, n_cols, P=user_history_index)\n",
    "train_sgd(sgd_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = matrixComplete(train, test, user_history_index)\n",
    "complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1 is: train/2.020499705837473 --- test/2.041398116822025\n",
      "Loss after epoch #2 is: train/1.8452362869045502 --- test/1.865790746658124\n",
      "Loss after epoch #3 is: train/1.7346698915775396 --- test/1.7596933304825082\n",
      "Loss after epoch #4 is: train/1.6533089115962853 --- test/1.6795294299675327\n",
      "Loss after epoch #5 is: train/1.588659848357695 --- test/1.6176778006070938\n",
      "Loss after epoch #6 is: train/1.5349666322235342 --- test/1.565850032869927\n",
      "Loss after epoch #7 is: train/1.4890800486591818 --- test/1.522309095276012\n",
      "Loss after epoch #8 is: train/1.4490750668045365 --- test/1.484344056758278\n",
      "Loss after epoch #9 is: train/1.413681801009058 --- test/1.4510888285122052\n",
      "Loss after epoch #10 is: train/1.3820134039759004 --- test/1.4214149450622864\n",
      "Loss after epoch #11 is: train/1.353422476735558 --- test/1.3947930631991257\n",
      "Loss after epoch #12 is: train/1.3274193638069962 --- test/1.3706614024606327\n",
      "Loss after epoch #13 is: train/1.303623058336582 --- test/1.3486759435000297\n",
      "Loss after epoch #14 is: train/1.2817302186025605 --- test/1.3285140157905175\n",
      "Loss after epoch #15 is: train/1.2614948201997802 --- test/1.309943423503531\n",
      "Loss after epoch #16 is: train/1.2427142797130863 --- test/1.2927585841557545\n",
      "Loss after epoch #17 is: train/1.2252196861540041 --- test/1.2767975889335827\n",
      "Loss after epoch #18 is: train/1.208868719759589 --- test/1.2619199552805942\n",
      "Loss after epoch #19 is: train/1.193540390762186 --- test/1.2480094301821638\n",
      "Loss after epoch #20 is: train/1.1791310439049332 --- test/1.2349653783896333\n",
      "Loss after epoch #21 is: train/1.1655512681354154 --- test/1.2227021255151593\n",
      "Loss after epoch #22 is: train/1.1527234681618257 --- test/1.211145125831714\n",
      "Loss after epoch #23 is: train/1.1405799305020177 --- test/1.2002297709943726\n",
      "Loss after epoch #24 is: train/1.1290612655074945 --- test/1.189899411677465\n",
      "Loss after epoch #25 is: train/1.1181151398963225 --- test/1.1801043251597079\n",
      "Loss after epoch #26 is: train/1.1076952367699515 --- test/1.1708005571216082\n",
      "Loss after epoch #27 is: train/1.0977603959012405 --- test/1.1619491440812855\n",
      "Loss after epoch #28 is: train/1.0882738983258875 --- test/1.1535153722469993\n",
      "Loss after epoch #29 is: train/1.0792028674839218 --- test/1.1454682138335455\n",
      "Loss after epoch #30 is: train/1.0705177652256492 --- test/1.1377798247223592\n",
      "Loss after epoch #31 is: train/1.062191965566093 --- test/1.1304251378950034\n",
      "Loss after epoch #32 is: train/1.054201392550395 --- test/1.1233815100233415\n",
      "Loss after epoch #33 is: train/1.046524211277916 --- test/1.1166284260517196\n",
      "Loss after epoch #34 is: train/1.0391405632251127 --- test/1.1101472440313558\n",
      "Loss after epoch #35 is: train/1.0320323386546009 --- test/1.1039209778719221\n",
      "Loss after epoch #36 is: train/1.0251829802048897 --- test/1.097934109415472\n",
      "Loss after epoch #37 is: train/1.0185773128005144 --- test/1.0921724265184367\n",
      "Loss after epoch #38 is: train/1.0122013958635103 --- test/1.086622882342758\n",
      "Loss after epoch #39 is: train/1.0060423944883594 --- test/1.0812734730425895\n",
      "Loss after epoch #40 is: train/1.0000884667970362 --- test/1.0761131308691918\n",
      "Loss after epoch #41 is: train/0.9943286651444143 --- test/1.071131630569957\n",
      "Loss after epoch #42 is: train/0.9887528492170324 --- test/1.0663195071054554\n",
      "Loss after epoch #43 is: train/0.9833516093758272 --- test/1.061667983136045\n",
      "Loss after epoch #44 is: train/0.9781161988481772 --- test/1.0571689049114352\n",
      "Loss after epoch #45 is: train/0.9730384735864167 --- test/1.0528146854421363\n",
      "Loss after epoch #46 is: train/0.9681108387866147 --- test/1.0485982539826166\n",
      "Loss after epoch #47 is: train/0.9633262012092497 --- test/1.044513011011255\n",
      "Loss after epoch #48 is: train/0.9586779265674723 --- test/1.0405527880054957\n",
      "Loss after epoch #49 is: train/0.9541598013531024 --- test/1.0367118114148757\n",
      "Loss after epoch #50 is: train/0.9497659985586584 --- test/1.0329846703171504\n",
      "Loss after epoch #51 is: train/0.945491046828363 --- test/1.029366287315304\n",
      "Loss after epoch #52 is: train/0.9413298026343809 --- test/1.0258518922930187\n",
      "Loss after epoch #53 is: train/0.9372774251284521 --- test/1.0224369986978459\n",
      "Loss after epoch #54 is: train/0.9333293533650266 --- test/1.0191173820647772\n",
      "Loss after epoch #55 is: train/0.9294812856313194 --- test/1.0158890605302866\n",
      "Loss after epoch #56 is: train/0.9257291606533686 --- test/1.012748277118758\n",
      "Loss after epoch #57 is: train/0.9220691404761273 --- test/1.009691483610584\n",
      "Loss after epoch #58 is: train/0.9184975948405126 --- test/1.0067153258247532\n",
      "Loss after epoch #59 is: train/0.9150110869018432 --- test/1.0038166301690352\n",
      "Loss after epoch #60 is: train/0.9116063601526774 --- test/1.0009923913284011\n",
      "Loss after epoch #61 is: train/0.908280326429177 --- test/0.9982397609775199\n",
      "Loss after epoch #62 is: train/0.9050300548941204 --- test/0.9955560374163585\n",
      "Loss after epoch #63 is: train/0.9018527619018529 --- test/0.9929386560393908\n",
      "Loss after epoch #64 is: train/0.8987458016611026 --- test/0.9903851805589464\n",
      "Loss after epoch #65 is: train/0.8957066576208578 --- test/0.9878932949119679\n",
      "Loss after epoch #66 is: train/0.892732934512643 --- test/0.9854607957871226\n",
      "Loss after epoch #67 is: train/0.8898223509896506 --- test/0.9830855857159446\n",
      "Loss after epoch #68 is: train/0.8869727328094731 --- test/0.980765666677591\n",
      "Loss after epoch #69 is: train/0.8841820065126866 --- test/0.978499134172026\n",
      "Loss after epoch #70 is: train/0.8814481935544277 --- test/0.976284171721038\n",
      "Loss after epoch #71 is: train/0.8787694048504013 --- test/0.9741190457605804\n",
      "Loss after epoch #72 is: train/0.8761438357025887 --- test/0.9720021008915253\n",
      "Loss after epoch #73 is: train/0.8735697610733031 --- test/0.9699317554591366\n",
      "Loss after epoch #74 is: train/0.8710455311792566 --- test/0.967906497434408\n",
      "Loss after epoch #75 is: train/0.8685695673799855 --- test/0.9659248805729618\n",
      "Loss after epoch #76 is: train/0.8661403583373697 --- test/0.9639855208294612\n",
      "Loss after epoch #77 is: train/0.8637564564251253 --- test/0.962087093007528\n",
      "Loss after epoch #78 is: train/0.8614164743690595 --- test/0.9602283276269569\n",
      "Loss after epoch #79 is: train/0.859119082100593 --- test/0.9584080079916574\n",
      "Loss after epoch #80 is: train/0.8568630038075995 --- test/0.9566249674432116\n",
      "Loss after epoch #81 is: train/0.8546470151679887 --- test/0.9548780867862486\n",
      "Loss after epoch #82 is: train/0.8524699407527219 --- test/0.9531662918730244\n",
      "Loss after epoch #83 is: train/0.8503306515860554 --- test/0.9514885513356653\n",
      "Loss after epoch #84 is: train/0.8482280628518438 --- test/0.9498438744554962\n",
      "Loss after epoch #85 is: train/0.8461611317356394 --- test/0.9482313091597465\n",
      "Loss after epoch #86 is: train/0.8441288553931666 --- test/0.9466499401367315\n",
      "Loss after epoch #87 is: train/0.842130269036504 --- test/0.9450988870613015\n",
      "Loss after epoch #88 is: train/0.8401644441299913 --- test/0.9435773029230288\n",
      "Loss after epoch #89 is: train/0.8382304866885038 --- test/0.9420843724501772\n",
      "Loss after epoch #90 is: train/0.8363275356713101 --- test/0.9406193106230485\n",
      "Loss after epoch #91 is: train/0.8344547614652406 --- test/0.9391813612707887\n",
      "Loss after epoch #92 is: train/0.83261136445137 --- test/0.9377697957461897\n",
      "Loss after epoch #93 is: train/0.8307965736498562 --- test/0.9363839116734289\n",
      "Loss after epoch #94 is: train/0.829009645437961 --- test/0.9350230317640688\n",
      "Loss after epoch #95 is: train/0.8272498623366549 --- test/0.9336865026969772\n",
      "Loss after epoch #96 is: train/0.8255165318615276 --- test/0.9323736940581533\n",
      "Loss after epoch #97 is: train/0.823808985434042 --- test/0.9310839973367249\n",
      "Loss after epoch #98 is: train/0.8221265773494429 --- test/0.929816824973649\n",
      "Loss after epoch #99 is: train/0.8204686837978926 --- test/0.9285716094599047\n",
      "Loss after epoch #100 is: train/0.8188347019356419 --- test/0.9273478024811702\n"
     ]
    }
   ],
   "source": [
    " def all_examples_step(model):\n",
    "    \"\"\"Update the model using the gradient averaged over all training examples\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    # To average the gradient over all training examples, it's convenient to\n",
    "    #    initialize arrays of zeros to hold the full gradients, and then update\n",
    "    #    these arrays at each training example, just like in the SGD procedure\n",
    "    grad_users = np.zeros_like(user_features)\n",
    "    grad_movies = np.zeros_like(movie_features)\n",
    "    # We only need to compute the model's predicted ratings once\n",
    "    predicted = predict(model)\n",
    "    for user, movie, rating in zip(train_users, train_movies, train_ratings):\n",
    "        # Mimic the SGD procedure, but store the gradients so they can be averaged\n",
    "        residual = predicted[user, movie] - rating\n",
    "        grad_users[user] += 2 * residual * movie_features[movie]\n",
    "        grad_movies[movie] += 2 * residual * user_features[user]\n",
    "    user_features -= learning_rate/m * grad_users # Update using the averaged gradients\n",
    "    movie_features -= learning_rate/m * grad_movies\n",
    "\n",
    "    \n",
    "def train_full(model, epochs):\n",
    "    \"\"\"Train the model for a number of epochs using gradients estimated from the entire training set\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    for epoch in range(epochs):\n",
    "        all_examples_step(model)\n",
    "        predicted = predict(model)\n",
    "        train_loss = np.mean((train_ratings - predicted[train_users, train_movies])**2)\n",
    "        test_loss = np.mean((test_ratings - predicted[test_users, test_movies])**2)\n",
    "        print(\"Loss after epoch #{} is: train/{} --- test/{}\".format(epoch+1, train_loss, test_loss))\n",
    "        \n",
    "full_model = initialize(n_users, n_movies, k)\n",
    "learning_rate = 8. # Since we are averaging very sparse gradients,\n",
    "# the gradients will be small and we can use a large learning rate\n",
    "train_full(full_model, 100) # We only get a single update to the model from each epoch, so we'll need a lot more epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1 is: train/0.9796749884518493 --- test/1.0411660488539876\n",
      "Loss after epoch #2 is: train/0.8437374393115624 --- test/0.925966987361918\n",
      "Loss after epoch #3 is: train/0.784311191958209 --- test/0.8829530230630737\n",
      "Loss after epoch #4 is: train/0.7489426708883228 --- test/0.8611252495609698\n",
      "Loss after epoch #5 is: train/0.7245977046432523 --- test/0.8483312823137272\n",
      "Loss after epoch #6 is: train/0.7063208459315726 --- test/0.8402340279084958\n",
      "Loss after epoch #7 is: train/0.6917601707419518 --- test/0.8349122722847582\n",
      "Loss after epoch #8 is: train/0.6796369806969069 --- test/0.8313865436232689\n",
      "Loss after epoch #9 is: train/0.6691896456852302 --- test/0.8291039647521733\n",
      "Loss after epoch #10 is: train/0.6599343294677924 --- test/0.8277265922024674\n"
     ]
    }
   ],
   "source": [
    "# k=5\n",
    "learning_rate = 0.005\n",
    "k = 5 # the number of features (for each user/movie)\n",
    "m = len(train_ratings) # the size of the training set\n",
    "n_users = max(train_users)+1 # the largest index, plus 1\n",
    "n_movies = max(train_movies)+1\n",
    "\n",
    "\n",
    "sgd_model = initialize(n_users, n_movies, k)\n",
    "train_sgd(sgd_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1 is: train/0.9107490224332668 --- test/1.0244002481638164\n",
      "Loss after epoch #2 is: train/0.7903246204202476 --- test/0.9343603225841822\n",
      "Loss after epoch #3 is: train/0.7314271132558519 --- test/0.8973574367684758\n",
      "Loss after epoch #4 is: train/0.6933162072776323 --- test/0.8787343859615363\n",
      "Loss after epoch #5 is: train/0.6648578208137972 --- test/0.8689558249653597\n",
      "Loss after epoch #6 is: train/0.6415847904793435 --- test/0.8642175541220632\n",
      "Loss after epoch #7 is: train/0.6213306212223534 --- test/0.8626972005379843\n",
      "Loss after epoch #8 is: train/0.602929240882611 --- test/0.8634097649480269\n",
      "Loss after epoch #9 is: train/0.5857218599711521 --- test/0.8657695011093927\n",
      "Loss after epoch #10 is: train/0.5693365174623799 --- test/0.8693973044897281\n"
     ]
    }
   ],
   "source": [
    "# k=15\n",
    "learning_rate = 0.005\n",
    "k = 15 # the number of features (for each user/movie)\n",
    "m = len(train_ratings) # the size of the training set\n",
    "n_users = max(train_users)+1 # the largest index, plus 1\n",
    "n_movies = max(train_movies)+1\n",
    "\n",
    "sgd_model = initialize(n_users, n_movies, k)\n",
    "train_sgd(sgd_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1 is: train/0.8826911106673225 --- test/0.9798710565376084\n",
      "Loss after epoch #2 is: train/0.7576263129044959 --- test/0.9152627330836701\n",
      "Loss after epoch #3 is: train/0.6904738091783367 --- test/0.8908730561148943\n",
      "Loss after epoch #4 is: train/0.6435845922399914 --- test/0.8806929549738497\n",
      "Loss after epoch #5 is: train/0.6060401289782779 --- test/0.8775516029910546\n",
      "Loss after epoch #6 is: train/0.5734216363750773 --- test/0.8786278871027476\n",
      "Loss after epoch #7 is: train/0.5436757440573078 --- test/0.8825768676231919\n",
      "Loss after epoch #8 is: train/0.5158304999765658 --- test/0.888649651986109\n",
      "Loss after epoch #9 is: train/0.48947223113411986 --- test/0.8963541432566015\n",
      "Loss after epoch #10 is: train/0.4644765471384368 --- test/0.9053089464739565\n"
     ]
    }
   ],
   "source": [
    "# k=15\n",
    "learning_rate = 0.005\n",
    "k = 30 # the number of features (for each user/movie)\n",
    "m = len(train_ratings) # the size of the training set\n",
    "n_users = max(train_users)+1 # the largest index, plus 1\n",
    "n_movies = max(train_movies)+1\n",
    "\n",
    "sgd_model = initialize(n_users, n_movies, k)\n",
    "train_sgd(sgd_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1 is: train/0.9324304990798615 --- test/1.0401212085123999\n",
      "Loss after epoch #2 is: train/0.8081840125315494 --- test/0.9383605117721585\n",
      "Loss after epoch #3 is: train/0.7509945633018889 --- test/0.8979239486277207\n",
      "Loss after epoch #4 is: train/0.7155744790345461 --- test/0.8769689255431026\n",
      "Loss after epoch #5 is: train/0.690160819674249 --- test/0.8649548392672366\n",
      "Loss after epoch #6 is: train/0.670201422555586 --- test/0.8578976502169289\n",
      "Loss after epoch #7 is: train/0.653518526085807 --- test/0.8539350537810769\n",
      "Loss after epoch #8 is: train/0.6389266811313314 --- test/0.8520744518431724\n",
      "Loss after epoch #9 is: train/0.6257257595461857 --- test/0.8517360299984773\n",
      "Loss after epoch #10 is: train/0.6134813502160514 --- test/0.8525555183149279\n"
     ]
    }
   ],
   "source": [
    "# k=10\n",
    "learning_rate = 0.005\n",
    "k = 10 # the number of features (for each user/movie)\n",
    "m = len(train_ratings) # the size of the training set\n",
    "n_users = max(train_users)+1 # the largest index, plus 1\n",
    "n_movies = max(train_movies)+1\n",
    "\n",
    "sgd_model = initialize(n_users, n_movies, k)\n",
    "train_sgd(sgd_model, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
