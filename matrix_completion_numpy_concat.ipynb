{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful packages\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# load the browsing history data\n",
    "user_history = pd.read_csv(\"user_history.csv\")\n",
    "user_history_without_user_ID = user_history.drop(['USER ID'],axis=1)\n",
    "user_history_indexed = user_history.set_index('USER ID')\n",
    "user_ratings = pd.read_csv(\"user_ratings.csv\")\n",
    "\n",
    "# load the ratings data\n",
    "train = pd.read_csv(\"train_rating.csv\")\n",
    "test = pd.read_csv(\"test_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id_to_ind = dict(zip(user_history['USER ID'].to_numpy(), range(len(user_history['USER ID'].to_numpy()))))\n",
    "ind_to_row_id = dict(zip(range(len(user_history['USER ID'].to_numpy())), user_history['USER ID'].to_numpy()))\n",
    "\n",
    "col_id_to_ind = dict(zip(train['PRODUCT'].unique(), range(len(train['PRODUCT'].unique()))))\n",
    "ind_to_col_id = dict(zip(range(len(train['PRODUCT'].unique())), train['PRODUCT'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history_indexed_normalized = user_history_indexed.copy()\n",
    "user_history_indexed_normalized -= user_history_indexed_normalized.mean()\n",
    "user_history_indexed_normalized /= user_history_indexed_normalized.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing PCA (in order to do PCA we must strip the first column then transpose our matrix)\n",
    "X = user_history_indexed_normalized.to_numpy()\n",
    "X_meanzero = (X - np.mean(X, axis = 0)) # subtract the mean \n",
    "X = (X_meanzero / np.std(X, axis = 0)) # divide by the standard deviation\n",
    "U,S,Vt = np.linalg.svd(X, full_matrices=True)\n",
    "\n",
    "# Compute the fraction of the total spectrum in the first n singular values\n",
    "# spectrum_fractions = []\n",
    "# for n in range(S.shape[0]):\n",
    "#     spectrum_fractions.append(sum(S[0:n]**2) / sum(S**2))\n",
    "# plt.scatter(np.array(range(len(spectrum_fractions))), np.array(spectrum_fractions), s=10, c='b', marker='o')\n",
    "\n",
    "#Now let us use this top 20 singular vectors to make a matrix \n",
    "user_data_t20 = pd.DataFrame(data = U[:,0:20], index = user_history['USER ID'])\n",
    "\n",
    "# #Visualize the data in 2D\n",
    "# ax1 = user_data_t2.plot.scatter(x=0, y=1)\n",
    "\n",
    "reduced_features = user_data_t20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reduce the dimensionality of the data to 2 dimensions to visualize it \n",
    "# user_data_t2 = pd.DataFrame(data = U[:,0:2], index = user_history['USER ID'])\n",
    "\n",
    "# #Use k-means to separate the data into 3 clusters\n",
    "# km = KMeans(n_clusters = 3)\n",
    "# km.fit(X)\n",
    "# prediction = km.predict(X)\n",
    "# clusters_df = pd.DataFrame(data = prediction, index = user_history['USER ID'])\n",
    "# colors = {0:'red', 1:'green', 2:'blue'}\n",
    "# ax1 = user_data_t2.plot.scatter(x=0, y=1, c=clusters_df[0].map(colors))\n",
    "\n",
    "# clusters_df = clusters_df.rename(columns={0: 'Cluster'})\n",
    "\n",
    "# # separate the user_history df into clusters\n",
    "# user_history_clustered = pd.concat([user_history_indexed, clusters_df], axis=1)\n",
    "\n",
    "# cluster0 = user_history_clustered[user_history_clustered['Cluster']==0]\n",
    "# cluster1 = user_history_clustered[user_history_clustered['Cluster']==1]\n",
    "# cluster2 = user_history_clustered[user_history_clustered['Cluster']==2]\n",
    "\n",
    "# all_clusters = [cluster0, cluster1, cluster2]\n",
    "\n",
    "# num_dimensions_for_each_cluster = []\n",
    "# cutoff = 0.9\n",
    "\n",
    "# #find the optimal number of dimensions (min that keeps at least 90% of the variation) for each cluster separately\n",
    "# for cluster in all_clusters:\n",
    "#     X = cluster.loc[:, cluster.columns != 'Cluster'].to_numpy()\n",
    "#     X_meanzero = (X - np.mean(X, axis = 0)) # subtract the mean \n",
    "#     X = (X_meanzero / np.std(X, axis = 0)) # divide by the standard deviation\n",
    "#     U,S,Vt = np.linalg.svd(X, full_matrices=True)\n",
    "#     fractions = np.zeros((100))\n",
    "#     fractions_greater_than_cutoff = np.zeros((100))\n",
    "#     for num_components in range(1,Vt.shape[0]+1):\n",
    "#         # Compute the fraction of the total spectrum in the first n singular values\n",
    "#         fractions[num_components-1] = sum(S[0:num_components]**2) / sum(S**2)\n",
    "#         fractions_greater_than_cutoff[num_components-1] = sum(S[0:num_components]**2) / sum(S**2)\n",
    "#     fractions_greater_than_cutoff[fractions_greater_than_cutoff<cutoff]=1\n",
    "#     num_dimensions_for_each_cluster.append(np.argmin(fractions_greater_than_cutoff))\n",
    "\n",
    "# #do PCA on each cluster with its optimal number of dimensions\n",
    "# reduced_cluster_data = []\n",
    "# for i in range(len(all_clusters)):\n",
    "#     X = all_clusters[i].loc[:, all_clusters[i].columns != 'Cluster'].to_numpy()\n",
    "#     X_meanzero = (X - np.mean(X, axis = 0)) # subtract the mean \n",
    "#     X = (X_meanzero / np.std(X, axis = 0)) # divide by the standard deviation\n",
    "#     U,S,Vt = np.linalg.svd(X, full_matrices=True)\n",
    "#     reduced_cluster_data.append(pd.DataFrame(data = U[:,0:num_dimensions_for_each_cluster[i]], \n",
    "#                                 index=all_clusters[i].reset_index(inplace=False)['USER ID']))\n",
    "\n",
    "# reduced_features = pd.concat(reduced_cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_index = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_clusters = train.merge(pd.DataFrame(user_history_clustered['Cluster']).reset_index())\n",
    "train_cluster_0 = train_with_clusters[train_with_clusters['Cluster']==0].drop(columns=['Cluster'])   #10149 entries\n",
    "train_cluster_1 = train_with_clusters[train_with_clusters['Cluster']==1].drop(columns=['Cluster'])   #10111 entries\n",
    "train_cluster_2 = train_with_clusters[train_with_clusters['Cluster']==2].drop(columns=['Cluster'])   #10092 entries\n",
    "train_clusters = [train_cluster_0, train_cluster_1, train_cluster_2]\n",
    "\n",
    "test_with_clusters = test.merge(pd.DataFrame(user_history_clustered['Cluster']).reset_index())\n",
    "test_cluster_0 = test_with_clusters[test_with_clusters['Cluster']==0].drop(columns=['Cluster'])  #1119 entries\n",
    "test_cluster_1 = test_with_clusters[test_with_clusters['Cluster']==1].drop(columns=['Cluster'])  #1168 entries\n",
    "test_cluster_2 = test_with_clusters[test_with_clusters['Cluster']==2].drop(columns=['Cluster'])  #1086 entries\n",
    "test_clusters = [test_cluster_0, test_cluster_1, test_cluster_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_features = reduced_features.sort_index()\n",
    "# reduced_features = reduced_cluster_data[cluster_index].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = len(col_id_to_ind)\n",
    "\n",
    "# added_cols = user_history_indexed_normalized\n",
    "added_cols = reduced_features\n",
    "\n",
    "added_col_id_to_ind = dict(zip(added_cols.columns.to_numpy(), range(n_cols, n_cols+added_cols.shape[1])))\n",
    "ind_to_added_col_id = dict(zip(range(n_cols, n_cols+added_cols.shape[1]), added_cols.columns.to_numpy()))\n",
    "\n",
    "all_cols_id_to_ind = {**col_id_to_ind, **added_col_id_to_ind}\n",
    "ind_to_all_cols_id = {**ind_to_col_id, **ind_to_added_col_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_cols_melted = added_cols.reset_index(inplace=False).melt('USER ID', var_name='PRODUCT', value_name='RATING')\n",
    "\n",
    "train = train_clusters[cluster_index]\n",
    "test = test_clusters[cluster_index]\n",
    "\n",
    "train_appended = pd.concat([train, added_cols_melted], axis=0)                #480352 x 3\n",
    "train_appended = train_appended.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "\n",
    "# train_ratings = train['RATING']\n",
    "# train_rows = train['USER ID']\n",
    "# train_cols = train['PRODUCT']\n",
    "\n",
    "train_ratings = train_appended['RATING']\n",
    "train_rows = train_appended['USER ID']\n",
    "train_cols = train_appended['PRODUCT']\n",
    "\n",
    "test_ratings = test['RATING']\n",
    "test_rows = test['USER ID']\n",
    "test_cols = test['PRODUCT']\n",
    "\n",
    "m = len(train_ratings) # the size of the training set                             #30352 for 1st approach, 480352 for 2nd\n",
    "n_rows = user_history['USER ID'].unique().shape[0] # the largest index, plus 1    #4500\n",
    "n_cols = train_cols.unique().shape[0]                                             #75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_inds = reduced_cluster_data[cluster_index].reset_index()['USER ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id_to_ind_cluster = dict(zip(cluster_inds, range(len(cluster_inds))))\n",
    "ind_to_row_id_cluster = dict(zip(range(len(cluster_inds)), cluster_inds))\n",
    "\n",
    "# row_id_to_ind = row_id_to_ind_cluster\n",
    "# ind_to_row_id = ind_to_row_id_cluster\n",
    "\n",
    "train_rows_inds = np.array([row_id_to_ind[r] for r in train_rows])\n",
    "train_cols_inds = np.array([all_cols_id_to_ind[c] for c in train_cols])\n",
    "\n",
    "test_rows_inds = np.array([row_id_to_ind[r] for r in test_rows])\n",
    "test_cols_inds = np.array([col_id_to_ind[c] for c in test_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_stats = csv.reader(open('train_rating_stats.csv','r'))\n",
    "train_mean_and_std = [row[0] for row in train_data_stats][1:]\n",
    "train_data_mean = float(train_mean_and_std[0])\n",
    "train_data_std = float(train_mean_and_std[1])\n",
    "\n",
    "train_ratings_unnormalized = train_ratings.copy()\n",
    "train_ratings_unnormalized *= train_data_std\n",
    "train_ratings_unnormalized += train_data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(n_rows, n_cols, row_features=np.array([]), col_features=np.array([]), k=15):\n",
    "    \n",
    "    \"\"\"Initalize a random model, and normalize it so that it has sensible mean and variance\"\"\"\n",
    "    # (The normalization helps make sure we start out at a reasonable parameter scale, which speeds up training)\n",
    "    if row_features.size != 0:\n",
    "        k = row_features.shape[1]\n",
    "    elif col_features.size != 0:\n",
    "        k = col_features.shape[1]\n",
    "    if row_features.size == 0:\n",
    "        row_features = np.random.normal(size=(n_rows, k))\n",
    "    else:\n",
    "        row_features -= row_features.mean(axis=1)[:,None]\n",
    "        row_features /= row_features.std(axis=1)[:,None]\n",
    "    if col_features.size == 0:\n",
    "        col_features = np.random.normal(size=(n_cols, k))\n",
    "        \n",
    "    raw_predictions = predict((row_features, col_features))\n",
    "    \n",
    "    s = np.sqrt(2*raw_predictions.std()) # We want to start out with roughly unit variance\n",
    "    b = np.sqrt((train_data_mean - raw_predictions.mean()/s)/k) #We want to start out with average rating 3.5\n",
    "    row_features /= s\n",
    "    row_features += b\n",
    "    col_features /= s\n",
    "    col_features += b\n",
    "    \n",
    "    return (row_features, col_features)\n",
    "\n",
    "def predict(model):\n",
    "    \"\"\"The model's predictions for all row/col pairs\"\"\"\n",
    "    row_features, col_features = model\n",
    "    return row_features @ col_features.T\n",
    "\n",
    "def single_example_step(model, row, col, rating):\n",
    "    \"\"\"Update the model using the gradient at a single training example\"\"\"\n",
    "    row_features, col_features = model\n",
    "    residual = np.dot(row_features[row], col_features[col]) - rating\n",
    "    grad_rows = 2 * residual * col_features[col] # the gradient for the row_features matrix\n",
    "    grad_cols = 2 * residual * row_features[row] # the gradient for the col_features matrix\n",
    "    row_features[row] -= learning_rate*grad_rows\n",
    "    col_features[col] -= learning_rate*grad_cols\n",
    "\n",
    "def train_sgd(model, num_epochs, batch_size):\n",
    "    \"\"\"Train the model for a number of epochs via SGD (batch size=1)\"\"\"\n",
    "    row_features, col_features = model\n",
    "    train_MSEs = []\n",
    "    test_MSEs = []\n",
    "    # It's good practice to shuffle your data before doing batch gradient descent,\n",
    "    # so that each mini-batch peforms like a random sample from the dataset\n",
    "    for epoch in range(num_epochs):\n",
    "        shuffle = np.random.permutation(m)\n",
    "        shuffled_rows = train_rows.reset_index(inplace=False).loc[shuffle]['USER ID']\n",
    "        shuffled_cols = train_cols.reset_index(inplace=False).loc[shuffle]['PRODUCT']\n",
    "        shuffled_ratings = train_ratings.reset_index(inplace=False).loc[shuffle]['RATING']\n",
    "        for row, col, rating in zip(shuffled_rows[:batch_size], shuffled_cols[:batch_size], shuffled_ratings[:batch_size]):\n",
    "            # update the model using the gradient at a single example\n",
    "            single_example_step(model, row_id_to_ind[row], all_cols_id_to_ind[col], rating)\n",
    "        # after each Epoch, we'll evaluate our model\n",
    "        predicted = predict(model)        \n",
    "        predicted_unnormalized = predicted.copy()\n",
    "        predicted_unnormalized *= train_data_std\n",
    "        predicted_unnormalized += train_data_mean\n",
    "        predicted_bounded = np.clip(predicted_unnormalized,0,10)\n",
    "        train_loss = np.mean((train_ratings_unnormalized - predicted_unnormalized[train_rows_inds, train_cols_inds])**2)\n",
    "        test_loss = np.mean((test_ratings - predicted_unnormalized[test_rows_inds, test_cols_inds])**2)\n",
    "        print(\"Loss after epoch #{} is: train/{} --- test/{}\".format(epoch+1, train_loss, test_loss))\n",
    "        train_MSEs.append(train_loss)\n",
    "        test_MSEs.append(test_loss)\n",
    "    return predicted_unnormalized, train_MSEs[20:], test_MSEs[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1 is: train/13.801059901816348 --- test/126.28783002856613\n",
      "Loss after epoch #2 is: train/9.558455472776396 --- test/88.6986834668267\n",
      "Loss after epoch #3 is: train/6.8842826715146295 --- test/64.54045516448993\n",
      "Loss after epoch #4 is: train/5.268466020325066 --- test/50.374796645102606\n",
      "Loss after epoch #5 is: train/3.9661182848120315 --- test/38.66711753240395\n",
      "Loss after epoch #6 is: train/2.9719053227193237 --- test/29.217366188625004\n",
      "Loss after epoch #7 is: train/2.3163148169078265 --- test/22.944271060153415\n",
      "Loss after epoch #8 is: train/1.8684061064033641 --- test/18.665634167658585\n",
      "Loss after epoch #9 is: train/1.5796639205784606 --- test/15.98533562123944\n",
      "Loss after epoch #10 is: train/1.3611611245535353 --- test/13.813984182293149\n",
      "Loss after epoch #11 is: train/1.2048964591085964 --- test/12.351145444949434\n",
      "Loss after epoch #12 is: train/1.12126006779279 --- test/11.534518468038494\n",
      "Loss after epoch #13 is: train/1.037033896587739 --- test/10.668916876577965\n",
      "Loss after epoch #14 is: train/0.9884833696769134 --- test/10.151475826585571\n",
      "Loss after epoch #15 is: train/0.9360403999529252 --- test/9.736923363050337\n",
      "Loss after epoch #16 is: train/0.906902449416086 --- test/9.372091877516313\n",
      "Loss after epoch #17 is: train/0.8830330402621706 --- test/9.143010680594996\n",
      "Loss after epoch #18 is: train/0.8735897607674764 --- test/8.988213164693462\n",
      "Loss after epoch #19 is: train/0.8553913688076694 --- test/8.810174444055196\n",
      "Loss after epoch #20 is: train/0.8515332975106693 --- test/8.817359190845815\n",
      "Loss after epoch #21 is: train/0.843652832651774 --- test/8.719809380888409\n",
      "Loss after epoch #22 is: train/0.8369067766979044 --- test/8.690354628276015\n",
      "Loss after epoch #23 is: train/0.8290562771183833 --- test/8.660021532167226\n",
      "Loss after epoch #24 is: train/0.8276010575882027 --- test/8.635073030336745\n",
      "Loss after epoch #25 is: train/0.827287055239149 --- test/8.685361673166645\n",
      "Loss after epoch #26 is: train/0.8186936030140033 --- test/8.527812384787232\n",
      "Loss after epoch #27 is: train/0.8140860993712468 --- test/8.495296755507237\n",
      "Loss after epoch #28 is: train/0.81263092070352 --- test/8.566186139394391\n",
      "Loss after epoch #29 is: train/0.8094452684344882 --- test/8.604386288548909\n",
      "Loss after epoch #30 is: train/0.8056358007934306 --- test/8.569429191121044\n",
      "Loss after epoch #31 is: train/0.8013597539790764 --- test/8.556443906923574\n",
      "Loss after epoch #32 is: train/0.8026005488726553 --- test/8.597582922366485\n",
      "Loss after epoch #33 is: train/0.800444217184979 --- test/8.465974509606841\n",
      "Loss after epoch #34 is: train/0.799355978486209 --- test/8.541251983617\n",
      "Loss after epoch #35 is: train/0.8001606071167017 --- test/8.48120204513599\n",
      "Loss after epoch #36 is: train/0.7938839070889706 --- test/8.519855240944008\n",
      "Loss after epoch #37 is: train/0.7927113615838253 --- test/8.536062305012335\n",
      "Loss after epoch #38 is: train/0.7952257675547633 --- test/8.608164907614885\n",
      "Loss after epoch #39 is: train/0.7944126038609421 --- test/8.665203728765116\n",
      "Loss after epoch #40 is: train/0.7930563147748436 --- test/8.70366661386831\n",
      "Loss after epoch #41 is: train/0.7924415383856505 --- test/8.75571642914819\n",
      "Loss after epoch #42 is: train/0.7888642023900648 --- test/8.636142744108453\n",
      "Loss after epoch #43 is: train/0.7865167078896018 --- test/8.579393376929463\n",
      "Loss after epoch #44 is: train/0.7811406166412115 --- test/8.518901889256679\n",
      "Loss after epoch #45 is: train/0.7785463767787353 --- test/8.542244795365038\n",
      "Loss after epoch #46 is: train/0.7762085577518828 --- test/8.560368311930706\n",
      "Loss after epoch #47 is: train/0.7716929716687515 --- test/8.493963377907011\n",
      "Loss after epoch #48 is: train/0.7688785923077668 --- test/8.34560378579673\n",
      "Loss after epoch #49 is: train/0.7655949469840527 --- test/8.334204806117674\n",
      "Loss after epoch #50 is: train/0.7647367063464526 --- test/8.337692541221369\n",
      "Loss after epoch #51 is: train/0.7647521761356031 --- test/8.267717200743274\n",
      "Loss after epoch #52 is: train/0.7605060511926729 --- test/8.30226677121099\n",
      "Loss after epoch #53 is: train/0.7617887278589683 --- test/8.30263048633921\n",
      "Loss after epoch #54 is: train/0.7643435844160673 --- test/8.30724693381534\n",
      "Loss after epoch #55 is: train/0.7601244356369738 --- test/8.243236344441346\n",
      "Loss after epoch #56 is: train/0.7576173921572186 --- test/8.26961805152003\n",
      "Loss after epoch #57 is: train/0.7538670484900183 --- test/8.290128653482878\n",
      "Loss after epoch #58 is: train/0.7523369139305599 --- test/8.334387807251186\n",
      "Loss after epoch #59 is: train/0.7502177570173038 --- test/8.338981548250231\n",
      "Loss after epoch #60 is: train/0.7485581833073259 --- test/8.368794893206912\n",
      "Loss after epoch #61 is: train/0.7460701229036742 --- test/8.334089524465108\n",
      "Loss after epoch #62 is: train/0.7447469999577396 --- test/8.351400346384777\n",
      "Loss after epoch #63 is: train/0.7430927750614553 --- test/8.281836921808852\n",
      "Loss after epoch #64 is: train/0.7434385731703456 --- test/8.262718436469878\n",
      "Loss after epoch #65 is: train/0.7416210890483484 --- test/8.24908001697288\n",
      "Loss after epoch #66 is: train/0.7411207978654584 --- test/8.293033624847617\n",
      "Loss after epoch #67 is: train/0.7372521269561532 --- test/8.255704742061974\n",
      "Loss after epoch #68 is: train/0.7395330108515263 --- test/8.342630927937023\n",
      "Loss after epoch #69 is: train/0.733586266044873 --- test/8.189162133608308\n",
      "Loss after epoch #70 is: train/0.7338502252091966 --- test/8.29634273131826\n",
      "Loss after epoch #71 is: train/0.7325356691728087 --- test/8.303747582270338\n",
      "Loss after epoch #72 is: train/0.7311209835149229 --- test/8.311657619188585\n",
      "Loss after epoch #73 is: train/0.7325059663991499 --- test/8.406700014530719\n",
      "Loss after epoch #74 is: train/0.7271692273055529 --- test/8.265791950008431\n",
      "Loss after epoch #75 is: train/0.728440291423243 --- test/8.248375729376834\n",
      "Loss after epoch #76 is: train/0.721992092604605 --- test/8.071134098880506\n",
      "Loss after epoch #77 is: train/0.7205315825256063 --- test/8.07838994003556\n",
      "Loss after epoch #78 is: train/0.7195980134160999 --- test/8.186900587606594\n",
      "Loss after epoch #79 is: train/0.7142865168205279 --- test/8.066510491538057\n",
      "Loss after epoch #80 is: train/0.7170819053012516 --- test/8.089107635204497\n",
      "Loss after epoch #81 is: train/0.715861015673051 --- test/8.062424018453338\n",
      "Loss after epoch #82 is: train/0.7100579175281073 --- test/8.069784173885289\n",
      "Loss after epoch #83 is: train/0.7089227438221075 --- test/8.076464454407484\n",
      "Loss after epoch #84 is: train/0.7095423374308192 --- test/8.155227741422962\n",
      "Loss after epoch #85 is: train/0.7074018496636242 --- test/8.070584018838938\n",
      "Loss after epoch #86 is: train/0.7036262248337345 --- test/8.061134994485254\n",
      "Loss after epoch #87 is: train/0.7019598546377244 --- test/8.070480320186359\n",
      "Loss after epoch #88 is: train/0.7062225921811728 --- test/8.091290609092091\n",
      "Loss after epoch #89 is: train/0.7032552383727242 --- test/8.045567623220066\n",
      "Loss after epoch #90 is: train/0.7008754986731571 --- test/7.999438448116896\n",
      "Loss after epoch #91 is: train/0.6993258522508761 --- test/7.99701650363328\n",
      "Loss after epoch #92 is: train/0.6992045322851282 --- test/7.989099556460757\n",
      "Loss after epoch #93 is: train/0.6990565660015965 --- test/7.96327701014369\n",
      "Loss after epoch #94 is: train/0.6998443384261523 --- test/8.062900011955758\n",
      "Loss after epoch #95 is: train/0.6983551101425894 --- test/8.086759572147871\n",
      "Loss after epoch #96 is: train/0.6958203293036731 --- test/8.08321327599439\n",
      "Loss after epoch #97 is: train/0.6933234116295526 --- test/8.110665746306665\n",
      "Loss after epoch #98 is: train/0.6909315353526893 --- test/8.082148443852878\n",
      "Loss after epoch #99 is: train/0.687863941391972 --- test/8.088649305609078\n",
      "Loss after epoch #100 is: train/0.6862518318512406 --- test/8.083034762146733\n",
      "Loss after epoch #101 is: train/0.6900099032311228 --- test/8.133121456311043\n",
      "Loss after epoch #102 is: train/0.6906720824445045 --- test/8.206640404213998\n",
      "Loss after epoch #103 is: train/0.6876203924821385 --- test/8.122433992910153\n",
      "Loss after epoch #104 is: train/0.6849105921644875 --- test/8.017350980726668\n",
      "Loss after epoch #105 is: train/0.6840105119482164 --- test/8.09387573348219\n",
      "Loss after epoch #106 is: train/0.6789216330261828 --- test/7.966666378749513\n",
      "Loss after epoch #107 is: train/0.6791348165355635 --- test/7.936723430806942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #108 is: train/0.6755061592176121 --- test/7.8829285512764065\n",
      "Loss after epoch #109 is: train/0.673516692174917 --- test/7.844030822469455\n",
      "Loss after epoch #110 is: train/0.6713700776930326 --- test/7.870049347705512\n",
      "Loss after epoch #111 is: train/0.6696588702921276 --- test/7.8687770352119415\n",
      "Loss after epoch #112 is: train/0.6731633866515647 --- test/7.835823219201701\n",
      "Loss after epoch #113 is: train/0.6696861890269525 --- test/7.8319129280739\n",
      "Loss after epoch #114 is: train/0.6694313494358107 --- test/7.826331753395051\n",
      "Loss after epoch #115 is: train/0.668676430565846 --- test/7.788522101446922\n",
      "Loss after epoch #116 is: train/0.6650576017231907 --- test/7.777312438835566\n",
      "Loss after epoch #117 is: train/0.6651500707711294 --- test/7.783354037564241\n",
      "Loss after epoch #118 is: train/0.6626649544207737 --- test/7.748551582446674\n",
      "Loss after epoch #119 is: train/0.6632279258595248 --- test/7.767780979369519\n",
      "Loss after epoch #120 is: train/0.6640764078181978 --- test/7.828649334043086\n",
      "Loss after epoch #121 is: train/0.6635592151394606 --- test/7.828723845611646\n",
      "Loss after epoch #122 is: train/0.6628805895347623 --- test/7.85788706598408\n",
      "Loss after epoch #123 is: train/0.6610959060858626 --- test/7.827606669710574\n",
      "Loss after epoch #124 is: train/0.6580655904549451 --- test/7.8440555690161125\n",
      "Loss after epoch #125 is: train/0.6589847494284965 --- test/7.871838202182739\n",
      "Loss after epoch #126 is: train/0.6574744665795407 --- test/7.860530207906554\n",
      "Loss after epoch #127 is: train/0.6555683731892865 --- test/7.9108131161503135\n",
      "Loss after epoch #128 is: train/0.6546274645377056 --- test/7.8858161070612764\n",
      "Loss after epoch #129 is: train/0.6528615817028075 --- test/7.871093284254693\n",
      "Loss after epoch #130 is: train/0.6522323185716542 --- test/7.833193390832048\n",
      "Loss after epoch #131 is: train/0.6512753228350889 --- test/7.76873176676602\n",
      "Loss after epoch #132 is: train/0.6513584488625266 --- test/7.820269875988279\n",
      "Loss after epoch #133 is: train/0.6513700520917763 --- test/7.862125342900378\n",
      "Loss after epoch #134 is: train/0.6483973184840632 --- test/7.866363863605269\n",
      "Loss after epoch #135 is: train/0.6485328817120984 --- test/7.834607740620723\n",
      "Loss after epoch #136 is: train/0.6487260120886892 --- test/7.818793805107058\n",
      "Loss after epoch #137 is: train/0.647325831511745 --- test/7.789469573264098\n",
      "Loss after epoch #138 is: train/0.6471428377841166 --- test/7.791845415943139\n",
      "Loss after epoch #139 is: train/0.6469267838221343 --- test/7.772485051252036\n",
      "Loss after epoch #140 is: train/0.6448556611883844 --- test/7.803366368032273\n",
      "Loss after epoch #141 is: train/0.6426854197275301 --- test/7.843410500830448\n",
      "Loss after epoch #142 is: train/0.63991719751781 --- test/7.897658269408994\n",
      "Loss after epoch #143 is: train/0.6377821421870679 --- test/7.83713663246279\n",
      "Loss after epoch #144 is: train/0.6384308191533882 --- test/7.806526661968197\n",
      "Loss after epoch #145 is: train/0.64130510247056 --- test/7.871712567353676\n",
      "Loss after epoch #146 is: train/0.6371888475422709 --- test/7.8305428684649305\n",
      "Loss after epoch #147 is: train/0.6371261334189847 --- test/7.82564811998522\n",
      "Loss after epoch #148 is: train/0.6328634814688683 --- test/7.765354560439118\n",
      "Loss after epoch #149 is: train/0.6345514603231898 --- test/7.726031910383408\n",
      "Loss after epoch #150 is: train/0.6297777212216487 --- test/7.679667151512799\n",
      "Loss after epoch #151 is: train/0.6272028012153934 --- test/7.721114473456821\n",
      "Loss after epoch #152 is: train/0.6264012055001733 --- test/7.731635058886569\n",
      "Loss after epoch #153 is: train/0.6258266382573355 --- test/7.760103351426477\n",
      "Loss after epoch #154 is: train/0.6281856774505806 --- test/7.762464270045651\n",
      "Loss after epoch #155 is: train/0.6252263963776871 --- test/7.781406234348294\n",
      "Loss after epoch #156 is: train/0.624405640789775 --- test/7.799520527711227\n",
      "Loss after epoch #157 is: train/0.6255887134185091 --- test/7.8043660474844625\n",
      "Loss after epoch #158 is: train/0.6258512675909353 --- test/7.828926186873277\n",
      "Loss after epoch #159 is: train/0.626535735071952 --- test/7.834931615369446\n",
      "Loss after epoch #160 is: train/0.6269604101615989 --- test/7.8908356954032195\n",
      "Loss after epoch #161 is: train/0.6231364280072647 --- test/7.804704020773924\n",
      "Loss after epoch #162 is: train/0.6182730819678832 --- test/7.7934845938651565\n",
      "Loss after epoch #163 is: train/0.6160245766816131 --- test/7.731589686521907\n",
      "Loss after epoch #164 is: train/0.6141149988794478 --- test/7.782747456449915\n",
      "Loss after epoch #165 is: train/0.6147907317007661 --- test/7.765138739442177\n",
      "Loss after epoch #166 is: train/0.6156750031839185 --- test/7.75757129136365\n",
      "Loss after epoch #167 is: train/0.6123164406314539 --- test/7.738775976414812\n",
      "Loss after epoch #168 is: train/0.6118554548241011 --- test/7.731157174339107\n",
      "Loss after epoch #169 is: train/0.6130165860701607 --- test/7.750819802976837\n",
      "Loss after epoch #170 is: train/0.6125668387963681 --- test/7.763392763555449\n",
      "Loss after epoch #171 is: train/0.612416653523539 --- test/7.757308914327918\n",
      "Loss after epoch #172 is: train/0.6131425943432697 --- test/7.689009533465185\n",
      "Loss after epoch #173 is: train/0.6103737242311592 --- test/7.685970318476467\n",
      "Loss after epoch #174 is: train/0.6082544542482025 --- test/7.678712111736633\n",
      "Loss after epoch #175 is: train/0.6056364411123343 --- test/7.716727118890287\n",
      "Loss after epoch #176 is: train/0.6016471313851035 --- test/7.682387885674077\n",
      "Loss after epoch #177 is: train/0.6056088155831589 --- test/7.750345233685296\n",
      "Loss after epoch #178 is: train/0.6075572440099597 --- test/7.800922552806982\n",
      "Loss after epoch #179 is: train/0.6057685560856401 --- test/7.831677420200632\n",
      "Loss after epoch #180 is: train/0.6046247878615049 --- test/7.901622339660523\n",
      "Loss after epoch #181 is: train/0.6011334057614376 --- test/7.9096682395616\n",
      "Loss after epoch #182 is: train/0.5975933012525404 --- test/7.845453278669472\n",
      "Loss after epoch #183 is: train/0.5966609970067718 --- test/7.857216792008903\n",
      "Loss after epoch #184 is: train/0.5930807288104467 --- test/7.78449176770448\n",
      "Loss after epoch #185 is: train/0.5938285489081832 --- test/7.8623271861241095\n",
      "Loss after epoch #186 is: train/0.5910663567090485 --- test/7.8345226719054315\n",
      "Loss after epoch #187 is: train/0.5896830396577546 --- test/7.83392334751681\n",
      "Loss after epoch #188 is: train/0.5896732140485861 --- test/7.7992218610561945\n",
      "Loss after epoch #189 is: train/0.5862679993639013 --- test/7.759699892185441\n",
      "Loss after epoch #190 is: train/0.5837148173645728 --- test/7.688912182290452\n",
      "Loss after epoch #191 is: train/0.5827640328838757 --- test/7.626389920823226\n",
      "Loss after epoch #192 is: train/0.5821853237547325 --- test/7.639541628326343\n",
      "Loss after epoch #193 is: train/0.5803937725144843 --- test/7.686140495977402\n",
      "Loss after epoch #194 is: train/0.5799094198444515 --- test/7.659712497921752\n",
      "Loss after epoch #195 is: train/0.5793784811554615 --- test/7.737372995293403\n",
      "Loss after epoch #196 is: train/0.5781598756074915 --- test/7.714544058631735\n",
      "Loss after epoch #197 is: train/0.5781804876373843 --- test/7.7277658766337005\n",
      "Loss after epoch #198 is: train/0.5746671120062933 --- test/7.659143237163225\n",
      "Loss after epoch #199 is: train/0.5713948155614501 --- test/7.604565114875189\n",
      "Loss after epoch #200 is: train/0.5741670055512933 --- test/7.6636606603834085\n",
      "Loss after epoch #201 is: train/0.5712062867482057 --- test/7.6445727431667905\n",
      "Loss after epoch #202 is: train/0.5713195476028063 --- test/7.6745726396635865\n",
      "Loss after epoch #203 is: train/0.5687530391847091 --- test/7.674145539946885\n",
      "Loss after epoch #204 is: train/0.5652078459108338 --- test/7.680073209016966\n",
      "Loss after epoch #205 is: train/0.5635276290222184 --- test/7.653159118751016\n",
      "Loss after epoch #206 is: train/0.564252773809813 --- test/7.595396459361438\n",
      "Loss after epoch #207 is: train/0.5658487491002424 --- test/7.643520041501782\n",
      "Loss after epoch #208 is: train/0.5650657019526929 --- test/7.606361066540288\n",
      "Loss after epoch #209 is: train/0.5637431555410776 --- test/7.637106158547557\n",
      "Loss after epoch #210 is: train/0.5627234409508598 --- test/7.606908504443671\n",
      "Loss after epoch #211 is: train/0.5618908126085647 --- test/7.580878172845937\n",
      "Loss after epoch #212 is: train/0.5609727105417307 --- test/7.544124259843473\n",
      "Loss after epoch #213 is: train/0.5587875058869542 --- test/7.480302422095064\n",
      "Loss after epoch #214 is: train/0.5578202452717734 --- test/7.503532699584288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #215 is: train/0.5535840541866811 --- test/7.538267677492518\n",
      "Loss after epoch #216 is: train/0.5519372204096931 --- test/7.562240932480807\n",
      "Loss after epoch #217 is: train/0.5546128888856622 --- test/7.530374441709383\n",
      "Loss after epoch #218 is: train/0.5527876715130643 --- test/7.557490795716512\n",
      "Loss after epoch #219 is: train/0.5506724065697186 --- test/7.5232038162921\n",
      "Loss after epoch #220 is: train/0.5493333190568814 --- test/7.548469865371959\n",
      "Loss after epoch #221 is: train/0.5486887594523876 --- test/7.513670153948848\n",
      "Loss after epoch #222 is: train/0.5467438831786413 --- test/7.495872131188925\n",
      "Loss after epoch #223 is: train/0.5461468121437925 --- test/7.541812144850681\n",
      "Loss after epoch #224 is: train/0.5473952243517306 --- test/7.477811136465506\n",
      "Loss after epoch #225 is: train/0.5446683069796522 --- test/7.52048582615829\n",
      "Loss after epoch #226 is: train/0.5448538654188022 --- test/7.531311634051237\n",
      "Loss after epoch #227 is: train/0.5417617343397271 --- test/7.510862865442784\n",
      "Loss after epoch #228 is: train/0.5401876403102976 --- test/7.515917774995575\n",
      "Loss after epoch #229 is: train/0.5366816616597488 --- test/7.483962964498921\n",
      "Loss after epoch #230 is: train/0.5365576686948277 --- test/7.518737243802478\n",
      "Loss after epoch #231 is: train/0.5356014088873177 --- test/7.529863784630621\n",
      "Loss after epoch #232 is: train/0.5360992700615984 --- test/7.538590859770414\n",
      "Loss after epoch #233 is: train/0.5342728788266445 --- test/7.5579309653964355\n",
      "Loss after epoch #234 is: train/0.532748616432966 --- test/7.526120666850394\n",
      "Loss after epoch #235 is: train/0.5307571050031157 --- test/7.521311918894285\n",
      "Loss after epoch #236 is: train/0.5281392950149765 --- test/7.459651685937845\n",
      "Loss after epoch #237 is: train/0.5273273082346709 --- test/7.468320224822019\n",
      "Loss after epoch #238 is: train/0.5254709313530574 --- test/7.477468780099037\n",
      "Loss after epoch #239 is: train/0.5217291542191782 --- test/7.46226869259483\n",
      "Loss after epoch #240 is: train/0.5241896896168425 --- test/7.530341704923878\n",
      "Loss after epoch #241 is: train/0.5237116766457389 --- test/7.57889556057716\n",
      "Loss after epoch #242 is: train/0.5230436608144238 --- test/7.618494360332233\n",
      "Loss after epoch #243 is: train/0.5224870155328096 --- test/7.647928037618335\n",
      "Loss after epoch #244 is: train/0.5202793820449265 --- test/7.569086520135406\n",
      "Loss after epoch #245 is: train/0.5207655291899584 --- test/7.546618245569045\n",
      "Loss after epoch #246 is: train/0.5195481482612337 --- test/7.559337374346545\n",
      "Loss after epoch #247 is: train/0.5163162945646145 --- test/7.494879784935609\n",
      "Loss after epoch #248 is: train/0.5130127965417587 --- test/7.460325810036884\n",
      "Loss after epoch #249 is: train/0.5130527392915214 --- test/7.505718498148679\n",
      "Loss after epoch #250 is: train/0.5097019609511514 --- test/7.442968409092754\n",
      "Loss after epoch #251 is: train/0.5097542769987455 --- test/7.388932822854432\n",
      "Loss after epoch #252 is: train/0.5078228290114115 --- test/7.445246669408281\n",
      "Loss after epoch #253 is: train/0.5082556615167761 --- test/7.488965758965873\n",
      "Loss after epoch #254 is: train/0.5050105400019436 --- test/7.4155407034799214\n",
      "Loss after epoch #255 is: train/0.501320827525492 --- test/7.375926454333429\n",
      "Loss after epoch #256 is: train/0.501116633742695 --- test/7.395357440410661\n",
      "Loss after epoch #257 is: train/0.5031820507336859 --- test/7.413580655843019\n",
      "Loss after epoch #258 is: train/0.49920273946162236 --- test/7.367136623143344\n",
      "Loss after epoch #259 is: train/0.4989728193324983 --- test/7.395806336796\n",
      "Loss after epoch #260 is: train/0.49726829642046677 --- test/7.379120200464162\n",
      "Loss after epoch #261 is: train/0.49601342372413837 --- test/7.400789148447144\n",
      "Loss after epoch #262 is: train/0.4980271688500952 --- test/7.465384680089697\n",
      "Loss after epoch #263 is: train/0.4975682631125108 --- test/7.521896163581945\n",
      "Loss after epoch #264 is: train/0.49542795908044135 --- test/7.506382194513589\n",
      "Loss after epoch #265 is: train/0.49329067428018336 --- test/7.47853270673101\n",
      "Loss after epoch #266 is: train/0.4944789053809903 --- test/7.465583150209838\n",
      "Loss after epoch #267 is: train/0.4900110954469928 --- test/7.424831675457667\n",
      "Loss after epoch #268 is: train/0.4876191578755834 --- test/7.374579276760306\n",
      "Loss after epoch #269 is: train/0.4882642316672197 --- test/7.343306169950656\n",
      "Loss after epoch #270 is: train/0.48634496247626696 --- test/7.320396810036545\n",
      "Loss after epoch #271 is: train/0.4842561322543662 --- test/7.311924514601613\n",
      "Loss after epoch #272 is: train/0.4803665185930427 --- test/7.261541394670997\n",
      "Loss after epoch #273 is: train/0.4796473964605536 --- test/7.181333155995338\n",
      "Loss after epoch #274 is: train/0.47792854970683846 --- test/7.27795768845835\n",
      "Loss after epoch #275 is: train/0.47584289640393684 --- test/7.261988394062372\n",
      "Loss after epoch #276 is: train/0.4741649756564358 --- test/7.191617911750663\n",
      "Loss after epoch #277 is: train/0.47448069884119404 --- test/7.210275649954879\n",
      "Loss after epoch #278 is: train/0.47271332595981 --- test/7.225159926442992\n",
      "Loss after epoch #279 is: train/0.471065829337701 --- test/7.177506839069774\n",
      "Loss after epoch #280 is: train/0.4692433289834847 --- test/7.197735974053453\n",
      "Loss after epoch #281 is: train/0.46969288474802423 --- test/7.208288721120795\n",
      "Loss after epoch #282 is: train/0.4708795485027683 --- test/7.214697873718699\n",
      "Loss after epoch #283 is: train/0.4681658056243226 --- test/7.172557953367937\n",
      "Loss after epoch #284 is: train/0.4681720565529454 --- test/7.197828719671509\n",
      "Loss after epoch #285 is: train/0.46482856767862285 --- test/7.141442700431727\n",
      "Loss after epoch #286 is: train/0.4646181846233119 --- test/7.1476207391517725\n",
      "Loss after epoch #287 is: train/0.4643056743147951 --- test/7.180599216754694\n",
      "Loss after epoch #288 is: train/0.46152777867241657 --- test/7.166424598107358\n",
      "Loss after epoch #289 is: train/0.45869393222610827 --- test/7.113868034487826\n",
      "Loss after epoch #290 is: train/0.45555025786010106 --- test/7.088639334054138\n",
      "Loss after epoch #291 is: train/0.45220323134327745 --- test/7.045692474691501\n",
      "Loss after epoch #292 is: train/0.4522730034708726 --- test/7.083703485480803\n",
      "Loss after epoch #293 is: train/0.4518540330352833 --- test/7.08554598141171\n",
      "Loss after epoch #294 is: train/0.45033426518031916 --- test/7.056805111347451\n",
      "Loss after epoch #295 is: train/0.44945038970141166 --- test/7.092761838455541\n",
      "Loss after epoch #296 is: train/0.44792889896596894 --- test/7.130145143318176\n",
      "Loss after epoch #297 is: train/0.44930494586454106 --- test/7.109227203619221\n",
      "Loss after epoch #298 is: train/0.4518471477795438 --- test/7.100509513327003\n",
      "Loss after epoch #299 is: train/0.4481599681732972 --- test/7.076423954110285\n",
      "Loss after epoch #300 is: train/0.4463493333622021 --- test/7.050159975467467\n",
      "Loss after epoch #301 is: train/0.4441102151544707 --- test/7.028852546006867\n",
      "Loss after epoch #302 is: train/0.4401464514881031 --- test/7.041354940906417\n",
      "Loss after epoch #303 is: train/0.4377220995033807 --- test/7.0066920095532845\n",
      "Loss after epoch #304 is: train/0.4382540565770406 --- test/6.973940397417065\n",
      "Loss after epoch #305 is: train/0.4382353957624327 --- test/6.9302870843720585\n",
      "Loss after epoch #306 is: train/0.43631070227491425 --- test/6.929952176477244\n",
      "Loss after epoch #307 is: train/0.4335303281159545 --- test/6.897988587863467\n",
      "Loss after epoch #308 is: train/0.4312668578755307 --- test/6.886589845302039\n",
      "Loss after epoch #309 is: train/0.4300214978845614 --- test/6.88536650751368\n",
      "Loss after epoch #310 is: train/0.42909452684968336 --- test/6.802072604107158\n",
      "Loss after epoch #311 is: train/0.42731928702302824 --- test/6.793514919555332\n",
      "Loss after epoch #312 is: train/0.4253612041980228 --- test/6.752363434341585\n",
      "Loss after epoch #313 is: train/0.42480724218638616 --- test/6.746331540368583\n",
      "Loss after epoch #314 is: train/0.4236146944902848 --- test/6.80489752164743\n",
      "Loss after epoch #315 is: train/0.4231939044430036 --- test/6.80122199261097\n",
      "Loss after epoch #316 is: train/0.4193570488637085 --- test/6.825979584723819\n",
      "Loss after epoch #317 is: train/0.4186501457354114 --- test/6.857610456276666\n",
      "Loss after epoch #318 is: train/0.4166816452888452 --- test/6.804861702935809\n",
      "Loss after epoch #319 is: train/0.41590192281921085 --- test/6.8202342325409875\n",
      "Loss after epoch #320 is: train/0.4155883885826802 --- test/6.837095057720374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #321 is: train/0.4128762161296444 --- test/6.831686167289141\n",
      "Loss after epoch #322 is: train/0.41310685883694975 --- test/6.858662368301045\n",
      "Loss after epoch #323 is: train/0.41117766242322373 --- test/6.878120735242104\n",
      "Loss after epoch #324 is: train/0.41009696094601256 --- test/6.903627251653182\n",
      "Loss after epoch #325 is: train/0.408496172386172 --- test/6.864619803997911\n",
      "Loss after epoch #326 is: train/0.40604547678039166 --- test/6.824913863333809\n",
      "Loss after epoch #327 is: train/0.40487373109562025 --- test/6.777341428645138\n",
      "Loss after epoch #328 is: train/0.40310159096647974 --- test/6.75186376797344\n",
      "Loss after epoch #329 is: train/0.4033201054211219 --- test/6.740194929219286\n",
      "Loss after epoch #330 is: train/0.4010451669972374 --- test/6.704403520563203\n",
      "Loss after epoch #331 is: train/0.40108844783883474 --- test/6.765897834601932\n",
      "Loss after epoch #332 is: train/0.3995507783261224 --- test/6.7618621761387425\n",
      "Loss after epoch #333 is: train/0.39971076279146145 --- test/6.73308207810981\n",
      "Loss after epoch #334 is: train/0.39704258220399713 --- test/6.697733856932662\n",
      "Loss after epoch #335 is: train/0.39474013511802336 --- test/6.688224596094624\n",
      "Loss after epoch #336 is: train/0.39556477592346706 --- test/6.6917707145524385\n",
      "Loss after epoch #337 is: train/0.392356126749674 --- test/6.650443288397127\n",
      "Loss after epoch #338 is: train/0.3939123469149766 --- test/6.633795442958727\n",
      "Loss after epoch #339 is: train/0.3891275071539693 --- test/6.594869796804228\n",
      "Loss after epoch #340 is: train/0.3903376640363229 --- test/6.615697812379737\n",
      "Loss after epoch #341 is: train/0.3895823741633322 --- test/6.634265300442013\n",
      "Loss after epoch #342 is: train/0.3889263339099258 --- test/6.664974583666349\n",
      "Loss after epoch #343 is: train/0.3877226333095158 --- test/6.635464453761714\n",
      "Loss after epoch #344 is: train/0.3857783129162244 --- test/6.604925350195221\n",
      "Loss after epoch #345 is: train/0.3816991072594304 --- test/6.585721960100993\n",
      "Loss after epoch #346 is: train/0.3797987641802255 --- test/6.586497304567517\n",
      "Loss after epoch #347 is: train/0.3785547849214622 --- test/6.5667655619278475\n",
      "Loss after epoch #348 is: train/0.37650385732621616 --- test/6.529820516884031\n",
      "Loss after epoch #349 is: train/0.3734477549601974 --- test/6.495682663086115\n",
      "Loss after epoch #350 is: train/0.37305566751465247 --- test/6.5241012982058635\n",
      "Loss after epoch #351 is: train/0.3718017994181344 --- test/6.491811714995868\n",
      "Loss after epoch #352 is: train/0.3709463533440193 --- test/6.50740270299981\n",
      "Loss after epoch #353 is: train/0.37021588370034053 --- test/6.5203799607630994\n",
      "Loss after epoch #354 is: train/0.3684445016498403 --- test/6.484594289901917\n",
      "Loss after epoch #355 is: train/0.36796905800176355 --- test/6.538628720521564\n",
      "Loss after epoch #356 is: train/0.3676926949214974 --- test/6.568051050311631\n",
      "Loss after epoch #357 is: train/0.36579665922359816 --- test/6.545057203717251\n",
      "Loss after epoch #358 is: train/0.3654220579362001 --- test/6.6066664499662044\n",
      "Loss after epoch #359 is: train/0.3636819331551521 --- test/6.580707218589446\n",
      "Loss after epoch #360 is: train/0.3618872444377459 --- test/6.525537251935453\n",
      "Loss after epoch #361 is: train/0.3606917058254451 --- test/6.564643064894332\n",
      "Loss after epoch #362 is: train/0.3585412207195562 --- test/6.482701435279778\n",
      "Loss after epoch #363 is: train/0.3592176242628586 --- test/6.463440839305312\n",
      "Loss after epoch #364 is: train/0.3600518018926544 --- test/6.425520010225589\n",
      "Loss after epoch #365 is: train/0.3595346830713222 --- test/6.48189356599304\n",
      "Loss after epoch #366 is: train/0.35779318244386277 --- test/6.485804511206427\n",
      "Loss after epoch #367 is: train/0.3558562826881092 --- test/6.438848233158357\n",
      "Loss after epoch #368 is: train/0.35392738187908906 --- test/6.421819842439894\n",
      "Loss after epoch #369 is: train/0.35414237499039536 --- test/6.456776095844618\n",
      "Loss after epoch #370 is: train/0.3514454302917065 --- test/6.393200836563027\n",
      "Loss after epoch #371 is: train/0.35069355619949094 --- test/6.352141929352775\n",
      "Loss after epoch #372 is: train/0.34667165609201733 --- test/6.324522493655564\n",
      "Loss after epoch #373 is: train/0.34584904693860796 --- test/6.306280228729974\n",
      "Loss after epoch #374 is: train/0.3444024269232419 --- test/6.349924736046997\n",
      "Loss after epoch #375 is: train/0.3438551486863097 --- test/6.289828115111917\n",
      "Loss after epoch #376 is: train/0.34362833497515316 --- test/6.291524519204729\n",
      "Loss after epoch #377 is: train/0.34202009279973666 --- test/6.312504171061755\n",
      "Loss after epoch #378 is: train/0.33917692494290363 --- test/6.2815830616469475\n",
      "Loss after epoch #379 is: train/0.3381868237717854 --- test/6.300743615521895\n",
      "Loss after epoch #380 is: train/0.334801356828089 --- test/6.29176028510642\n",
      "Loss after epoch #381 is: train/0.3330129280296946 --- test/6.259283935198777\n",
      "Loss after epoch #382 is: train/0.33393318557615964 --- test/6.278468509226354\n",
      "Loss after epoch #383 is: train/0.3321673342738917 --- test/6.27822516904351\n",
      "Loss after epoch #384 is: train/0.3300737751391042 --- test/6.276927543963625\n",
      "Loss after epoch #385 is: train/0.32997692484906815 --- test/6.204767633001367\n",
      "Loss after epoch #386 is: train/0.3282453998268049 --- test/6.1723314985221105\n",
      "Loss after epoch #387 is: train/0.3252830188415663 --- test/6.124946072139544\n",
      "Loss after epoch #388 is: train/0.32458005870547174 --- test/6.088288215322945\n",
      "Loss after epoch #389 is: train/0.3244609893771985 --- test/6.071268614563502\n",
      "Loss after epoch #390 is: train/0.321540024233451 --- test/6.047895765708818\n",
      "Loss after epoch #391 is: train/0.32151223889061525 --- test/6.025523978190236\n",
      "Loss after epoch #392 is: train/0.31988697984335723 --- test/6.06341660246248\n",
      "Loss after epoch #393 is: train/0.31833731825826056 --- test/6.040955484924993\n",
      "Loss after epoch #394 is: train/0.31884895923623807 --- test/6.074766649834987\n",
      "Loss after epoch #395 is: train/0.31834018502671446 --- test/6.063249486072933\n",
      "Loss after epoch #396 is: train/0.31725343890696456 --- test/6.059663367239717\n",
      "Loss after epoch #397 is: train/0.31562179011129854 --- test/6.0354748887183405\n",
      "Loss after epoch #398 is: train/0.3146998843747624 --- test/6.034138161829446\n",
      "Loss after epoch #399 is: train/0.31252949318609136 --- test/6.057407978733971\n",
      "Loss after epoch #400 is: train/0.3115423352060863 --- test/6.009425804667418\n",
      "Loss after epoch #401 is: train/0.31077763221256377 --- test/6.017162337915066\n",
      "Loss after epoch #402 is: train/0.3116139060005315 --- test/6.03262008749594\n",
      "Loss after epoch #403 is: train/0.3102057698316895 --- test/5.978316892614582\n",
      "Loss after epoch #404 is: train/0.30905014023427285 --- test/5.944949277262068\n",
      "Loss after epoch #405 is: train/0.3075909852705412 --- test/5.941599241038856\n",
      "Loss after epoch #406 is: train/0.30528261862503064 --- test/5.98596012309555\n",
      "Loss after epoch #407 is: train/0.30404556549206346 --- test/5.963167998497521\n",
      "Loss after epoch #408 is: train/0.30295501344030473 --- test/5.942112090202163\n",
      "Loss after epoch #409 is: train/0.30332681996587174 --- test/5.9471606795167\n",
      "Loss after epoch #410 is: train/0.3013510496097275 --- test/5.916357047088755\n",
      "Loss after epoch #411 is: train/0.2986655448714895 --- test/5.855426675748641\n",
      "Loss after epoch #412 is: train/0.29909183665058797 --- test/5.889292213090622\n",
      "Loss after epoch #413 is: train/0.2978641358960611 --- test/5.8672669020829655\n",
      "Loss after epoch #414 is: train/0.2939685297586667 --- test/5.859255739656663\n",
      "Loss after epoch #415 is: train/0.2927211259092085 --- test/5.824534180681983\n",
      "Loss after epoch #416 is: train/0.29210224734792867 --- test/5.8757223158038965\n",
      "Loss after epoch #417 is: train/0.2907516157733385 --- test/5.843456646399739\n",
      "Loss after epoch #418 is: train/0.28929561247535535 --- test/5.868275585815845\n",
      "Loss after epoch #419 is: train/0.28784492395711714 --- test/5.829595132842626\n",
      "Loss after epoch #420 is: train/0.28695962182348556 --- test/5.826784406350208\n",
      "Loss after epoch #421 is: train/0.2874079395374201 --- test/5.802671756360556\n",
      "Loss after epoch #422 is: train/0.28673960262697623 --- test/5.805314883905817\n",
      "Loss after epoch #423 is: train/0.2850518838002384 --- test/5.80383666891029\n",
      "Loss after epoch #424 is: train/0.2861323061933714 --- test/5.841001721285262\n",
      "Loss after epoch #425 is: train/0.28432627029201646 --- test/5.813301630805562\n",
      "Loss after epoch #426 is: train/0.28106507048683627 --- test/5.80132217580591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #427 is: train/0.27928575375356735 --- test/5.765602631612991\n",
      "Loss after epoch #428 is: train/0.27913237542948344 --- test/5.772682868565943\n",
      "Loss after epoch #429 is: train/0.27760757643238043 --- test/5.735280381543492\n",
      "Loss after epoch #430 is: train/0.2756097610940963 --- test/5.7120196220314154\n",
      "Loss after epoch #431 is: train/0.2757924183844091 --- test/5.696263629256145\n",
      "Loss after epoch #432 is: train/0.2755749925309969 --- test/5.720560406349769\n",
      "Loss after epoch #433 is: train/0.2741857825899457 --- test/5.727956776533479\n",
      "Loss after epoch #434 is: train/0.27390268225461256 --- test/5.7693322786426435\n",
      "Loss after epoch #435 is: train/0.27265542463130094 --- test/5.737449047215176\n",
      "Loss after epoch #436 is: train/0.27098943023404215 --- test/5.677523457931996\n",
      "Loss after epoch #437 is: train/0.2714174172833569 --- test/5.644457879900246\n",
      "Loss after epoch #438 is: train/0.26996440711337666 --- test/5.627059314049848\n",
      "Loss after epoch #439 is: train/0.2681634198878513 --- test/5.562896991493081\n",
      "Loss after epoch #440 is: train/0.2683227785504759 --- test/5.555194357720173\n",
      "Loss after epoch #441 is: train/0.26790628391563776 --- test/5.573550403338078\n",
      "Loss after epoch #442 is: train/0.2662662945730221 --- test/5.589602672773608\n",
      "Loss after epoch #443 is: train/0.2653075407098219 --- test/5.560291281777286\n",
      "Loss after epoch #444 is: train/0.26496652001152765 --- test/5.596972066990667\n",
      "Loss after epoch #445 is: train/0.2627972214497293 --- test/5.627612048378798\n",
      "Loss after epoch #446 is: train/0.26298077476738213 --- test/5.614439863526403\n",
      "Loss after epoch #447 is: train/0.2613881674925433 --- test/5.6098950505600795\n",
      "Loss after epoch #448 is: train/0.25987586755209563 --- test/5.598103503848095\n",
      "Loss after epoch #449 is: train/0.2589393570473436 --- test/5.555102398208033\n",
      "Loss after epoch #450 is: train/0.2578929104831741 --- test/5.513102263976302\n",
      "Loss after epoch #451 is: train/0.25737573383587453 --- test/5.498120849332905\n",
      "Loss after epoch #452 is: train/0.256644755848114 --- test/5.475375449935491\n",
      "Loss after epoch #453 is: train/0.25521322756150505 --- test/5.486368801307932\n",
      "Loss after epoch #454 is: train/0.2542601490476973 --- test/5.472652466223882\n",
      "Loss after epoch #455 is: train/0.25271239751817054 --- test/5.460689549945565\n",
      "Loss after epoch #456 is: train/0.25186597553165346 --- test/5.453264130150079\n",
      "Loss after epoch #457 is: train/0.2514803616831404 --- test/5.444667629121289\n",
      "Loss after epoch #458 is: train/0.2510984164550528 --- test/5.467550377713142\n",
      "Loss after epoch #459 is: train/0.25013078501971775 --- test/5.4627695704555865\n",
      "Loss after epoch #460 is: train/0.24983063299575875 --- test/5.452741633094977\n",
      "Loss after epoch #461 is: train/0.24739080971561692 --- test/5.422782025189795\n",
      "Loss after epoch #462 is: train/0.24628533580729584 --- test/5.436611532289885\n",
      "Loss after epoch #463 is: train/0.244501580782586 --- test/5.423099990639523\n",
      "Loss after epoch #464 is: train/0.24346449548381421 --- test/5.383602943122711\n",
      "Loss after epoch #465 is: train/0.24258843116473303 --- test/5.410883849886567\n",
      "Loss after epoch #466 is: train/0.24222680337277003 --- test/5.457220260832295\n",
      "Loss after epoch #467 is: train/0.24168525591220477 --- test/5.413326679826399\n",
      "Loss after epoch #468 is: train/0.23970913018379872 --- test/5.438042815633438\n",
      "Loss after epoch #469 is: train/0.238832552018558 --- test/5.422194403461539\n",
      "Loss after epoch #470 is: train/0.23747614501702677 --- test/5.442733372251399\n",
      "Loss after epoch #471 is: train/0.23704002150815737 --- test/5.404572981966183\n",
      "Loss after epoch #472 is: train/0.23655025978710253 --- test/5.366184838587832\n",
      "Loss after epoch #473 is: train/0.23496207723933513 --- test/5.375093028162235\n",
      "Loss after epoch #474 is: train/0.233830798357625 --- test/5.365585064954139\n",
      "Loss after epoch #475 is: train/0.23355187068076155 --- test/5.374879408531167\n",
      "Loss after epoch #476 is: train/0.23336607409441587 --- test/5.360708697013598\n",
      "Loss after epoch #477 is: train/0.23305823740380188 --- test/5.339037966990872\n",
      "Loss after epoch #478 is: train/0.23140139400534207 --- test/5.3281483235218365\n",
      "Loss after epoch #479 is: train/0.23085224281270295 --- test/5.315018248181766\n",
      "Loss after epoch #480 is: train/0.2305490123471422 --- test/5.310871897151049\n",
      "Loss after epoch #481 is: train/0.2288726720786258 --- test/5.2613721390874755\n",
      "Loss after epoch #482 is: train/0.22766755478420514 --- test/5.275419954081195\n",
      "Loss after epoch #483 is: train/0.22676781112547967 --- test/5.271650886440363\n",
      "Loss after epoch #484 is: train/0.22614991155194034 --- test/5.249633500361462\n",
      "Loss after epoch #485 is: train/0.22571504302215661 --- test/5.240852444729571\n",
      "Loss after epoch #486 is: train/0.2247522754790901 --- test/5.230719928722112\n",
      "Loss after epoch #487 is: train/0.22473586576406754 --- test/5.271331746995462\n",
      "Loss after epoch #488 is: train/0.2236134004931819 --- test/5.231235919504025\n",
      "Loss after epoch #489 is: train/0.2240020489406604 --- test/5.237382349395762\n",
      "Loss after epoch #490 is: train/0.2236459151703787 --- test/5.228145715622341\n",
      "Loss after epoch #491 is: train/0.22108693111197472 --- test/5.213609029817813\n",
      "Loss after epoch #492 is: train/0.21887684872742638 --- test/5.17956973654425\n",
      "Loss after epoch #493 is: train/0.21830535470800536 --- test/5.184146625179965\n",
      "Loss after epoch #494 is: train/0.21783669351707785 --- test/5.174373078899948\n",
      "Loss after epoch #495 is: train/0.21747589105040083 --- test/5.1672678565641394\n",
      "Loss after epoch #496 is: train/0.21828701872602466 --- test/5.17627519358276\n",
      "Loss after epoch #497 is: train/0.21709251877120647 --- test/5.133714872386825\n",
      "Loss after epoch #498 is: train/0.2159848337591404 --- test/5.12009000901701\n",
      "Loss after epoch #499 is: train/0.21449487066458534 --- test/5.093048617115623\n",
      "Loss after epoch #500 is: train/0.21261919954946262 --- test/5.089697575496153\n",
      "Loss after epoch #501 is: train/0.2126678977071408 --- test/5.056841331330674\n",
      "Loss after epoch #502 is: train/0.2114674871350019 --- test/5.031303464538932\n",
      "Loss after epoch #503 is: train/0.2106228394474773 --- test/5.024353870683689\n",
      "Loss after epoch #504 is: train/0.2115700026886828 --- test/5.039607816688532\n",
      "Loss after epoch #505 is: train/0.2107528683785982 --- test/5.043621849937741\n",
      "Loss after epoch #506 is: train/0.20939098096991313 --- test/5.043043905862883\n",
      "Loss after epoch #507 is: train/0.2091365419917934 --- test/5.06020124757824\n",
      "Loss after epoch #508 is: train/0.20852757301618627 --- test/5.0523739975043\n",
      "Loss after epoch #509 is: train/0.20770048459553805 --- test/5.007645518976868\n",
      "Loss after epoch #510 is: train/0.2075567387984713 --- test/5.0252244837978095\n",
      "Loss after epoch #511 is: train/0.20599296368315403 --- test/4.999331736692906\n",
      "Loss after epoch #512 is: train/0.20482753355619057 --- test/4.971987724391279\n",
      "Loss after epoch #513 is: train/0.2031752868118544 --- test/4.974607438594646\n",
      "Loss after epoch #514 is: train/0.20330405085887063 --- test/5.020345716686997\n",
      "Loss after epoch #515 is: train/0.20249963008860386 --- test/4.99253884413971\n",
      "Loss after epoch #516 is: train/0.201924615017744 --- test/4.989311998200046\n",
      "Loss after epoch #517 is: train/0.2007238496258533 --- test/4.952653220398047\n",
      "Loss after epoch #518 is: train/0.2006723395640006 --- test/4.974911663835387\n",
      "Loss after epoch #519 is: train/0.1996671080168673 --- test/4.954109811951722\n",
      "Loss after epoch #520 is: train/0.19912492263279635 --- test/4.935449627914874\n",
      "Loss after epoch #521 is: train/0.19786408828737653 --- test/4.911499405564521\n",
      "Loss after epoch #522 is: train/0.19591660148815263 --- test/4.884903783745984\n",
      "Loss after epoch #523 is: train/0.19523900406376646 --- test/4.905119273491039\n",
      "Loss after epoch #524 is: train/0.1949241771295436 --- test/4.860350289538293\n",
      "Loss after epoch #525 is: train/0.19434021220379355 --- test/4.875750479882268\n",
      "Loss after epoch #526 is: train/0.19451939427125303 --- test/4.8505566339378055\n",
      "Loss after epoch #527 is: train/0.19300557404579674 --- test/4.853641844650659\n",
      "Loss after epoch #528 is: train/0.19235898388659628 --- test/4.852530037276914\n",
      "Loss after epoch #529 is: train/0.19140357437346298 --- test/4.821165132446266\n",
      "Loss after epoch #530 is: train/0.19004327001822974 --- test/4.811368958729016\n",
      "Loss after epoch #531 is: train/0.189506699764604 --- test/4.790639486179188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #532 is: train/0.1895945714557758 --- test/4.811067997595037\n",
      "Loss after epoch #533 is: train/0.1884800327594273 --- test/4.778454779257922\n",
      "Loss after epoch #534 is: train/0.18692052715352833 --- test/4.783680513805257\n",
      "Loss after epoch #535 is: train/0.1872656387555611 --- test/4.7624539075655505\n",
      "Loss after epoch #536 is: train/0.18667358978727466 --- test/4.750511226912636\n",
      "Loss after epoch #537 is: train/0.18609295300736162 --- test/4.752204361422568\n",
      "Loss after epoch #538 is: train/0.18560781994810988 --- test/4.72878694525268\n",
      "Loss after epoch #539 is: train/0.18587349503919018 --- test/4.723395161207697\n",
      "Loss after epoch #540 is: train/0.1863193219891622 --- test/4.7258950483395825\n",
      "Loss after epoch #541 is: train/0.18534161917635028 --- test/4.721274217892663\n",
      "Loss after epoch #542 is: train/0.18476643202017393 --- test/4.703264171997251\n",
      "Loss after epoch #543 is: train/0.18413276534345752 --- test/4.6951159495481285\n",
      "Loss after epoch #544 is: train/0.18283593057732242 --- test/4.663440015374822\n",
      "Loss after epoch #545 is: train/0.18246780510975402 --- test/4.680131926590972\n",
      "Loss after epoch #546 is: train/0.1807935453507057 --- test/4.639671424179304\n",
      "Loss after epoch #547 is: train/0.1799003458098948 --- test/4.653789884288002\n",
      "Loss after epoch #548 is: train/0.18009572178442398 --- test/4.679284652325062\n",
      "Loss after epoch #549 is: train/0.17843286364194014 --- test/4.674210777310423\n",
      "Loss after epoch #550 is: train/0.17799569673090254 --- test/4.637571218674251\n",
      "Loss after epoch #551 is: train/0.17785051796971504 --- test/4.6542683963950555\n",
      "Loss after epoch #552 is: train/0.17714749041119524 --- test/4.630269647586277\n",
      "Loss after epoch #553 is: train/0.17616094304060476 --- test/4.633965613931473\n",
      "Loss after epoch #554 is: train/0.17586272239631853 --- test/4.647233063267976\n",
      "Loss after epoch #555 is: train/0.17459905199241205 --- test/4.605090790486715\n",
      "Loss after epoch #556 is: train/0.17342387893262814 --- test/4.594485979931331\n",
      "Loss after epoch #557 is: train/0.17340709643972443 --- test/4.588297884945916\n",
      "Loss after epoch #558 is: train/0.17301951118576062 --- test/4.590479083232815\n",
      "Loss after epoch #559 is: train/0.17137572211211802 --- test/4.578410513032532\n",
      "Loss after epoch #560 is: train/0.17165945585010914 --- test/4.603854259765836\n",
      "Loss after epoch #561 is: train/0.17158241131844265 --- test/4.611453022986809\n",
      "Loss after epoch #562 is: train/0.17104064860761287 --- test/4.594427904412749\n",
      "Loss after epoch #563 is: train/0.16957272113492114 --- test/4.570238228756394\n",
      "Loss after epoch #564 is: train/0.16977312246450368 --- test/4.557890810009507\n",
      "Loss after epoch #565 is: train/0.1692020684564564 --- test/4.551156423487247\n",
      "Loss after epoch #566 is: train/0.16762242707286729 --- test/4.52078503811894\n",
      "Loss after epoch #567 is: train/0.16721614235659563 --- test/4.51300819472327\n",
      "Loss after epoch #568 is: train/0.1670598234100344 --- test/4.504866355723014\n",
      "Loss after epoch #569 is: train/0.16553474801295004 --- test/4.488264208595064\n",
      "Loss after epoch #570 is: train/0.16526065712770358 --- test/4.4845005637482105\n",
      "Loss after epoch #571 is: train/0.16476269259861476 --- test/4.473960097851135\n",
      "Loss after epoch #572 is: train/0.16364182344531175 --- test/4.440425044385708\n",
      "Loss after epoch #573 is: train/0.16340932194893074 --- test/4.444739487742913\n",
      "Loss after epoch #574 is: train/0.16259533606996515 --- test/4.436972248138172\n",
      "Loss after epoch #575 is: train/0.16206923046011706 --- test/4.4338723892254945\n",
      "Loss after epoch #576 is: train/0.16119388008803054 --- test/4.435049136533657\n",
      "Loss after epoch #577 is: train/0.16085525059816805 --- test/4.412382977413281\n",
      "Loss after epoch #578 is: train/0.16041020363652667 --- test/4.402683248199261\n",
      "Loss after epoch #579 is: train/0.15966191554779277 --- test/4.406098698613308\n",
      "Loss after epoch #580 is: train/0.15868160101388343 --- test/4.409919181898613\n",
      "Loss after epoch #581 is: train/0.15819572257955847 --- test/4.398994432208443\n",
      "Loss after epoch #582 is: train/0.15678463539818557 --- test/4.36601927477921\n",
      "Loss after epoch #583 is: train/0.15668432429327925 --- test/4.343508050132851\n",
      "Loss after epoch #584 is: train/0.15570580542598011 --- test/4.333499954257345\n",
      "Loss after epoch #585 is: train/0.15583051242590615 --- test/4.335321344741421\n",
      "Loss after epoch #586 is: train/0.1551895049535842 --- test/4.340805066035033\n",
      "Loss after epoch #587 is: train/0.1547976406713406 --- test/4.342848798300047\n",
      "Loss after epoch #588 is: train/0.15454656532376457 --- test/4.330199182684238\n",
      "Loss after epoch #589 is: train/0.15384837636374699 --- test/4.323325814926595\n",
      "Loss after epoch #590 is: train/0.15305221156232182 --- test/4.324690084885281\n",
      "Loss after epoch #591 is: train/0.15202437524769702 --- test/4.300049014445354\n",
      "Loss after epoch #592 is: train/0.1516180966071423 --- test/4.286190348895886\n",
      "Loss after epoch #593 is: train/0.1504179689689731 --- test/4.279697972951554\n",
      "Loss after epoch #594 is: train/0.1500579590393127 --- test/4.280034092206014\n",
      "Loss after epoch #595 is: train/0.14991389196400398 --- test/4.281441159132731\n",
      "Loss after epoch #596 is: train/0.14861227062569243 --- test/4.245596907523877\n",
      "Loss after epoch #597 is: train/0.1481483081751095 --- test/4.235640079472103\n",
      "Loss after epoch #598 is: train/0.14828232322761875 --- test/4.219487638084049\n",
      "Loss after epoch #599 is: train/0.1482184915424228 --- test/4.21570037034055\n",
      "Loss after epoch #600 is: train/0.14766098769763863 --- test/4.192422651136921\n",
      "Loss after epoch #601 is: train/0.14652195431796253 --- test/4.207327562579731\n",
      "Loss after epoch #602 is: train/0.145713759694446 --- test/4.205094956361567\n",
      "Loss after epoch #603 is: train/0.14546426789283012 --- test/4.18524148011315\n",
      "Loss after epoch #604 is: train/0.1457364664902809 --- test/4.198424554958793\n",
      "Loss after epoch #605 is: train/0.14456957026917017 --- test/4.191965856322656\n",
      "Loss after epoch #606 is: train/0.14424936827742582 --- test/4.184993762465425\n",
      "Loss after epoch #607 is: train/0.1443727029379079 --- test/4.1714439960281045\n",
      "Loss after epoch #608 is: train/0.14364069860078207 --- test/4.166606260868443\n",
      "Loss after epoch #609 is: train/0.1431175239399365 --- test/4.182582723194209\n",
      "Loss after epoch #610 is: train/0.1429979923641075 --- test/4.169127323384177\n",
      "Loss after epoch #611 is: train/0.1421168890335245 --- test/4.1577242322124475\n",
      "Loss after epoch #612 is: train/0.14207418249458242 --- test/4.159159562952619\n",
      "Loss after epoch #613 is: train/0.14170913919314373 --- test/4.150649695149592\n",
      "Loss after epoch #614 is: train/0.1409666537102079 --- test/4.1435024582847895\n",
      "Loss after epoch #615 is: train/0.14077173269082072 --- test/4.133984650188102\n",
      "Loss after epoch #616 is: train/0.14067848172433264 --- test/4.1119695343600435\n",
      "Loss after epoch #617 is: train/0.13969312431890674 --- test/4.109192055725628\n",
      "Loss after epoch #618 is: train/0.13858949741282267 --- test/4.109392123592303\n",
      "Loss after epoch #619 is: train/0.1383906707527603 --- test/4.09108588511524\n",
      "Loss after epoch #620 is: train/0.1377240457734332 --- test/4.106824758702665\n",
      "Loss after epoch #621 is: train/0.1370803196806208 --- test/4.0863296443158115\n",
      "Loss after epoch #622 is: train/0.1367956206780918 --- test/4.064889023271297\n",
      "Loss after epoch #623 is: train/0.13686810786294307 --- test/4.061178007654775\n",
      "Loss after epoch #624 is: train/0.13600791673212245 --- test/4.0542074956631575\n",
      "Loss after epoch #625 is: train/0.13606834123672848 --- test/4.060110349221436\n",
      "Loss after epoch #626 is: train/0.13592700573715263 --- test/4.0475606234858414\n",
      "Loss after epoch #627 is: train/0.13526809999950534 --- test/4.027686671739389\n",
      "Loss after epoch #628 is: train/0.13429861444437247 --- test/4.005437183604283\n",
      "Loss after epoch #629 is: train/0.13427377384680236 --- test/4.035095462508099\n",
      "Loss after epoch #630 is: train/0.13366733985934112 --- test/4.034920037828373\n",
      "Loss after epoch #631 is: train/0.13358971654432367 --- test/4.039296218771319\n",
      "Loss after epoch #632 is: train/0.13350995859461237 --- test/4.023548611418026\n",
      "Loss after epoch #633 is: train/0.13318710761534713 --- test/4.02514798858699\n",
      "Loss after epoch #634 is: train/0.13342319985018347 --- test/4.021672026327799\n",
      "Loss after epoch #635 is: train/0.13308009915322827 --- test/4.009748656234878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #636 is: train/0.13239429461340982 --- test/3.9837270656961836\n",
      "Loss after epoch #637 is: train/0.13170080476344143 --- test/3.964713234964279\n",
      "Loss after epoch #638 is: train/0.13022401334632522 --- test/3.93869722641284\n",
      "Loss after epoch #639 is: train/0.12959079288742392 --- test/3.9055616494539294\n",
      "Loss after epoch #640 is: train/0.12945734433853168 --- test/3.914295475297103\n",
      "Loss after epoch #641 is: train/0.12937129223227772 --- test/3.90233089946446\n",
      "Loss after epoch #642 is: train/0.1289159649651208 --- test/3.898575151787586\n",
      "Loss after epoch #643 is: train/0.12846337448011638 --- test/3.9039286895078518\n",
      "Loss after epoch #644 is: train/0.12826231775460573 --- test/3.898063227695047\n",
      "Loss after epoch #645 is: train/0.1274562787291305 --- test/3.8932222374183567\n",
      "Loss after epoch #646 is: train/0.12771510616998544 --- test/3.8942069546060103\n",
      "Loss after epoch #647 is: train/0.12667557597800058 --- test/3.8797366267480586\n",
      "Loss after epoch #648 is: train/0.1262732437722234 --- test/3.871504678773079\n",
      "Loss after epoch #649 is: train/0.12602412295152665 --- test/3.889788970416532\n",
      "Loss after epoch #650 is: train/0.12576455204045692 --- test/3.8967950919202132\n",
      "Loss after epoch #651 is: train/0.12618921835661862 --- test/3.89657182744827\n",
      "Loss after epoch #652 is: train/0.12554846365770989 --- test/3.8647334935771607\n",
      "Loss after epoch #653 is: train/0.12490141193758295 --- test/3.853790804163202\n",
      "Loss after epoch #654 is: train/0.12423958667902935 --- test/3.8431644462760417\n",
      "Loss after epoch #655 is: train/0.12351765943556052 --- test/3.83512601517372\n",
      "Loss after epoch #656 is: train/0.12327882309556847 --- test/3.8339706667995825\n",
      "Loss after epoch #657 is: train/0.12251681413277063 --- test/3.8230382863775487\n",
      "Loss after epoch #658 is: train/0.12269707408709436 --- test/3.827018067306085\n",
      "Loss after epoch #659 is: train/0.1230640044281573 --- test/3.806737929031282\n",
      "Loss after epoch #660 is: train/0.12263733170106103 --- test/3.8162625414931903\n",
      "Loss after epoch #661 is: train/0.12254378362754974 --- test/3.8102777636928065\n",
      "Loss after epoch #662 is: train/0.12171344806098384 --- test/3.7767971529024322\n",
      "Loss after epoch #663 is: train/0.12139533204347253 --- test/3.7814523763034176\n",
      "Loss after epoch #664 is: train/0.12094908654152009 --- test/3.787178194755826\n",
      "Loss after epoch #665 is: train/0.12039166917516844 --- test/3.7771757579608085\n",
      "Loss after epoch #666 is: train/0.11980467703116342 --- test/3.7827728069935627\n",
      "Loss after epoch #667 is: train/0.119211474303162 --- test/3.771075326812169\n",
      "Loss after epoch #668 is: train/0.11847229277625834 --- test/3.7416412728930846\n",
      "Loss after epoch #669 is: train/0.11761906876177923 --- test/3.7327487826638324\n",
      "Loss after epoch #670 is: train/0.11731547191374748 --- test/3.7099236697333233\n",
      "Loss after epoch #671 is: train/0.11709796119092589 --- test/3.7035579348793277\n",
      "Loss after epoch #672 is: train/0.11732799768324473 --- test/3.7149100949277516\n",
      "Loss after epoch #673 is: train/0.11656302688333373 --- test/3.7035551329004632\n",
      "Loss after epoch #674 is: train/0.11629630219442119 --- test/3.7230109879276947\n",
      "Loss after epoch #675 is: train/0.11619132700658648 --- test/3.716017541866822\n",
      "Loss after epoch #676 is: train/0.1161780079400742 --- test/3.700607787427079\n",
      "Loss after epoch #677 is: train/0.11556148904437841 --- test/3.685779275771674\n",
      "Loss after epoch #678 is: train/0.11596686606617873 --- test/3.686272343502908\n",
      "Loss after epoch #679 is: train/0.11517889596503499 --- test/3.67184982347037\n",
      "Loss after epoch #680 is: train/0.11464092350804896 --- test/3.664817613822757\n",
      "Loss after epoch #681 is: train/0.1149690059968701 --- test/3.640869642293374\n",
      "Loss after epoch #682 is: train/0.11450794219263492 --- test/3.6421036316870556\n",
      "Loss after epoch #683 is: train/0.11538135278732102 --- test/3.642172530735355\n",
      "Loss after epoch #684 is: train/0.11440603425968354 --- test/3.6184695762317465\n",
      "Loss after epoch #685 is: train/0.11274993581849795 --- test/3.6060150876845736\n",
      "Loss after epoch #686 is: train/0.11323525487444418 --- test/3.6005513345873363\n",
      "Loss after epoch #687 is: train/0.1120570098084844 --- test/3.6132437927874963\n",
      "Loss after epoch #688 is: train/0.11195116127852164 --- test/3.6036070599334353\n",
      "Loss after epoch #689 is: train/0.11129168122274827 --- test/3.601913352306677\n",
      "Loss after epoch #690 is: train/0.1105557044816703 --- test/3.6023002722546993\n",
      "Loss after epoch #691 is: train/0.11016046527328116 --- test/3.60380593578582\n",
      "Loss after epoch #692 is: train/0.10989026583923883 --- test/3.5920320230478757\n",
      "Loss after epoch #693 is: train/0.10954946957408854 --- test/3.580647389148155\n",
      "Loss after epoch #694 is: train/0.10962447609827095 --- test/3.5708539025963546\n",
      "Loss after epoch #695 is: train/0.1096797273151372 --- test/3.568666397275255\n",
      "Loss after epoch #696 is: train/0.10936107052271817 --- test/3.5616644902851986\n",
      "Loss after epoch #697 is: train/0.10927890974308686 --- test/3.550234038234684\n",
      "Loss after epoch #698 is: train/0.10920074016274534 --- test/3.5414098047751885\n",
      "Loss after epoch #699 is: train/0.1089450450865907 --- test/3.5347404484436176\n",
      "Loss after epoch #700 is: train/0.10822952724421768 --- test/3.5270071715677482\n",
      "Loss after epoch #701 is: train/0.10783936011806164 --- test/3.521982234167909\n",
      "Loss after epoch #702 is: train/0.10722265088128612 --- test/3.518346112754425\n",
      "Loss after epoch #703 is: train/0.10730939860682906 --- test/3.5108680915619743\n",
      "Loss after epoch #704 is: train/0.10670372031999369 --- test/3.5089955958798824\n",
      "Loss after epoch #705 is: train/0.10664655270784656 --- test/3.5049737739650317\n",
      "Loss after epoch #706 is: train/0.10584076246476523 --- test/3.496213811531135\n",
      "Loss after epoch #707 is: train/0.10517680554931196 --- test/3.495477496784055\n",
      "Loss after epoch #708 is: train/0.10514224953504926 --- test/3.497912112116046\n",
      "Loss after epoch #709 is: train/0.10430222943400538 --- test/3.4881074666248\n",
      "Loss after epoch #710 is: train/0.10416996019989015 --- test/3.4833103137079195\n",
      "Loss after epoch #711 is: train/0.1042086613081156 --- test/3.468930042898773\n",
      "Loss after epoch #712 is: train/0.10459738083864561 --- test/3.4433868510304197\n",
      "Loss after epoch #713 is: train/0.10423911657052085 --- test/3.439638859788055\n",
      "Loss after epoch #714 is: train/0.10385300777016923 --- test/3.4393586255139508\n",
      "Loss after epoch #715 is: train/0.10315759699972028 --- test/3.4245704461812503\n",
      "Loss after epoch #716 is: train/0.10304277582170816 --- test/3.4261279454663516\n",
      "Loss after epoch #717 is: train/0.10265429237971788 --- test/3.4129232017893663\n",
      "Loss after epoch #718 is: train/0.10235879524633733 --- test/3.425763227149851\n",
      "Loss after epoch #719 is: train/0.10231581552686632 --- test/3.427382353052473\n",
      "Loss after epoch #720 is: train/0.10208906849257667 --- test/3.4184394236293465\n",
      "Loss after epoch #721 is: train/0.10158270889350086 --- test/3.4063739707670195\n",
      "Loss after epoch #722 is: train/0.10101354002871643 --- test/3.4131827205640866\n",
      "Loss after epoch #723 is: train/0.10108669805062971 --- test/3.401371986943519\n",
      "Loss after epoch #724 is: train/0.10133277289148744 --- test/3.405454240187207\n",
      "Loss after epoch #725 is: train/0.10070031899005494 --- test/3.4047759489977993\n",
      "Loss after epoch #726 is: train/0.10051094703620182 --- test/3.3748645964285204\n",
      "Loss after epoch #727 is: train/0.09993528005250742 --- test/3.3752666579316357\n",
      "Loss after epoch #728 is: train/0.09976257217868359 --- test/3.374629446233327\n",
      "Loss after epoch #729 is: train/0.09937315168984641 --- test/3.358554753991843\n",
      "Loss after epoch #730 is: train/0.09924710973008609 --- test/3.3427482197940632\n",
      "Loss after epoch #731 is: train/0.09899905938141719 --- test/3.3378088019200556\n",
      "Loss after epoch #732 is: train/0.0987780962559547 --- test/3.3316457023704342\n",
      "Loss after epoch #733 is: train/0.09865198717428941 --- test/3.3044152755502942\n",
      "Loss after epoch #734 is: train/0.09860267136184368 --- test/3.2982099345914495\n",
      "Loss after epoch #735 is: train/0.09791635283720432 --- test/3.2970275465359458\n",
      "Loss after epoch #736 is: train/0.09704235915008738 --- test/3.3154519697073397\n",
      "Loss after epoch #737 is: train/0.0967054293490111 --- test/3.306906539543818\n",
      "Loss after epoch #738 is: train/0.09618185166501245 --- test/3.300315881448354\n",
      "Loss after epoch #739 is: train/0.09603461853495628 --- test/3.2819998228171947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #740 is: train/0.0960440315651697 --- test/3.2863872136557943\n",
      "Loss after epoch #741 is: train/0.09575378871334916 --- test/3.28594692437123\n",
      "Loss after epoch #742 is: train/0.09548494069324567 --- test/3.2814152661334806\n",
      "Loss after epoch #743 is: train/0.095542422371534 --- test/3.2730162842231434\n",
      "Loss after epoch #744 is: train/0.09534103363108193 --- test/3.273565468777739\n",
      "Loss after epoch #745 is: train/0.09505700167217576 --- test/3.2818468657480935\n",
      "Loss after epoch #746 is: train/0.09503081975080578 --- test/3.277575459818646\n",
      "Loss after epoch #747 is: train/0.0946918884423972 --- test/3.2819626984520265\n",
      "Loss after epoch #748 is: train/0.09432566746576973 --- test/3.2889338342928633\n",
      "Loss after epoch #749 is: train/0.09403523409841033 --- test/3.2775422860318995\n",
      "Loss after epoch #750 is: train/0.09380418092809371 --- test/3.2721903309800218\n",
      "Loss after epoch #751 is: train/0.09356669001806253 --- test/3.278179340145544\n",
      "Loss after epoch #752 is: train/0.09320262371601258 --- test/3.2563319986790793\n",
      "Loss after epoch #753 is: train/0.0927053082460589 --- test/3.2410348203903534\n",
      "Loss after epoch #754 is: train/0.09247759798803148 --- test/3.238689647151901\n",
      "Loss after epoch #755 is: train/0.09287999198698088 --- test/3.2314738403660352\n",
      "Loss after epoch #756 is: train/0.09260599136556867 --- test/3.2316212101821744\n",
      "Loss after epoch #757 is: train/0.09278666381259548 --- test/3.2241718993141895\n",
      "Loss after epoch #758 is: train/0.09224976757741973 --- test/3.2237506978174615\n",
      "Loss after epoch #759 is: train/0.09199139483746879 --- test/3.2119054482582006\n",
      "Loss after epoch #760 is: train/0.09137253166761164 --- test/3.208518428765416\n",
      "Loss after epoch #761 is: train/0.09116740974363466 --- test/3.217169598566014\n",
      "Loss after epoch #762 is: train/0.09110155155198013 --- test/3.20467272165333\n",
      "Loss after epoch #763 is: train/0.09075088308861248 --- test/3.197770547726661\n",
      "Loss after epoch #764 is: train/0.09077060556638963 --- test/3.1981228081849378\n",
      "Loss after epoch #765 is: train/0.0903298288729105 --- test/3.203693856792628\n",
      "Loss after epoch #766 is: train/0.09014976185791707 --- test/3.195493476575512\n",
      "Loss after epoch #767 is: train/0.0896483134859502 --- test/3.2061649387458138\n",
      "Loss after epoch #768 is: train/0.08895920227995972 --- test/3.1858294731059527\n",
      "Loss after epoch #769 is: train/0.08864832837377129 --- test/3.1743554799233147\n",
      "Loss after epoch #770 is: train/0.08829417413627294 --- test/3.1784372712949187\n",
      "Loss after epoch #771 is: train/0.08840091445039144 --- test/3.179865496200217\n",
      "Loss after epoch #772 is: train/0.08825092421230775 --- test/3.1696015392024393\n",
      "Loss after epoch #773 is: train/0.08797066191550668 --- test/3.17145199817737\n",
      "Loss after epoch #774 is: train/0.08743865287337531 --- test/3.156670450592943\n",
      "Loss after epoch #775 is: train/0.08700968798383152 --- test/3.148679073000228\n",
      "Loss after epoch #776 is: train/0.08728042013930816 --- test/3.137852540040664\n",
      "Loss after epoch #777 is: train/0.08719044101259582 --- test/3.1454072914249704\n",
      "Loss after epoch #778 is: train/0.08670738672233003 --- test/3.1335789597827137\n",
      "Loss after epoch #779 is: train/0.08638653319192693 --- test/3.1129984426939195\n",
      "Loss after epoch #780 is: train/0.0860369091056567 --- test/3.108666074966921\n",
      "Loss after epoch #781 is: train/0.08594466095581689 --- test/3.1081868787918703\n",
      "Loss after epoch #782 is: train/0.0858926428760859 --- test/3.1063282253881987\n",
      "Loss after epoch #783 is: train/0.08584804201488899 --- test/3.100551963553381\n",
      "Loss after epoch #784 is: train/0.08520451805969703 --- test/3.091549962009069\n",
      "Loss after epoch #785 is: train/0.08489803219915547 --- test/3.09052019595431\n",
      "Loss after epoch #786 is: train/0.08498697352377975 --- test/3.0851825141317395\n",
      "Loss after epoch #787 is: train/0.08437368073300193 --- test/3.0865132042549392\n",
      "Loss after epoch #788 is: train/0.084550683292664 --- test/3.0936169950846906\n",
      "Loss after epoch #789 is: train/0.08410489672809292 --- test/3.076483071194442\n",
      "Loss after epoch #790 is: train/0.08372193637597869 --- test/3.0626950451153663\n",
      "Loss after epoch #791 is: train/0.08380631340099712 --- test/3.0693970526309435\n",
      "Loss after epoch #792 is: train/0.08340329288180497 --- test/3.0721466613594566\n",
      "Loss after epoch #793 is: train/0.08342354479941154 --- test/3.078470837403071\n",
      "Loss after epoch #794 is: train/0.0832895438720571 --- test/3.0782247713520774\n",
      "Loss after epoch #795 is: train/0.0829847181261238 --- test/3.0704114038598513\n",
      "Loss after epoch #796 is: train/0.08267780804591418 --- test/3.059466528243898\n",
      "Loss after epoch #797 is: train/0.08238559282256536 --- test/3.0636756865603134\n",
      "Loss after epoch #798 is: train/0.08231739328421926 --- test/3.0526231176491865\n",
      "Loss after epoch #799 is: train/0.08209482997696378 --- test/3.0503615504350683\n",
      "Loss after epoch #800 is: train/0.08219847555589066 --- test/3.0426099206562944\n",
      "Loss after epoch #801 is: train/0.08227819608392654 --- test/3.033221003440615\n",
      "Loss after epoch #802 is: train/0.08177093294378565 --- test/3.0270102798588048\n",
      "Loss after epoch #803 is: train/0.08134273533794863 --- test/3.026955662970071\n",
      "Loss after epoch #804 is: train/0.0812152124361331 --- test/3.0188329768493594\n",
      "Loss after epoch #805 is: train/0.08094972024422065 --- test/3.018476762957915\n",
      "Loss after epoch #806 is: train/0.08069578703329866 --- test/3.0102424434502337\n",
      "Loss after epoch #807 is: train/0.0806311651476172 --- test/3.004786016128515\n",
      "Loss after epoch #808 is: train/0.0804324884161972 --- test/3.000485235345234\n",
      "Loss after epoch #809 is: train/0.08031215623651898 --- test/2.9960023584890494\n",
      "Loss after epoch #810 is: train/0.08019117880899704 --- test/2.997771025658518\n",
      "Loss after epoch #811 is: train/0.07979251850174078 --- test/2.991847587642997\n",
      "Loss after epoch #812 is: train/0.07976589040565678 --- test/2.993947400900372\n",
      "Loss after epoch #813 is: train/0.07980866841101994 --- test/2.9826248937982123\n",
      "Loss after epoch #814 is: train/0.07976520147982956 --- test/2.97949697946419\n",
      "Loss after epoch #815 is: train/0.07955863183763869 --- test/2.9741789853290084\n",
      "Loss after epoch #816 is: train/0.07954392654917374 --- test/2.9791435969808484\n",
      "Loss after epoch #817 is: train/0.07932037811243031 --- test/2.9766236470894283\n",
      "Loss after epoch #818 is: train/0.07901497528471302 --- test/2.9756641060015507\n",
      "Loss after epoch #819 is: train/0.07855921788956548 --- test/2.959919178221623\n",
      "Loss after epoch #820 is: train/0.07848879168961355 --- test/2.9605450825896895\n",
      "Loss after epoch #821 is: train/0.07833105991500278 --- test/2.954432708959893\n",
      "Loss after epoch #822 is: train/0.0777840156138401 --- test/2.94968105293563\n",
      "Loss after epoch #823 is: train/0.07751210980537253 --- test/2.9559299407778723\n",
      "Loss after epoch #824 is: train/0.07736758351961662 --- test/2.9510332175282348\n",
      "Loss after epoch #825 is: train/0.07720701368272626 --- test/2.942988511665188\n",
      "Loss after epoch #826 is: train/0.07693053366717938 --- test/2.92906094889315\n",
      "Loss after epoch #827 is: train/0.07697999295792307 --- test/2.9290808211712647\n",
      "Loss after epoch #828 is: train/0.07708900339441724 --- test/2.929662485305511\n",
      "Loss after epoch #829 is: train/0.07703754897627768 --- test/2.926985122563673\n",
      "Loss after epoch #830 is: train/0.07669277100885573 --- test/2.9154552226353188\n",
      "Loss after epoch #831 is: train/0.07649971722071129 --- test/2.8969480512334367\n",
      "Loss after epoch #832 is: train/0.07575852048903021 --- test/2.901982732134205\n",
      "Loss after epoch #833 is: train/0.0756566527450411 --- test/2.9098206632811547\n",
      "Loss after epoch #834 is: train/0.07571243244644563 --- test/2.932823268426498\n",
      "Loss after epoch #835 is: train/0.07583099882187803 --- test/2.9309242013442556\n",
      "Loss after epoch #836 is: train/0.07573554263896418 --- test/2.9258379716016063\n",
      "Loss after epoch #837 is: train/0.07571521100844553 --- test/2.929036384904173\n",
      "Loss after epoch #838 is: train/0.07559294228372533 --- test/2.92238334759565\n",
      "Loss after epoch #839 is: train/0.075271818953361 --- test/2.9154782310457454\n",
      "Loss after epoch #840 is: train/0.07498450406199159 --- test/2.9120281225994016\n",
      "Loss after epoch #841 is: train/0.07470720283882706 --- test/2.9028414452051265\n",
      "Loss after epoch #842 is: train/0.07427888420710055 --- test/2.8902514610526273\n",
      "Loss after epoch #843 is: train/0.07439592435248321 --- test/2.8911383096547785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #844 is: train/0.07417867000334524 --- test/2.885659643031319\n",
      "Loss after epoch #845 is: train/0.07396034085664514 --- test/2.8765913801998675\n",
      "Loss after epoch #846 is: train/0.07397652096488155 --- test/2.8786142567524644\n",
      "Loss after epoch #847 is: train/0.07401664532797196 --- test/2.8730522832488674\n",
      "Loss after epoch #848 is: train/0.07410566191912339 --- test/2.866386058368766\n",
      "Loss after epoch #849 is: train/0.07376764568773429 --- test/2.8623758052117383\n",
      "Loss after epoch #850 is: train/0.07367962939064719 --- test/2.860549914994797\n",
      "Loss after epoch #851 is: train/0.07327413593663462 --- test/2.8398161187091504\n",
      "Loss after epoch #852 is: train/0.07313176307346937 --- test/2.8310867147558842\n",
      "Loss after epoch #853 is: train/0.07266956937251665 --- test/2.821135512664713\n",
      "Loss after epoch #854 is: train/0.07239760397373524 --- test/2.8278241241645548\n",
      "Loss after epoch #855 is: train/0.07221763913669896 --- test/2.8102069366019085\n",
      "Loss after epoch #856 is: train/0.07219627635403067 --- test/2.812188375230638\n",
      "Loss after epoch #857 is: train/0.07165304604904671 --- test/2.8029773715678643\n",
      "Loss after epoch #858 is: train/0.07156672018533199 --- test/2.808431390920158\n",
      "Loss after epoch #859 is: train/0.07128156555225844 --- test/2.797860720148136\n",
      "Loss after epoch #860 is: train/0.07129097889226459 --- test/2.799647326284709\n",
      "Loss after epoch #861 is: train/0.07146805231953826 --- test/2.814754538722272\n",
      "Loss after epoch #862 is: train/0.0712190271597799 --- test/2.8079738965757004\n",
      "Loss after epoch #863 is: train/0.07095466769011832 --- test/2.810698387658645\n",
      "Loss after epoch #864 is: train/0.07077993543995871 --- test/2.799145138103522\n",
      "Loss after epoch #865 is: train/0.07042817876037606 --- test/2.7934251873901768\n",
      "Loss after epoch #866 is: train/0.07068788635436325 --- test/2.806371074489567\n",
      "Loss after epoch #867 is: train/0.07031280894436069 --- test/2.8005180365701863\n",
      "Loss after epoch #868 is: train/0.07028057517457201 --- test/2.7908712307493913\n",
      "Loss after epoch #869 is: train/0.06990071711008411 --- test/2.769315448488627\n",
      "Loss after epoch #870 is: train/0.06983824314430394 --- test/2.772245917770654\n",
      "Loss after epoch #871 is: train/0.06971629834100128 --- test/2.765303482240699\n",
      "Loss after epoch #872 is: train/0.06949369495397031 --- test/2.7777414657285804\n",
      "Loss after epoch #873 is: train/0.06909778524896146 --- test/2.766376940512708\n",
      "Loss after epoch #874 is: train/0.06912571078417583 --- test/2.7637137408614847\n",
      "Loss after epoch #875 is: train/0.06885279590328097 --- test/2.757613196484062\n",
      "Loss after epoch #876 is: train/0.06897913905160837 --- test/2.7539676502048245\n",
      "Loss after epoch #877 is: train/0.06876890521210376 --- test/2.74758475120549\n",
      "Loss after epoch #878 is: train/0.0685934044792491 --- test/2.739735933440713\n",
      "Loss after epoch #879 is: train/0.0687138664693238 --- test/2.7195292080433133\n",
      "Loss after epoch #880 is: train/0.06861173641923335 --- test/2.720394247389189\n",
      "Loss after epoch #881 is: train/0.06857142960116692 --- test/2.734851038569151\n",
      "Loss after epoch #882 is: train/0.06873711431477973 --- test/2.720099056028971\n",
      "Loss after epoch #883 is: train/0.06847391166001916 --- test/2.7225568179089836\n",
      "Loss after epoch #884 is: train/0.0685166010385356 --- test/2.7225729384960755\n",
      "Loss after epoch #885 is: train/0.06833601535381893 --- test/2.7139821622182945\n",
      "Loss after epoch #886 is: train/0.06816336813115705 --- test/2.712886047389138\n",
      "Loss after epoch #887 is: train/0.0681204181778042 --- test/2.7162922651770507\n",
      "Loss after epoch #888 is: train/0.06779237936656601 --- test/2.7027319540356216\n",
      "Loss after epoch #889 is: train/0.0671959043412079 --- test/2.6987953020588975\n",
      "Loss after epoch #890 is: train/0.06725390460111828 --- test/2.6968993605445015\n",
      "Loss after epoch #891 is: train/0.0667935922368286 --- test/2.6865117371894427\n",
      "Loss after epoch #892 is: train/0.06676804945488293 --- test/2.687453607816335\n",
      "Loss after epoch #893 is: train/0.06687120225237911 --- test/2.6946611492653147\n",
      "Loss after epoch #894 is: train/0.0667198802056428 --- test/2.6906980685466095\n",
      "Loss after epoch #895 is: train/0.0665579922425754 --- test/2.69397580088358\n",
      "Loss after epoch #896 is: train/0.06656139049632333 --- test/2.693498952631123\n",
      "Loss after epoch #897 is: train/0.06648639603616323 --- test/2.694426156023023\n",
      "Loss after epoch #898 is: train/0.06611831161366268 --- test/2.6770774345706565\n",
      "Loss after epoch #899 is: train/0.06608852314783957 --- test/2.685788382317859\n",
      "Loss after epoch #900 is: train/0.06581858893602839 --- test/2.671410225107596\n",
      "Loss after epoch #901 is: train/0.06593465832472421 --- test/2.6791384129261253\n",
      "Loss after epoch #902 is: train/0.06555396806094585 --- test/2.675614946861366\n",
      "Loss after epoch #903 is: train/0.06568855906063312 --- test/2.680852619748048\n",
      "Loss after epoch #904 is: train/0.06538159794043591 --- test/2.6618801134154526\n",
      "Loss after epoch #905 is: train/0.06531140665833957 --- test/2.665291778517697\n",
      "Loss after epoch #906 is: train/0.06509599355923269 --- test/2.6602117845434856\n",
      "Loss after epoch #907 is: train/0.06462963936892975 --- test/2.6499107750792374\n",
      "Loss after epoch #908 is: train/0.06433205842064492 --- test/2.635254180956817\n",
      "Loss after epoch #909 is: train/0.06411835249924493 --- test/2.633682347762104\n",
      "Loss after epoch #910 is: train/0.06414056719152665 --- test/2.625175934924538\n",
      "Loss after epoch #911 is: train/0.0642400500527881 --- test/2.630692513102232\n",
      "Loss after epoch #912 is: train/0.06418946104656945 --- test/2.6269059664181342\n",
      "Loss after epoch #913 is: train/0.06384165854278602 --- test/2.6238365455021997\n",
      "Loss after epoch #914 is: train/0.06416831842026831 --- test/2.6335343749381424\n",
      "Loss after epoch #915 is: train/0.06389024333183477 --- test/2.637918661490004\n",
      "Loss after epoch #916 is: train/0.06398396219537657 --- test/2.6227811512776937\n",
      "Loss after epoch #917 is: train/0.06423575712368673 --- test/2.6124593641834144\n",
      "Loss after epoch #918 is: train/0.06398772757108155 --- test/2.6066164861466468\n",
      "Loss after epoch #919 is: train/0.06387204508769877 --- test/2.6050039846521034\n",
      "Loss after epoch #920 is: train/0.06324902435854338 --- test/2.6049067155446313\n",
      "Loss after epoch #921 is: train/0.06307783502865219 --- test/2.6041632792583926\n",
      "Loss after epoch #922 is: train/0.062776411978947 --- test/2.6032132380203885\n",
      "Loss after epoch #923 is: train/0.06266832012713443 --- test/2.5931459761384996\n",
      "Loss after epoch #924 is: train/0.06265909671738641 --- test/2.6000703456014294\n",
      "Loss after epoch #925 is: train/0.06265938378211183 --- test/2.60006445450011\n",
      "Loss after epoch #926 is: train/0.0624019362583694 --- test/2.5895185208243943\n",
      "Loss after epoch #927 is: train/0.062230520762135685 --- test/2.584931115165643\n",
      "Loss after epoch #928 is: train/0.06237202154691828 --- test/2.5790186037353924\n",
      "Loss after epoch #929 is: train/0.06275292760596865 --- test/2.579016704501192\n",
      "Loss after epoch #930 is: train/0.06231054034101008 --- test/2.580718862456946\n",
      "Loss after epoch #931 is: train/0.062058273846058436 --- test/2.5720578749626264\n",
      "Loss after epoch #932 is: train/0.06179229837963339 --- test/2.577052057455084\n",
      "Loss after epoch #933 is: train/0.0615103392309448 --- test/2.569876987725155\n",
      "Loss after epoch #934 is: train/0.06164925170426201 --- test/2.568918758038912\n",
      "Loss after epoch #935 is: train/0.06137895776701721 --- test/2.558853529948087\n",
      "Loss after epoch #936 is: train/0.060969091683666495 --- test/2.556507723331834\n",
      "Loss after epoch #937 is: train/0.060760597606810005 --- test/2.5469488360276604\n",
      "Loss after epoch #938 is: train/0.06072562126522023 --- test/2.552441996360928\n",
      "Loss after epoch #939 is: train/0.06077769649875665 --- test/2.544407037117876\n",
      "Loss after epoch #940 is: train/0.060676385227549076 --- test/2.537366006280684\n",
      "Loss after epoch #941 is: train/0.06060105487392316 --- test/2.521903550252557\n",
      "Loss after epoch #942 is: train/0.06034790800782219 --- test/2.522591855764202\n",
      "Loss after epoch #943 is: train/0.0602490908366811 --- test/2.512624092247686\n",
      "Loss after epoch #944 is: train/0.06008744692151465 --- test/2.513209616226002\n",
      "Loss after epoch #945 is: train/0.05995132564485043 --- test/2.5104055092493307\n",
      "Loss after epoch #946 is: train/0.05980002632100927 --- test/2.521296262752696\n",
      "Loss after epoch #947 is: train/0.05971602276124094 --- test/2.51673223395423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #948 is: train/0.059566321699910774 --- test/2.5207678970076275\n",
      "Loss after epoch #949 is: train/0.05945164915017101 --- test/2.506037992136991\n",
      "Loss after epoch #950 is: train/0.05930794926075336 --- test/2.50791042935499\n",
      "Loss after epoch #951 is: train/0.05966014905270885 --- test/2.505037005235667\n",
      "Loss after epoch #952 is: train/0.05982448822845992 --- test/2.5064532989384816\n",
      "Loss after epoch #953 is: train/0.05969699606075476 --- test/2.506593198338848\n",
      "Loss after epoch #954 is: train/0.05954892178096968 --- test/2.4974094799798316\n",
      "Loss after epoch #955 is: train/0.059390645958160924 --- test/2.493503495582866\n",
      "Loss after epoch #956 is: train/0.05907873919319056 --- test/2.4900773589683953\n",
      "Loss after epoch #957 is: train/0.05896188756943351 --- test/2.4825561451404563\n",
      "Loss after epoch #958 is: train/0.05888920162508151 --- test/2.4773404534898384\n",
      "Loss after epoch #959 is: train/0.0586710244900006 --- test/2.4773740217042373\n",
      "Loss after epoch #960 is: train/0.058639221829687026 --- test/2.4832854134190874\n",
      "Loss after epoch #961 is: train/0.0586383245029662 --- test/2.4861794377603847\n",
      "Loss after epoch #962 is: train/0.058733723538604315 --- test/2.481413546434512\n",
      "Loss after epoch #963 is: train/0.058392719046735445 --- test/2.4797389153748446\n",
      "Loss after epoch #964 is: train/0.057992571469287074 --- test/2.4639698106196426\n",
      "Loss after epoch #965 is: train/0.058046123749189105 --- test/2.4661193060063176\n",
      "Loss after epoch #966 is: train/0.057931788414461276 --- test/2.458907692843548\n",
      "Loss after epoch #967 is: train/0.05790209607420218 --- test/2.4540670945633867\n",
      "Loss after epoch #968 is: train/0.05787585344514192 --- test/2.449406751389388\n",
      "Loss after epoch #969 is: train/0.057700198782101975 --- test/2.4421614260335818\n",
      "Loss after epoch #970 is: train/0.057704960925917564 --- test/2.442654945441188\n",
      "Loss after epoch #971 is: train/0.05768489040139485 --- test/2.433325175120349\n",
      "Loss after epoch #972 is: train/0.05774954614050419 --- test/2.429938000341942\n",
      "Loss after epoch #973 is: train/0.05748198777758169 --- test/2.4197657463986055\n",
      "Loss after epoch #974 is: train/0.057300615136951706 --- test/2.4241373084322433\n",
      "Loss after epoch #975 is: train/0.05716264433252507 --- test/2.42398709196226\n",
      "Loss after epoch #976 is: train/0.05719357909799321 --- test/2.4204851647018635\n",
      "Loss after epoch #977 is: train/0.05698153143448842 --- test/2.4179151276548314\n",
      "Loss after epoch #978 is: train/0.05695481381951143 --- test/2.4111372404381033\n",
      "Loss after epoch #979 is: train/0.056662162237157074 --- test/2.4160622548228665\n",
      "Loss after epoch #980 is: train/0.05643629263794516 --- test/2.417555374466613\n",
      "Loss after epoch #981 is: train/0.0566096737426642 --- test/2.40705547786285\n",
      "Loss after epoch #982 is: train/0.056537816593904305 --- test/2.3971649265156616\n",
      "Loss after epoch #983 is: train/0.05635481608507376 --- test/2.398805846900723\n",
      "Loss after epoch #984 is: train/0.05607559884231964 --- test/2.4008158477516686\n",
      "Loss after epoch #985 is: train/0.056139500677467695 --- test/2.4028698340444494\n",
      "Loss after epoch #986 is: train/0.05610877713207697 --- test/2.3951966781886878\n",
      "Loss after epoch #987 is: train/0.055793699714689636 --- test/2.3915398979716276\n",
      "Loss after epoch #988 is: train/0.05583630224626022 --- test/2.3896350271990556\n",
      "Loss after epoch #989 is: train/0.05580200809616298 --- test/2.389737862916072\n",
      "Loss after epoch #990 is: train/0.05570915708353697 --- test/2.3924271587607553\n",
      "Loss after epoch #991 is: train/0.05560518589906738 --- test/2.3850064950993124\n",
      "Loss after epoch #992 is: train/0.0555453489467333 --- test/2.3819403588010335\n",
      "Loss after epoch #993 is: train/0.055649752844709494 --- test/2.3775237161885623\n",
      "Loss after epoch #994 is: train/0.055181322500182595 --- test/2.3750716192285277\n",
      "Loss after epoch #995 is: train/0.05518167260911228 --- test/2.3777161855973836\n",
      "Loss after epoch #996 is: train/0.05498116266727201 --- test/2.3759011510467847\n",
      "Loss after epoch #997 is: train/0.05496028308757042 --- test/2.3668462831768626\n",
      "Loss after epoch #998 is: train/0.05515711723845296 --- test/2.3696587967396807\n",
      "Loss after epoch #999 is: train/0.05510934733055276 --- test/2.3661136113250363\n",
      "Loss after epoch #1000 is: train/0.05472742725484945 --- test/2.3573454564401106\n",
      "Loss after epoch #1001 is: train/0.05479178152830462 --- test/2.358017636006122\n",
      "Loss after epoch #1002 is: train/0.054644895916450296 --- test/2.346247348991895\n",
      "Loss after epoch #1003 is: train/0.054341849413722804 --- test/2.339922340084194\n",
      "Loss after epoch #1004 is: train/0.05432319003903319 --- test/2.3461973814985106\n",
      "Loss after epoch #1005 is: train/0.054199629000268056 --- test/2.347855150310259\n",
      "Loss after epoch #1006 is: train/0.054088367529838186 --- test/2.354369980435476\n",
      "Loss after epoch #1007 is: train/0.05409688979460574 --- test/2.3537412815297936\n",
      "Loss after epoch #1008 is: train/0.05391264896795801 --- test/2.3528335716474325\n",
      "Loss after epoch #1009 is: train/0.053556627110080066 --- test/2.3413486099828242\n",
      "Loss after epoch #1010 is: train/0.05361037652750392 --- test/2.332327692584778\n",
      "Loss after epoch #1011 is: train/0.053861022899532374 --- test/2.332138680513745\n",
      "Loss after epoch #1012 is: train/0.05380883312970502 --- test/2.321267798998904\n",
      "Loss after epoch #1013 is: train/0.05411981283212086 --- test/2.326948465190346\n",
      "Loss after epoch #1014 is: train/0.053897296079864675 --- test/2.3169738986820128\n",
      "Loss after epoch #1015 is: train/0.05381332803635297 --- test/2.3194553649992167\n",
      "Loss after epoch #1016 is: train/0.05394517714537211 --- test/2.320224801399998\n",
      "Loss after epoch #1017 is: train/0.05380859911813303 --- test/2.3310677717022954\n",
      "Loss after epoch #1018 is: train/0.05359762514732393 --- test/2.3148298004952546\n",
      "Loss after epoch #1019 is: train/0.05348272015741659 --- test/2.316131056833903\n",
      "Loss after epoch #1020 is: train/0.053508042046554016 --- test/2.313539604336292\n",
      "Loss after epoch #1021 is: train/0.05337860220078452 --- test/2.3145791954504733\n",
      "Loss after epoch #1022 is: train/0.0532026693942315 --- test/2.311353892165417\n",
      "Loss after epoch #1023 is: train/0.05310683650077667 --- test/2.29859713883753\n",
      "Loss after epoch #1024 is: train/0.052936607959214704 --- test/2.298094565791205\n",
      "Loss after epoch #1025 is: train/0.05283463191680716 --- test/2.2986532228416423\n",
      "Loss after epoch #1026 is: train/0.052653904387163726 --- test/2.294820580632392\n",
      "Loss after epoch #1027 is: train/0.052345402444959464 --- test/2.297259415999356\n",
      "Loss after epoch #1028 is: train/0.052398921990801775 --- test/2.288654172497197\n",
      "Loss after epoch #1029 is: train/0.05235105455794537 --- test/2.2908009832988845\n",
      "Loss after epoch #1030 is: train/0.05231172528361569 --- test/2.286959994860682\n",
      "Loss after epoch #1031 is: train/0.05215929029871543 --- test/2.289669454434024\n",
      "Loss after epoch #1032 is: train/0.05192546612763237 --- test/2.294276092975252\n",
      "Loss after epoch #1033 is: train/0.052085969915229104 --- test/2.298310663155929\n",
      "Loss after epoch #1034 is: train/0.052042177478135505 --- test/2.2920362805173253\n",
      "Loss after epoch #1035 is: train/0.0519183011821359 --- test/2.2870616579545326\n",
      "Loss after epoch #1036 is: train/0.051902041107211985 --- test/2.2888683879282015\n",
      "Loss after epoch #1037 is: train/0.05182408954246521 --- test/2.286495920609429\n",
      "Loss after epoch #1038 is: train/0.051593853015816545 --- test/2.2857793478227353\n",
      "Loss after epoch #1039 is: train/0.05171916453852967 --- test/2.2848186362136924\n",
      "Loss after epoch #1040 is: train/0.05180712386856795 --- test/2.285174243049381\n",
      "Loss after epoch #1041 is: train/0.05147327838245001 --- test/2.283985121505798\n",
      "Loss after epoch #1042 is: train/0.05142266782632417 --- test/2.2796540623510584\n",
      "Loss after epoch #1043 is: train/0.05143296932351867 --- test/2.2779528998221723\n",
      "Loss after epoch #1044 is: train/0.05146584682360708 --- test/2.274316301135461\n",
      "Loss after epoch #1045 is: train/0.05145113730547558 --- test/2.2665576921330852\n",
      "Loss after epoch #1046 is: train/0.05127299784168343 --- test/2.2742894471745605\n",
      "Loss after epoch #1047 is: train/0.05133692958536844 --- test/2.2659776106018548\n",
      "Loss after epoch #1048 is: train/0.05113652757454202 --- test/2.2676429219179153\n",
      "Loss after epoch #1049 is: train/0.05117985493739432 --- test/2.269597263736027\n",
      "Loss after epoch #1050 is: train/0.0509764547959678 --- test/2.266370473430566\n",
      "Loss after epoch #1051 is: train/0.05087357242279745 --- test/2.2609894589126625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1052 is: train/0.050855169321072494 --- test/2.2584008493540693\n",
      "Loss after epoch #1053 is: train/0.050727834719296705 --- test/2.2558394943274256\n",
      "Loss after epoch #1054 is: train/0.05046502435905441 --- test/2.2513173797268564\n",
      "Loss after epoch #1055 is: train/0.05065833214347813 --- test/2.2523529058837255\n",
      "Loss after epoch #1056 is: train/0.05040754577493199 --- test/2.2490852734437747\n",
      "Loss after epoch #1057 is: train/0.05031424148288766 --- test/2.2482909867278402\n",
      "Loss after epoch #1058 is: train/0.05019977711245297 --- test/2.248592926393232\n",
      "Loss after epoch #1059 is: train/0.05005684013073129 --- test/2.2448200829222476\n",
      "Loss after epoch #1060 is: train/0.049971662835095 --- test/2.246359809159243\n",
      "Loss after epoch #1061 is: train/0.049525308489939926 --- test/2.22756533270703\n",
      "Loss after epoch #1062 is: train/0.04949650598944949 --- test/2.2253946162415135\n",
      "Loss after epoch #1063 is: train/0.04955363371534756 --- test/2.229005000985491\n",
      "Loss after epoch #1064 is: train/0.04970002558704391 --- test/2.230946089870862\n",
      "Loss after epoch #1065 is: train/0.049795610078444366 --- test/2.233305184051774\n",
      "Loss after epoch #1066 is: train/0.049441683590375755 --- test/2.227832408717493\n",
      "Loss after epoch #1067 is: train/0.04938510974557314 --- test/2.230043364684221\n",
      "Loss after epoch #1068 is: train/0.049328870265662864 --- test/2.2325093457703464\n",
      "Loss after epoch #1069 is: train/0.0493201742172818 --- test/2.2279499909149463\n",
      "Loss after epoch #1070 is: train/0.04915320792528902 --- test/2.2251354962067187\n",
      "Loss after epoch #1071 is: train/0.04900419136021807 --- test/2.2236027156532563\n",
      "Loss after epoch #1072 is: train/0.04905462363981165 --- test/2.2251034351505434\n",
      "Loss after epoch #1073 is: train/0.04887334908751889 --- test/2.219812624244221\n",
      "Loss after epoch #1074 is: train/0.04886355914386625 --- test/2.212772569313784\n",
      "Loss after epoch #1075 is: train/0.048928738179145774 --- test/2.215266957850666\n",
      "Loss after epoch #1076 is: train/0.048634576413869125 --- test/2.209781237607776\n",
      "Loss after epoch #1077 is: train/0.04876395015150664 --- test/2.215342268141156\n",
      "Loss after epoch #1078 is: train/0.04879117531422006 --- test/2.206197256254367\n",
      "Loss after epoch #1079 is: train/0.04864510122341465 --- test/2.199158022324506\n",
      "Loss after epoch #1080 is: train/0.04867714908411778 --- test/2.1914744488049553\n",
      "Loss after epoch #1081 is: train/0.0485018546700174 --- test/2.185652863420666\n",
      "Loss after epoch #1082 is: train/0.048391009435343246 --- test/2.1771951953046043\n",
      "Loss after epoch #1083 is: train/0.04851146908826603 --- test/2.1758490902161203\n",
      "Loss after epoch #1084 is: train/0.04815431039848295 --- test/2.1792098775887507\n",
      "Loss after epoch #1085 is: train/0.04806594006215612 --- test/2.1713556532561453\n",
      "Loss after epoch #1086 is: train/0.04777484884679438 --- test/2.1732197969927736\n",
      "Loss after epoch #1087 is: train/0.047947530526683026 --- test/2.179075373548904\n",
      "Loss after epoch #1088 is: train/0.048061896282134996 --- test/2.161126533222495\n",
      "Loss after epoch #1089 is: train/0.048163099036586755 --- test/2.156748328556029\n",
      "Loss after epoch #1090 is: train/0.04834459040232963 --- test/2.1558073459387446\n",
      "Loss after epoch #1091 is: train/0.04808764335665522 --- test/2.1662615245068113\n",
      "Loss after epoch #1092 is: train/0.04772882208201277 --- test/2.1652905297630824\n",
      "Loss after epoch #1093 is: train/0.04774862506587155 --- test/2.172238270440624\n",
      "Loss after epoch #1094 is: train/0.04764039271757816 --- test/2.174748085217425\n",
      "Loss after epoch #1095 is: train/0.047412398428920914 --- test/2.1725821978986466\n",
      "Loss after epoch #1096 is: train/0.047532654662578444 --- test/2.1694260387388047\n",
      "Loss after epoch #1097 is: train/0.047543970454130134 --- test/2.161382972852466\n",
      "Loss after epoch #1098 is: train/0.04744897720056681 --- test/2.160731540563945\n",
      "Loss after epoch #1099 is: train/0.04729251424260634 --- test/2.15272587494861\n",
      "Loss after epoch #1100 is: train/0.04737038717437418 --- test/2.149300246489214\n",
      "Loss after epoch #1101 is: train/0.04711526937379299 --- test/2.1440600778640673\n",
      "Loss after epoch #1102 is: train/0.04703225603378579 --- test/2.148603924776447\n",
      "Loss after epoch #1103 is: train/0.04682010329668483 --- test/2.141763595185696\n",
      "Loss after epoch #1104 is: train/0.04687399235821432 --- test/2.1372026206857253\n",
      "Loss after epoch #1105 is: train/0.04675373192841469 --- test/2.1377053793415244\n",
      "Loss after epoch #1106 is: train/0.046772031510739276 --- test/2.1325800557643992\n",
      "Loss after epoch #1107 is: train/0.04670774358331447 --- test/2.1360832111399723\n",
      "Loss after epoch #1108 is: train/0.04660403961232883 --- test/2.135158906100137\n",
      "Loss after epoch #1109 is: train/0.046696267283519936 --- test/2.130864019185579\n",
      "Loss after epoch #1110 is: train/0.04666115564492589 --- test/2.1315399651728555\n",
      "Loss after epoch #1111 is: train/0.046592694194369684 --- test/2.142839683095157\n",
      "Loss after epoch #1112 is: train/0.04660416819417824 --- test/2.143364444665054\n",
      "Loss after epoch #1113 is: train/0.04651860418689481 --- test/2.137997979018623\n",
      "Loss after epoch #1114 is: train/0.04649954501241147 --- test/2.123131398227011\n",
      "Loss after epoch #1115 is: train/0.046555888438498366 --- test/2.1285772343427403\n",
      "Loss after epoch #1116 is: train/0.04655205954738238 --- test/2.125026901074306\n",
      "Loss after epoch #1117 is: train/0.0466261155487247 --- test/2.123490185003513\n",
      "Loss after epoch #1118 is: train/0.046535898116353884 --- test/2.1240205559205\n",
      "Loss after epoch #1119 is: train/0.04642121575757213 --- test/2.1149433080445856\n",
      "Loss after epoch #1120 is: train/0.0460569482328679 --- test/2.110923970170354\n",
      "Loss after epoch #1121 is: train/0.04620066516885629 --- test/2.1057198384978557\n",
      "Loss after epoch #1122 is: train/0.04607948499474855 --- test/2.1048208710969973\n",
      "Loss after epoch #1123 is: train/0.04593224493622648 --- test/2.1087049864593155\n",
      "Loss after epoch #1124 is: train/0.04580835179251387 --- test/2.1105501712338035\n",
      "Loss after epoch #1125 is: train/0.045670647967155725 --- test/2.1043128281118824\n",
      "Loss after epoch #1126 is: train/0.04560000590524322 --- test/2.0999578172935\n",
      "Loss after epoch #1127 is: train/0.04545737817581521 --- test/2.097305135252581\n",
      "Loss after epoch #1128 is: train/0.04556864482425524 --- test/2.1010321467178055\n",
      "Loss after epoch #1129 is: train/0.045553554434646794 --- test/2.0965282049003693\n",
      "Loss after epoch #1130 is: train/0.0456100144000779 --- test/2.0906527735561022\n",
      "Loss after epoch #1131 is: train/0.045649903948256965 --- test/2.086204352299356\n",
      "Loss after epoch #1132 is: train/0.04556781823695941 --- test/2.0891497969770247\n",
      "Loss after epoch #1133 is: train/0.04571241974195472 --- test/2.0869553262234035\n",
      "Loss after epoch #1134 is: train/0.045618269810293766 --- test/2.078630173911769\n",
      "Loss after epoch #1135 is: train/0.045398561829457694 --- test/2.0740597212120373\n",
      "Loss after epoch #1136 is: train/0.04539114549022787 --- test/2.0613707005602055\n",
      "Loss after epoch #1137 is: train/0.04520686938875394 --- test/2.0598403215940815\n",
      "Loss after epoch #1138 is: train/0.04509409283518265 --- test/2.056658507307548\n",
      "Loss after epoch #1139 is: train/0.04503840673464572 --- test/2.062948272703963\n",
      "Loss after epoch #1140 is: train/0.04502219866498475 --- test/2.066129891082673\n",
      "Loss after epoch #1141 is: train/0.04488360495303747 --- test/2.0740993352987043\n",
      "Loss after epoch #1142 is: train/0.044781023625159296 --- test/2.0669059895416106\n",
      "Loss after epoch #1143 is: train/0.04473420758082266 --- test/2.0694332278251992\n",
      "Loss after epoch #1144 is: train/0.04485202374339391 --- test/2.070925813432685\n",
      "Loss after epoch #1145 is: train/0.0448073832090579 --- test/2.069567689592653\n",
      "Loss after epoch #1146 is: train/0.044710061390338195 --- test/2.0674272106851213\n",
      "Loss after epoch #1147 is: train/0.04475424786863334 --- test/2.06953328353785\n",
      "Loss after epoch #1148 is: train/0.044674822354191755 --- test/2.0738196739254717\n",
      "Loss after epoch #1149 is: train/0.04462820975682404 --- test/2.0734034600928886\n",
      "Loss after epoch #1150 is: train/0.044329501291113356 --- test/2.066170021678921\n",
      "Loss after epoch #1151 is: train/0.04409686047402267 --- test/2.0627045335281777\n",
      "Loss after epoch #1152 is: train/0.044267078462311954 --- test/2.0615989517473787\n",
      "Loss after epoch #1153 is: train/0.04432031110238559 --- test/2.0681761150282907\n",
      "Loss after epoch #1154 is: train/0.04419294024210453 --- test/2.0588354264471724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1155 is: train/0.044212251296475914 --- test/2.050864384859794\n",
      "Loss after epoch #1156 is: train/0.04414454411181732 --- test/2.042921743297663\n",
      "Loss after epoch #1157 is: train/0.04418013154498548 --- test/2.036986275867851\n",
      "Loss after epoch #1158 is: train/0.04404554608688978 --- test/2.0370101389497886\n",
      "Loss after epoch #1159 is: train/0.04406611974309788 --- test/2.037224899740232\n",
      "Loss after epoch #1160 is: train/0.044036272836113846 --- test/2.0411275507721203\n",
      "Loss after epoch #1161 is: train/0.04398569739152054 --- test/2.045971793303759\n",
      "Loss after epoch #1162 is: train/0.04394975563632903 --- test/2.048440230343323\n",
      "Loss after epoch #1163 is: train/0.044081871014376696 --- test/2.0478004298254513\n",
      "Loss after epoch #1164 is: train/0.044043156360304196 --- test/2.0402869646686024\n",
      "Loss after epoch #1165 is: train/0.04386334849016275 --- test/2.030929008931475\n",
      "Loss after epoch #1166 is: train/0.04376036511681373 --- test/2.0301735257597624\n",
      "Loss after epoch #1167 is: train/0.04363225445111208 --- test/2.0236910075675585\n",
      "Loss after epoch #1168 is: train/0.04355273186219243 --- test/2.0223237309676896\n",
      "Loss after epoch #1169 is: train/0.04351293995210457 --- test/2.019595193942183\n",
      "Loss after epoch #1170 is: train/0.04352784699951012 --- test/2.0188530447200685\n",
      "Loss after epoch #1171 is: train/0.04343968949539049 --- test/2.017860680539562\n",
      "Loss after epoch #1172 is: train/0.04326643361318876 --- test/2.0120054313963087\n",
      "Loss after epoch #1173 is: train/0.04307344388858509 --- test/2.010744994408243\n",
      "Loss after epoch #1174 is: train/0.04325490304678209 --- test/2.009472352669632\n",
      "Loss after epoch #1175 is: train/0.04332217565028092 --- test/2.0115335555079734\n",
      "Loss after epoch #1176 is: train/0.04318276262190744 --- test/2.0108634271430614\n",
      "Loss after epoch #1177 is: train/0.04303637373605876 --- test/2.008921715333179\n",
      "Loss after epoch #1178 is: train/0.04298853384454417 --- test/2.0061425264200965\n",
      "Loss after epoch #1179 is: train/0.04283993420665856 --- test/2.0085861399798475\n",
      "Loss after epoch #1180 is: train/0.042981271641741974 --- test/2.0047850071620275\n",
      "Loss after epoch #1181 is: train/0.042922418022310095 --- test/2.000113015631767\n",
      "Loss after epoch #1182 is: train/0.04302231884693452 --- test/1.9909592443972317\n",
      "Loss after epoch #1183 is: train/0.04294645940784692 --- test/1.9898243312821189\n",
      "Loss after epoch #1184 is: train/0.042713616948073144 --- test/1.9843432760136028\n",
      "Loss after epoch #1185 is: train/0.042796754024988934 --- test/1.985835477783427\n",
      "Loss after epoch #1186 is: train/0.042626465620190034 --- test/1.9936954047024393\n",
      "Loss after epoch #1187 is: train/0.042709137032887534 --- test/1.9889562094643323\n",
      "Loss after epoch #1188 is: train/0.042603779788977524 --- test/1.9861498297549127\n",
      "Loss after epoch #1189 is: train/0.04260948154385329 --- test/1.9915075738501575\n",
      "Loss after epoch #1190 is: train/0.04261478459484213 --- test/1.9866136964223833\n",
      "Loss after epoch #1191 is: train/0.042605930938942445 --- test/1.9824435539485568\n",
      "Loss after epoch #1192 is: train/0.042799244975300674 --- test/1.983749723198421\n",
      "Loss after epoch #1193 is: train/0.04252296570121959 --- test/1.9783467641393198\n",
      "Loss after epoch #1194 is: train/0.042480639419390606 --- test/1.9820295149794596\n",
      "Loss after epoch #1195 is: train/0.042327431443248194 --- test/1.9796854659863112\n",
      "Loss after epoch #1196 is: train/0.04214974403529562 --- test/1.976715785115342\n",
      "Loss after epoch #1197 is: train/0.04207966313164331 --- test/1.9750486752492777\n",
      "Loss after epoch #1198 is: train/0.04219015793328183 --- test/1.9719824411357416\n",
      "Loss after epoch #1199 is: train/0.0420196908289093 --- test/1.966489443861613\n",
      "Loss after epoch #1200 is: train/0.04214754750412895 --- test/1.9665719443595722\n",
      "Loss after epoch #1201 is: train/0.04211419272635968 --- test/1.9627181962549152\n",
      "Loss after epoch #1202 is: train/0.04180456491355804 --- test/1.9674616553947215\n",
      "Loss after epoch #1203 is: train/0.04155676231601236 --- test/1.9668849112324405\n",
      "Loss after epoch #1204 is: train/0.041534883222624004 --- test/1.9643134581198525\n",
      "Loss after epoch #1205 is: train/0.041621756104893 --- test/1.9600426033255043\n",
      "Loss after epoch #1206 is: train/0.04158408718924445 --- test/1.9630349507267235\n",
      "Loss after epoch #1207 is: train/0.04145603761566479 --- test/1.959022494731384\n",
      "Loss after epoch #1208 is: train/0.0413793291994025 --- test/1.9493571375175058\n",
      "Loss after epoch #1209 is: train/0.041386764457421 --- test/1.9520743191916525\n",
      "Loss after epoch #1210 is: train/0.041587018932643584 --- test/1.9488128584515572\n",
      "Loss after epoch #1211 is: train/0.041526135733977704 --- test/1.9441794305219204\n",
      "Loss after epoch #1212 is: train/0.04146143281119203 --- test/1.9423060155503102\n",
      "Loss after epoch #1213 is: train/0.04139138415599238 --- test/1.948111421741713\n",
      "Loss after epoch #1214 is: train/0.04139460640524955 --- test/1.9487782409019212\n",
      "Loss after epoch #1215 is: train/0.041455422450078105 --- test/1.9469562585952163\n",
      "Loss after epoch #1216 is: train/0.041360947227333134 --- test/1.9412033710140468\n",
      "Loss after epoch #1217 is: train/0.0412034017941957 --- test/1.9422440828622012\n",
      "Loss after epoch #1218 is: train/0.04096452356759813 --- test/1.939387100919864\n",
      "Loss after epoch #1219 is: train/0.040947608859492106 --- test/1.9398756649558442\n",
      "Loss after epoch #1220 is: train/0.04100015479039357 --- test/1.9414131473547267\n",
      "Loss after epoch #1221 is: train/0.04104463146112309 --- test/1.9396373425839752\n",
      "Loss after epoch #1222 is: train/0.041080917434753846 --- test/1.9388234771329251\n",
      "Loss after epoch #1223 is: train/0.040962696421794585 --- test/1.9430324410280229\n",
      "Loss after epoch #1224 is: train/0.041088766587234204 --- test/1.944176570259901\n",
      "Loss after epoch #1225 is: train/0.04092590913474628 --- test/1.9337203594427095\n",
      "Loss after epoch #1226 is: train/0.040973249733259234 --- test/1.9374285942709608\n",
      "Loss after epoch #1227 is: train/0.04099425789721437 --- test/1.93776716335368\n",
      "Loss after epoch #1228 is: train/0.04088161596360615 --- test/1.936959022373332\n",
      "Loss after epoch #1229 is: train/0.04074659985190431 --- test/1.9369578878382718\n",
      "Loss after epoch #1230 is: train/0.04083986179341779 --- test/1.9387785392645396\n",
      "Loss after epoch #1231 is: train/0.040758036271963456 --- test/1.9375114160519789\n",
      "Loss after epoch #1232 is: train/0.04085110979314028 --- test/1.9369349131983848\n",
      "Loss after epoch #1233 is: train/0.04071858267991751 --- test/1.929324294320567\n",
      "Loss after epoch #1234 is: train/0.04072339469389914 --- test/1.925522555803276\n",
      "Loss after epoch #1235 is: train/0.04092317836993448 --- test/1.926309069575179\n",
      "Loss after epoch #1236 is: train/0.040986872256213204 --- test/1.921438500813038\n",
      "Loss after epoch #1237 is: train/0.04077074885600898 --- test/1.9118212945780408\n",
      "Loss after epoch #1238 is: train/0.04049436787178288 --- test/1.9083573383270513\n",
      "Loss after epoch #1239 is: train/0.04059362458389226 --- test/1.910584535708147\n",
      "Loss after epoch #1240 is: train/0.04072295738698694 --- test/1.9147364946187624\n",
      "Loss after epoch #1241 is: train/0.040716363021303215 --- test/1.9180700406003748\n",
      "Loss after epoch #1242 is: train/0.04061136542217991 --- test/1.9236401675793762\n",
      "Loss after epoch #1243 is: train/0.04038666292775735 --- test/1.9200535272871437\n",
      "Loss after epoch #1244 is: train/0.04028480495034921 --- test/1.9110297030107288\n",
      "Loss after epoch #1245 is: train/0.040327618125542274 --- test/1.913055834325787\n",
      "Loss after epoch #1246 is: train/0.04015892491121507 --- test/1.9130867895003651\n",
      "Loss after epoch #1247 is: train/0.04020049691244321 --- test/1.9143107585242767\n",
      "Loss after epoch #1248 is: train/0.0402223732187887 --- test/1.9071898250103965\n",
      "Loss after epoch #1249 is: train/0.040323062181093924 --- test/1.9033920387711392\n",
      "Loss after epoch #1250 is: train/0.040343736926397734 --- test/1.9049027374237795\n",
      "Loss after epoch #1251 is: train/0.04016309860747587 --- test/1.9010101283912157\n",
      "Loss after epoch #1252 is: train/0.04003957462708476 --- test/1.9025544562014172\n",
      "Loss after epoch #1253 is: train/0.04008899173456586 --- test/1.8980765300869977\n",
      "Loss after epoch #1254 is: train/0.04004762817877867 --- test/1.9002043356124407\n",
      "Loss after epoch #1255 is: train/0.03974614468950871 --- test/1.8897309936533402\n",
      "Loss after epoch #1256 is: train/0.03975401731543535 --- test/1.895036500170026\n",
      "Loss after epoch #1257 is: train/0.03972217606406739 --- test/1.8955827487098964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1258 is: train/0.039920288219256056 --- test/1.8949923745243464\n",
      "Loss after epoch #1259 is: train/0.03975408126057952 --- test/1.8898153026323044\n",
      "Loss after epoch #1260 is: train/0.03971512959891436 --- test/1.8929820030335978\n",
      "Loss after epoch #1261 is: train/0.03949260002826699 --- test/1.8932294049028853\n",
      "Loss after epoch #1262 is: train/0.03933544039770766 --- test/1.889815660757332\n",
      "Loss after epoch #1263 is: train/0.039429689936404694 --- test/1.8849799187777805\n",
      "Loss after epoch #1264 is: train/0.0395103313575206 --- test/1.8891193541823887\n",
      "Loss after epoch #1265 is: train/0.03937515740038731 --- test/1.8842031926777887\n",
      "Loss after epoch #1266 is: train/0.039415701440445386 --- test/1.8820897010694795\n",
      "Loss after epoch #1267 is: train/0.03944433186381811 --- test/1.8839249737478412\n",
      "Loss after epoch #1268 is: train/0.03970578965198561 --- test/1.8851855036157992\n",
      "Loss after epoch #1269 is: train/0.03955813978347347 --- test/1.8824036964337532\n",
      "Loss after epoch #1270 is: train/0.03935253970633601 --- test/1.887064404884448\n",
      "Loss after epoch #1271 is: train/0.03930188963472585 --- test/1.8834035970647203\n",
      "Loss after epoch #1272 is: train/0.03930896083822104 --- test/1.879539253543189\n",
      "Loss after epoch #1273 is: train/0.0394155860459515 --- test/1.875285970901203\n",
      "Loss after epoch #1274 is: train/0.03951359710420937 --- test/1.8714560541759613\n",
      "Loss after epoch #1275 is: train/0.03931620702276397 --- test/1.8659283654178977\n",
      "Loss after epoch #1276 is: train/0.03918817067169201 --- test/1.8659433072196847\n",
      "Loss after epoch #1277 is: train/0.03883608267149839 --- test/1.8681382126320343\n",
      "Loss after epoch #1278 is: train/0.03891401890025405 --- test/1.8758450126155555\n",
      "Loss after epoch #1279 is: train/0.03900050417196459 --- test/1.8708119624373065\n",
      "Loss after epoch #1280 is: train/0.03897331769388428 --- test/1.8632350600691618\n",
      "Loss after epoch #1281 is: train/0.03900944689704738 --- test/1.869139333637918\n",
      "Loss after epoch #1282 is: train/0.03886074891855914 --- test/1.862315390849116\n",
      "Loss after epoch #1283 is: train/0.03891573193742504 --- test/1.861279147895277\n",
      "Loss after epoch #1284 is: train/0.038894208904870844 --- test/1.862716687925688\n",
      "Loss after epoch #1285 is: train/0.03865692744430109 --- test/1.863066088348271\n",
      "Loss after epoch #1286 is: train/0.03864705681598992 --- test/1.860348830272607\n",
      "Loss after epoch #1287 is: train/0.03861697760080656 --- test/1.858819404958973\n",
      "Loss after epoch #1288 is: train/0.03862627735926699 --- test/1.858294443835031\n",
      "Loss after epoch #1289 is: train/0.03867736064584135 --- test/1.8549818271044358\n",
      "Loss after epoch #1290 is: train/0.03849316632737824 --- test/1.8544429164815937\n",
      "Loss after epoch #1291 is: train/0.038512552312821656 --- test/1.8534892556560936\n",
      "Loss after epoch #1292 is: train/0.03849685458735119 --- test/1.8457695508161278\n",
      "Loss after epoch #1293 is: train/0.038451427550318794 --- test/1.8433680801865362\n",
      "Loss after epoch #1294 is: train/0.038302394310865774 --- test/1.8450736345779049\n",
      "Loss after epoch #1295 is: train/0.03824883030204304 --- test/1.843270822474517\n",
      "Loss after epoch #1296 is: train/0.038344561878392164 --- test/1.8388975462509598\n",
      "Loss after epoch #1297 is: train/0.03843050989573764 --- test/1.8335284435883106\n",
      "Loss after epoch #1298 is: train/0.03838121604729879 --- test/1.8313460488443587\n",
      "Loss after epoch #1299 is: train/0.038305809433772604 --- test/1.8276492623761995\n",
      "Loss after epoch #1300 is: train/0.03829705422472814 --- test/1.8254973076033119\n",
      "Loss after epoch #1301 is: train/0.038190825202751355 --- test/1.823782004860913\n",
      "Loss after epoch #1302 is: train/0.0381010733849272 --- test/1.8276275649592364\n",
      "Loss after epoch #1303 is: train/0.038048888820716144 --- test/1.8279243717372515\n",
      "Loss after epoch #1304 is: train/0.03810674519746925 --- test/1.8303191810734183\n",
      "Loss after epoch #1305 is: train/0.03799996465914913 --- test/1.8328319008626714\n",
      "Loss after epoch #1306 is: train/0.03799131545915651 --- test/1.8302427791546716\n",
      "Loss after epoch #1307 is: train/0.03791854408114236 --- test/1.8370145077793678\n",
      "Loss after epoch #1308 is: train/0.037932761442805905 --- test/1.8311753956718444\n",
      "Loss after epoch #1309 is: train/0.037959160155155436 --- test/1.8346574470903017\n",
      "Loss after epoch #1310 is: train/0.038065971591447124 --- test/1.8342910921933078\n",
      "Loss after epoch #1311 is: train/0.038125492319075874 --- test/1.822394350114241\n",
      "Loss after epoch #1312 is: train/0.03795270057473861 --- test/1.8234735762670646\n",
      "Loss after epoch #1313 is: train/0.03790791183398334 --- test/1.8180312809622021\n",
      "Loss after epoch #1314 is: train/0.03797789607250435 --- test/1.8153061343516648\n",
      "Loss after epoch #1315 is: train/0.03810650312714615 --- test/1.8169926212650171\n",
      "Loss after epoch #1316 is: train/0.03803005190949162 --- test/1.8126911465113118\n",
      "Loss after epoch #1317 is: train/0.037987827662977744 --- test/1.8094952252718721\n",
      "Loss after epoch #1318 is: train/0.037777986966292296 --- test/1.8042345763821506\n",
      "Loss after epoch #1319 is: train/0.03778185785099642 --- test/1.8000104381273476\n",
      "Loss after epoch #1320 is: train/0.03766956185005888 --- test/1.7997375511443348\n",
      "Loss after epoch #1321 is: train/0.037640427656029304 --- test/1.8033983238099247\n",
      "Loss after epoch #1322 is: train/0.03766822841311827 --- test/1.8024487273585006\n",
      "Loss after epoch #1323 is: train/0.03746397681230713 --- test/1.8003209441826675\n",
      "Loss after epoch #1324 is: train/0.037437714636992185 --- test/1.8019637993684718\n",
      "Loss after epoch #1325 is: train/0.03764808196348127 --- test/1.8026737985235213\n",
      "Loss after epoch #1326 is: train/0.0375577342020746 --- test/1.8058811343816425\n",
      "Loss after epoch #1327 is: train/0.03738011454443928 --- test/1.803413226553513\n",
      "Loss after epoch #1328 is: train/0.037553011551322645 --- test/1.8044519119811\n",
      "Loss after epoch #1329 is: train/0.037460330158074345 --- test/1.7988538966889824\n",
      "Loss after epoch #1330 is: train/0.03719119090610978 --- test/1.7951620982338805\n",
      "Loss after epoch #1331 is: train/0.03721973729798968 --- test/1.7944659230484112\n",
      "Loss after epoch #1332 is: train/0.03741909468531451 --- test/1.7969394019382054\n",
      "Loss after epoch #1333 is: train/0.03731397198888047 --- test/1.7948730668915085\n",
      "Loss after epoch #1334 is: train/0.037330391426956246 --- test/1.7914015695038674\n",
      "Loss after epoch #1335 is: train/0.03728735472162037 --- test/1.7910509566810096\n",
      "Loss after epoch #1336 is: train/0.037143128405833385 --- test/1.783276310125504\n",
      "Loss after epoch #1337 is: train/0.03740918628206991 --- test/1.7841615398811175\n",
      "Loss after epoch #1338 is: train/0.037181587312065154 --- test/1.7890490816656075\n",
      "Loss after epoch #1339 is: train/0.037082821398039235 --- test/1.7896015977985027\n",
      "Loss after epoch #1340 is: train/0.03713612313975384 --- test/1.7855199851854904\n",
      "Loss after epoch #1341 is: train/0.03714739110225284 --- test/1.7828658967984252\n",
      "Loss after epoch #1342 is: train/0.037152163085327546 --- test/1.7796451986718054\n",
      "Loss after epoch #1343 is: train/0.03686715736220207 --- test/1.7755656272001505\n",
      "Loss after epoch #1344 is: train/0.036883600384314284 --- test/1.7778220685014734\n",
      "Loss after epoch #1345 is: train/0.036850301098755534 --- test/1.772893132768761\n",
      "Loss after epoch #1346 is: train/0.036777242379302154 --- test/1.7690289909054777\n",
      "Loss after epoch #1347 is: train/0.03667368513988057 --- test/1.7707227908688188\n",
      "Loss after epoch #1348 is: train/0.03676202157415207 --- test/1.7753853442647043\n",
      "Loss after epoch #1349 is: train/0.0367442037578886 --- test/1.7809749187726651\n",
      "Loss after epoch #1350 is: train/0.03674322479660562 --- test/1.7803967596551316\n",
      "Loss after epoch #1351 is: train/0.03661157572993261 --- test/1.7787966676415607\n",
      "Loss after epoch #1352 is: train/0.0366627409921288 --- test/1.7785789292540735\n",
      "Loss after epoch #1353 is: train/0.036538439723182624 --- test/1.7773624455786108\n",
      "Loss after epoch #1354 is: train/0.03675439795702285 --- test/1.7806296208238277\n",
      "Loss after epoch #1355 is: train/0.036655467773616536 --- test/1.7753652442893633\n",
      "Loss after epoch #1356 is: train/0.03658226915381625 --- test/1.776100686562386\n",
      "Loss after epoch #1357 is: train/0.036594482789889075 --- test/1.7738385640182628\n",
      "Loss after epoch #1358 is: train/0.036514681240499836 --- test/1.78337445787371\n",
      "Loss after epoch #1359 is: train/0.036567897452548216 --- test/1.7831157354726916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1360 is: train/0.03644576764667192 --- test/1.7745939143340315\n",
      "Loss after epoch #1361 is: train/0.036478636183232827 --- test/1.7756844505301803\n",
      "Loss after epoch #1362 is: train/0.036372085638221426 --- test/1.7720961675933034\n",
      "Loss after epoch #1363 is: train/0.03630067846419784 --- test/1.7692334486328223\n",
      "Loss after epoch #1364 is: train/0.03640234057763045 --- test/1.7653710315933326\n",
      "Loss after epoch #1365 is: train/0.036504768295744186 --- test/1.7657345667396225\n",
      "Loss after epoch #1366 is: train/0.036776608719641654 --- test/1.7662011879027926\n",
      "Loss after epoch #1367 is: train/0.03655857797896926 --- test/1.766984753221205\n",
      "Loss after epoch #1368 is: train/0.036658518498745886 --- test/1.7658120636278347\n",
      "Loss after epoch #1369 is: train/0.03669995013853689 --- test/1.7539237132416994\n",
      "Loss after epoch #1370 is: train/0.03652351456445605 --- test/1.7604524378341224\n",
      "Loss after epoch #1371 is: train/0.036300124106808156 --- test/1.7528387259278628\n",
      "Loss after epoch #1372 is: train/0.036462368354060304 --- test/1.7536553930723116\n",
      "Loss after epoch #1373 is: train/0.03632706674285124 --- test/1.7509061896457154\n",
      "Loss after epoch #1374 is: train/0.03627711179369113 --- test/1.749858050303668\n",
      "Loss after epoch #1375 is: train/0.0361854252358963 --- test/1.7516363621424436\n",
      "Loss after epoch #1376 is: train/0.0360402811814831 --- test/1.7562281153049335\n",
      "Loss after epoch #1377 is: train/0.03607067654650561 --- test/1.754861841807179\n",
      "Loss after epoch #1378 is: train/0.03586629815348677 --- test/1.7511523139955116\n",
      "Loss after epoch #1379 is: train/0.0360276461092479 --- test/1.7513675051631945\n",
      "Loss after epoch #1380 is: train/0.03587991121797837 --- test/1.7434142952807672\n",
      "Loss after epoch #1381 is: train/0.03584792662378384 --- test/1.750234515264411\n",
      "Loss after epoch #1382 is: train/0.03586220657658579 --- test/1.7519697503855016\n",
      "Loss after epoch #1383 is: train/0.035852322951379516 --- test/1.7537871944766332\n",
      "Loss after epoch #1384 is: train/0.03586021952794511 --- test/1.7548773685678496\n",
      "Loss after epoch #1385 is: train/0.035840825373373075 --- test/1.7568169343774314\n",
      "Loss after epoch #1386 is: train/0.03602604305900232 --- test/1.7573049661057112\n",
      "Loss after epoch #1387 is: train/0.03585061351674702 --- test/1.7464108681679842\n",
      "Loss after epoch #1388 is: train/0.0359232406765251 --- test/1.7547368149400944\n",
      "Loss after epoch #1389 is: train/0.03587034790839987 --- test/1.7505545007324044\n",
      "Loss after epoch #1390 is: train/0.03570215467490464 --- test/1.7468663534542535\n",
      "Loss after epoch #1391 is: train/0.035655189969437215 --- test/1.743361948456036\n",
      "Loss after epoch #1392 is: train/0.03573060557796318 --- test/1.7426637216194063\n",
      "Loss after epoch #1393 is: train/0.03566759863916203 --- test/1.742562255552201\n",
      "Loss after epoch #1394 is: train/0.03563015291460216 --- test/1.7397807034028776\n",
      "Loss after epoch #1395 is: train/0.03552826685245316 --- test/1.7421920065674932\n",
      "Loss after epoch #1396 is: train/0.03547958780261725 --- test/1.7372515204902768\n",
      "Loss after epoch #1397 is: train/0.03546566930801961 --- test/1.7346644788249133\n",
      "Loss after epoch #1398 is: train/0.035421002802478965 --- test/1.7320835911113537\n",
      "Loss after epoch #1399 is: train/0.035462034407146736 --- test/1.7337817045765\n",
      "Loss after epoch #1400 is: train/0.03550166501637003 --- test/1.7346501627485404\n",
      "Loss after epoch #1401 is: train/0.035406330344710336 --- test/1.739843636915642\n",
      "Loss after epoch #1402 is: train/0.035415249938966935 --- test/1.735988638975214\n",
      "Loss after epoch #1403 is: train/0.03530068398286983 --- test/1.7390741124345253\n",
      "Loss after epoch #1404 is: train/0.035335712131967234 --- test/1.7346435521949888\n",
      "Loss after epoch #1405 is: train/0.035192753786116554 --- test/1.7346540595866229\n",
      "Loss after epoch #1406 is: train/0.035313791820664756 --- test/1.7415769036811672\n",
      "Loss after epoch #1407 is: train/0.035317719564993244 --- test/1.7363053386083653\n",
      "Loss after epoch #1408 is: train/0.0351453852232571 --- test/1.7296170365203545\n",
      "Loss after epoch #1409 is: train/0.03525747988897735 --- test/1.7314857176431353\n",
      "Loss after epoch #1410 is: train/0.03514489771056371 --- test/1.7307654660011855\n",
      "Loss after epoch #1411 is: train/0.03521289419268582 --- test/1.7286909076776338\n",
      "Loss after epoch #1412 is: train/0.03529291680493954 --- test/1.7252336231168008\n",
      "Loss after epoch #1413 is: train/0.03505223693448337 --- test/1.7222263951045402\n",
      "Loss after epoch #1414 is: train/0.03487182157274847 --- test/1.7147990909669208\n",
      "Loss after epoch #1415 is: train/0.03503758460904346 --- test/1.7156711511737264\n",
      "Loss after epoch #1416 is: train/0.035070845504760965 --- test/1.7206362753568911\n",
      "Loss after epoch #1417 is: train/0.0349518522367033 --- test/1.7260616902934312\n",
      "Loss after epoch #1418 is: train/0.034888176666212994 --- test/1.7258566231070476\n",
      "Loss after epoch #1419 is: train/0.034955321948990294 --- test/1.7187269938316494\n",
      "Loss after epoch #1420 is: train/0.03502660616692079 --- test/1.7143376288411536\n",
      "Loss after epoch #1421 is: train/0.035165807185614364 --- test/1.7200491981540802\n",
      "Loss after epoch #1422 is: train/0.03499883216735971 --- test/1.7148378765904808\n",
      "Loss after epoch #1423 is: train/0.0351067236355312 --- test/1.7146175151630616\n",
      "Loss after epoch #1424 is: train/0.035089203275229935 --- test/1.7154934975788994\n",
      "Loss after epoch #1425 is: train/0.03496081128197859 --- test/1.7180138341088278\n",
      "Loss after epoch #1426 is: train/0.03492690429592286 --- test/1.7131090414903671\n",
      "Loss after epoch #1427 is: train/0.034861250909519255 --- test/1.7114043349742039\n",
      "Loss after epoch #1428 is: train/0.034679948691049844 --- test/1.7128878409656418\n",
      "Loss after epoch #1429 is: train/0.03467875925765096 --- test/1.7122571621074947\n",
      "Loss after epoch #1430 is: train/0.034624910526317616 --- test/1.719846089111729\n",
      "Loss after epoch #1431 is: train/0.03451023457116526 --- test/1.7136669708842187\n",
      "Loss after epoch #1432 is: train/0.0346692032281553 --- test/1.713821899790373\n",
      "Loss after epoch #1433 is: train/0.0346148648915056 --- test/1.709482097871356\n",
      "Loss after epoch #1434 is: train/0.03449580701033584 --- test/1.7093337567132842\n",
      "Loss after epoch #1435 is: train/0.03462762300466032 --- test/1.710212126329554\n",
      "Loss after epoch #1436 is: train/0.03456373765428367 --- test/1.7127122834458652\n",
      "Loss after epoch #1437 is: train/0.03454230317930085 --- test/1.712022524944648\n",
      "Loss after epoch #1438 is: train/0.034444120317418454 --- test/1.712330379477718\n",
      "Loss after epoch #1439 is: train/0.03450994406499211 --- test/1.711054703913675\n",
      "Loss after epoch #1440 is: train/0.03449531397240296 --- test/1.7107101818808472\n",
      "Loss after epoch #1441 is: train/0.034529608677555786 --- test/1.7103178477538123\n",
      "Loss after epoch #1442 is: train/0.03439503582383395 --- test/1.7066372047910148\n",
      "Loss after epoch #1443 is: train/0.03456042734750104 --- test/1.7108971358772649\n",
      "Loss after epoch #1444 is: train/0.03454056787585465 --- test/1.705926884607367\n",
      "Loss after epoch #1445 is: train/0.03445190469401013 --- test/1.70544813044271\n",
      "Loss after epoch #1446 is: train/0.03443639560942598 --- test/1.6969005704788276\n",
      "Loss after epoch #1447 is: train/0.034295365399081186 --- test/1.6913688960428344\n",
      "Loss after epoch #1448 is: train/0.03427190943801247 --- test/1.6893017841736815\n",
      "Loss after epoch #1449 is: train/0.03427627166684209 --- test/1.6859988571473188\n",
      "Loss after epoch #1450 is: train/0.03434579594400421 --- test/1.6826352386840788\n",
      "Loss after epoch #1451 is: train/0.03418766650736244 --- test/1.6836995617560295\n",
      "Loss after epoch #1452 is: train/0.034248509170709485 --- test/1.6791527092740781\n",
      "Loss after epoch #1453 is: train/0.03424454274062202 --- test/1.6838725380356145\n",
      "Loss after epoch #1454 is: train/0.034112061133708835 --- test/1.682057471522145\n",
      "Loss after epoch #1455 is: train/0.034092411587809096 --- test/1.6839846409825463\n",
      "Loss after epoch #1456 is: train/0.034037228769005015 --- test/1.6811989308995872\n",
      "Loss after epoch #1457 is: train/0.034023869744649285 --- test/1.6804192421341155\n",
      "Loss after epoch #1458 is: train/0.03404682650456159 --- test/1.682825031430696\n",
      "Loss after epoch #1459 is: train/0.033964558145425754 --- test/1.679739775324532\n",
      "Loss after epoch #1460 is: train/0.033900709599400364 --- test/1.6806260562634172\n",
      "Loss after epoch #1461 is: train/0.033798542955290375 --- test/1.676916154249152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1462 is: train/0.03388242215042461 --- test/1.6793640909529148\n",
      "Loss after epoch #1463 is: train/0.03400830783225342 --- test/1.6857643782286234\n",
      "Loss after epoch #1464 is: train/0.03420536818659077 --- test/1.6851457779949988\n",
      "Loss after epoch #1465 is: train/0.034187474111009516 --- test/1.6863103312160683\n",
      "Loss after epoch #1466 is: train/0.03418178010617744 --- test/1.6881203109616274\n",
      "Loss after epoch #1467 is: train/0.03409381996252007 --- test/1.681123923835021\n",
      "Loss after epoch #1468 is: train/0.034054222044065344 --- test/1.677574585064519\n",
      "Loss after epoch #1469 is: train/0.03373687781949216 --- test/1.666474831012195\n",
      "Loss after epoch #1470 is: train/0.03389498416257863 --- test/1.6630544861759424\n",
      "Loss after epoch #1471 is: train/0.03377135550196984 --- test/1.663988211626859\n",
      "Loss after epoch #1472 is: train/0.033728582923414685 --- test/1.6614991717141043\n",
      "Loss after epoch #1473 is: train/0.033594630855187345 --- test/1.6636491376057\n",
      "Loss after epoch #1474 is: train/0.033544385851193716 --- test/1.6636882266205912\n",
      "Loss after epoch #1475 is: train/0.033625357232188464 --- test/1.6634331678069885\n",
      "Loss after epoch #1476 is: train/0.033557624143623815 --- test/1.655389685679217\n",
      "Loss after epoch #1477 is: train/0.03354673599894503 --- test/1.6555127256147197\n",
      "Loss after epoch #1478 is: train/0.033783783486821345 --- test/1.6597227500761902\n",
      "Loss after epoch #1479 is: train/0.03378183545168385 --- test/1.662503228345371\n",
      "Loss after epoch #1480 is: train/0.03382863342170816 --- test/1.6620674332755592\n",
      "Loss after epoch #1481 is: train/0.033962019861097856 --- test/1.661966521566536\n",
      "Loss after epoch #1482 is: train/0.03386001538350306 --- test/1.6611502019755895\n",
      "Loss after epoch #1483 is: train/0.03391827066761174 --- test/1.6619140557168088\n",
      "Loss after epoch #1484 is: train/0.03401636646980072 --- test/1.6614094174532281\n",
      "Loss after epoch #1485 is: train/0.03404298037171242 --- test/1.6644391368516507\n",
      "Loss after epoch #1486 is: train/0.03377728239781284 --- test/1.6643576315986794\n",
      "Loss after epoch #1487 is: train/0.03375368800123654 --- test/1.6634065025033957\n",
      "Loss after epoch #1488 is: train/0.03379207792220773 --- test/1.6649910112325987\n",
      "Loss after epoch #1489 is: train/0.03356924704680428 --- test/1.6542879980087233\n",
      "Loss after epoch #1490 is: train/0.03376362674859463 --- test/1.6507725764244305\n",
      "Loss after epoch #1491 is: train/0.03358688730764347 --- test/1.6440799490942615\n",
      "Loss after epoch #1492 is: train/0.033457595351211795 --- test/1.645515511215848\n",
      "Loss after epoch #1493 is: train/0.03344957048106531 --- test/1.6431206506731997\n",
      "Loss after epoch #1494 is: train/0.033486336034855145 --- test/1.6447027994271648\n",
      "Loss after epoch #1495 is: train/0.03341810038802888 --- test/1.648548855634389\n",
      "Loss after epoch #1496 is: train/0.03329591124229326 --- test/1.653635111989465\n",
      "Loss after epoch #1497 is: train/0.033479112226541964 --- test/1.658100634416854\n",
      "Loss after epoch #1498 is: train/0.03335824367039702 --- test/1.6499777879018924\n",
      "Loss after epoch #1499 is: train/0.03328193143504953 --- test/1.64144063631702\n",
      "Loss after epoch #1500 is: train/0.03331992453768634 --- test/1.642946421565195\n",
      "Loss after epoch #1501 is: train/0.03324513458257177 --- test/1.642131428690218\n",
      "Loss after epoch #1502 is: train/0.03322384178550695 --- test/1.6390218305452051\n",
      "Loss after epoch #1503 is: train/0.03325201134695647 --- test/1.6374401509165384\n",
      "Loss after epoch #1504 is: train/0.03322000455801884 --- test/1.6349603367518213\n",
      "Loss after epoch #1505 is: train/0.033348376382580876 --- test/1.6367599146390186\n",
      "Loss after epoch #1506 is: train/0.03311408323984401 --- test/1.6375867812962006\n",
      "Loss after epoch #1507 is: train/0.03318673436945432 --- test/1.639775083562739\n",
      "Loss after epoch #1508 is: train/0.033200575720374624 --- test/1.6361037732136545\n",
      "Loss after epoch #1509 is: train/0.03309860143215995 --- test/1.6328512023547446\n",
      "Loss after epoch #1510 is: train/0.03307759371749542 --- test/1.6369544318292932\n",
      "Loss after epoch #1511 is: train/0.033021784233637684 --- test/1.636786023739516\n",
      "Loss after epoch #1512 is: train/0.033057414677530234 --- test/1.6360648001512759\n",
      "Loss after epoch #1513 is: train/0.032994662394568657 --- test/1.6337979590130436\n",
      "Loss after epoch #1514 is: train/0.03301305834655016 --- test/1.6323072106436325\n",
      "Loss after epoch #1515 is: train/0.03304735130095501 --- test/1.633285097310213\n",
      "Loss after epoch #1516 is: train/0.033003004765990634 --- test/1.6296514882246316\n",
      "Loss after epoch #1517 is: train/0.03300988388654189 --- test/1.6266417624137839\n",
      "Loss after epoch #1518 is: train/0.03300909098095645 --- test/1.630949816339646\n",
      "Loss after epoch #1519 is: train/0.033091729594476364 --- test/1.6260437175334914\n",
      "Loss after epoch #1520 is: train/0.033097463897405976 --- test/1.6325975917181468\n",
      "Loss after epoch #1521 is: train/0.03316485120048723 --- test/1.6373966509266251\n",
      "Loss after epoch #1522 is: train/0.03295044349451478 --- test/1.6418975928145514\n",
      "Loss after epoch #1523 is: train/0.03297574425682964 --- test/1.6417110674275972\n",
      "Loss after epoch #1524 is: train/0.032975916485520146 --- test/1.6398505699522492\n",
      "Loss after epoch #1525 is: train/0.0331137331570267 --- test/1.6459511639394993\n",
      "Loss after epoch #1526 is: train/0.032959720817528904 --- test/1.6336201400544725\n",
      "Loss after epoch #1527 is: train/0.03288201286719902 --- test/1.6354887649088947\n",
      "Loss after epoch #1528 is: train/0.03281716179316264 --- test/1.6351752412826384\n",
      "Loss after epoch #1529 is: train/0.03274901511598506 --- test/1.6280532090031066\n",
      "Loss after epoch #1530 is: train/0.032854443701866166 --- test/1.6266242599621412\n",
      "Loss after epoch #1531 is: train/0.03293606572718576 --- test/1.631622630241595\n",
      "Loss after epoch #1532 is: train/0.03291517950179026 --- test/1.6297126238541144\n",
      "Loss after epoch #1533 is: train/0.03304142224640256 --- test/1.6287055429799455\n",
      "Loss after epoch #1534 is: train/0.03271431157465552 --- test/1.6222293438906035\n",
      "Loss after epoch #1535 is: train/0.032573039272714856 --- test/1.6219274131001293\n",
      "Loss after epoch #1536 is: train/0.032619025812353125 --- test/1.624852389059805\n",
      "Loss after epoch #1537 is: train/0.03252623907900201 --- test/1.624208797834321\n",
      "Loss after epoch #1538 is: train/0.03251588051916329 --- test/1.6218368651961026\n",
      "Loss after epoch #1539 is: train/0.032642800889019184 --- test/1.6197488105233662\n",
      "Loss after epoch #1540 is: train/0.03258339986157136 --- test/1.61580650856742\n",
      "Loss after epoch #1541 is: train/0.03263076580212901 --- test/1.6161248833254762\n",
      "Loss after epoch #1542 is: train/0.03255575892853584 --- test/1.6179242186879053\n",
      "Loss after epoch #1543 is: train/0.03263086564010682 --- test/1.6178810938401265\n",
      "Loss after epoch #1544 is: train/0.03257027316695115 --- test/1.6190756708326384\n",
      "Loss after epoch #1545 is: train/0.03264296971601506 --- test/1.6263040386829972\n",
      "Loss after epoch #1546 is: train/0.032790796687752456 --- test/1.6226762181431065\n",
      "Loss after epoch #1547 is: train/0.032674864884069586 --- test/1.621834840218784\n",
      "Loss after epoch #1548 is: train/0.03259644641854803 --- test/1.6151929693441731\n",
      "Loss after epoch #1549 is: train/0.03254634424250763 --- test/1.6108722322787261\n",
      "Loss after epoch #1550 is: train/0.0325558385376354 --- test/1.6131761351234362\n",
      "Loss after epoch #1551 is: train/0.03249028060815715 --- test/1.6169505127823245\n",
      "Loss after epoch #1552 is: train/0.032410641543062585 --- test/1.6140352710757713\n",
      "Loss after epoch #1553 is: train/0.03221271487622757 --- test/1.6127042031619616\n",
      "Loss after epoch #1554 is: train/0.03222375920240622 --- test/1.6087060583884911\n",
      "Loss after epoch #1555 is: train/0.032131352449617116 --- test/1.6156083240613515\n",
      "Loss after epoch #1556 is: train/0.032185554550468974 --- test/1.6182505509656446\n",
      "Loss after epoch #1557 is: train/0.032165460210195895 --- test/1.6178292133374015\n",
      "Loss after epoch #1558 is: train/0.03211797803611319 --- test/1.6164450689018852\n",
      "Loss after epoch #1559 is: train/0.03218735429183002 --- test/1.6099258415552316\n",
      "Loss after epoch #1560 is: train/0.03224128465134392 --- test/1.6104902876999345\n",
      "Loss after epoch #1561 is: train/0.032214447379228135 --- test/1.6094955491625176\n",
      "Loss after epoch #1562 is: train/0.03217764977294031 --- test/1.6117595514467127\n",
      "Loss after epoch #1563 is: train/0.032107930880382586 --- test/1.6107100518723412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1564 is: train/0.032050241592779666 --- test/1.6055185099632907\n",
      "Loss after epoch #1565 is: train/0.03205036265535924 --- test/1.6033239862428774\n",
      "Loss after epoch #1566 is: train/0.031958092750826596 --- test/1.6050395679443965\n",
      "Loss after epoch #1567 is: train/0.03200957597889094 --- test/1.6081841324129902\n",
      "Loss after epoch #1568 is: train/0.03202053318552496 --- test/1.606461806331575\n",
      "Loss after epoch #1569 is: train/0.03196257147287642 --- test/1.6039503962093125\n",
      "Loss after epoch #1570 is: train/0.03201813011324875 --- test/1.6058933694533217\n",
      "Loss after epoch #1571 is: train/0.03206133591078719 --- test/1.6062636968201103\n",
      "Loss after epoch #1572 is: train/0.03200017277473674 --- test/1.597372520821379\n",
      "Loss after epoch #1573 is: train/0.032032602993670514 --- test/1.590234915183213\n",
      "Loss after epoch #1574 is: train/0.03196034189738992 --- test/1.5912678451085112\n",
      "Loss after epoch #1575 is: train/0.032020182961980025 --- test/1.5929025772701189\n",
      "Loss after epoch #1576 is: train/0.03205676862329254 --- test/1.5891283809318784\n",
      "Loss after epoch #1577 is: train/0.03204477868880443 --- test/1.5909321500839788\n",
      "Loss after epoch #1578 is: train/0.03178701319739955 --- test/1.5867368337048493\n",
      "Loss after epoch #1579 is: train/0.03186873526627257 --- test/1.5823713082290336\n",
      "Loss after epoch #1580 is: train/0.03201099939486957 --- test/1.584001012707795\n",
      "Loss after epoch #1581 is: train/0.03192846729998777 --- test/1.5795073572343976\n",
      "Loss after epoch #1582 is: train/0.031921673327092275 --- test/1.5778423154486965\n",
      "Loss after epoch #1583 is: train/0.031875707233137734 --- test/1.578892867396304\n",
      "Loss after epoch #1584 is: train/0.031774605378858845 --- test/1.580944382575647\n",
      "Loss after epoch #1585 is: train/0.03182663865154507 --- test/1.57779022987319\n",
      "Loss after epoch #1586 is: train/0.031891590337140785 --- test/1.5814974462172775\n",
      "Loss after epoch #1587 is: train/0.03176215432829331 --- test/1.581796472044281\n",
      "Loss after epoch #1588 is: train/0.031822260550168034 --- test/1.5821279850913723\n",
      "Loss after epoch #1589 is: train/0.03169980422885284 --- test/1.5777396221631095\n",
      "Loss after epoch #1590 is: train/0.03159214678782406 --- test/1.5786096392806763\n",
      "Loss after epoch #1591 is: train/0.03158974346337005 --- test/1.5744922439059807\n",
      "Loss after epoch #1592 is: train/0.031638650454995115 --- test/1.574809326939037\n",
      "Loss after epoch #1593 is: train/0.031590019448531174 --- test/1.5783239900146835\n",
      "Loss after epoch #1594 is: train/0.03160991808712191 --- test/1.5769245932839424\n",
      "Loss after epoch #1595 is: train/0.03157681470343924 --- test/1.577297229816431\n",
      "Loss after epoch #1596 is: train/0.03160017517079694 --- test/1.578189030127571\n",
      "Loss after epoch #1597 is: train/0.03141249489315683 --- test/1.5819060685600366\n",
      "Loss after epoch #1598 is: train/0.03133266393309987 --- test/1.5780385588844512\n",
      "Loss after epoch #1599 is: train/0.03162934194694353 --- test/1.5806642425371202\n",
      "Loss after epoch #1600 is: train/0.031463712097705696 --- test/1.5776056780568153\n",
      "Loss after epoch #1601 is: train/0.03142080951990624 --- test/1.5786155726620459\n",
      "Loss after epoch #1602 is: train/0.03153734777822399 --- test/1.5774435644065614\n",
      "Loss after epoch #1603 is: train/0.03159259333489841 --- test/1.5762072489231702\n",
      "Loss after epoch #1604 is: train/0.03157356332525337 --- test/1.5775495114585123\n",
      "Loss after epoch #1605 is: train/0.03154445269468692 --- test/1.5712861933896003\n",
      "Loss after epoch #1606 is: train/0.03163040788123148 --- test/1.5742550120035408\n",
      "Loss after epoch #1607 is: train/0.031851914845948195 --- test/1.5773677307198017\n",
      "Loss after epoch #1608 is: train/0.03156613807658377 --- test/1.5780059411766274\n",
      "Loss after epoch #1609 is: train/0.03162218869740291 --- test/1.5737915429803842\n",
      "Loss after epoch #1610 is: train/0.03143128164416804 --- test/1.5741441219503043\n",
      "Loss after epoch #1611 is: train/0.031386178551854146 --- test/1.5744037262464015\n",
      "Loss after epoch #1612 is: train/0.03130153468952326 --- test/1.576917822722751\n",
      "Loss after epoch #1613 is: train/0.03122568641537317 --- test/1.5787518079809988\n",
      "Loss after epoch #1614 is: train/0.031211134161589577 --- test/1.5743319542187155\n",
      "Loss after epoch #1615 is: train/0.031289380188587075 --- test/1.5723930483909452\n",
      "Loss after epoch #1616 is: train/0.03126474204768099 --- test/1.5669307552528784\n",
      "Loss after epoch #1617 is: train/0.0314006910089307 --- test/1.5664297263690707\n",
      "Loss after epoch #1618 is: train/0.03144172438856617 --- test/1.5670396118170382\n",
      "Loss after epoch #1619 is: train/0.03143479246023747 --- test/1.5714979539300187\n",
      "Loss after epoch #1620 is: train/0.03139487905686357 --- test/1.570265924747829\n",
      "Loss after epoch #1621 is: train/0.03129534401286957 --- test/1.5745385677330845\n",
      "Loss after epoch #1622 is: train/0.031248729329263057 --- test/1.5780797346568227\n",
      "Loss after epoch #1623 is: train/0.031360930125685846 --- test/1.5786124808476387\n",
      "Loss after epoch #1624 is: train/0.0311735168288761 --- test/1.5737192702953227\n",
      "Loss after epoch #1625 is: train/0.031063324813376057 --- test/1.5663328935820704\n",
      "Loss after epoch #1626 is: train/0.031189265660112278 --- test/1.569495057562166\n",
      "Loss after epoch #1627 is: train/0.03118036616754988 --- test/1.570353193685207\n",
      "Loss after epoch #1628 is: train/0.03107360594898791 --- test/1.567068870681156\n",
      "Loss after epoch #1629 is: train/0.030894760690859075 --- test/1.559118760640208\n",
      "Loss after epoch #1630 is: train/0.03079664040041743 --- test/1.5527393206868512\n",
      "Loss after epoch #1631 is: train/0.030828185992660018 --- test/1.5513129033552064\n",
      "Loss after epoch #1632 is: train/0.030956104138016288 --- test/1.5476048967786158\n",
      "Loss after epoch #1633 is: train/0.03099543119238215 --- test/1.5550703385011695\n",
      "Loss after epoch #1634 is: train/0.030881863870802526 --- test/1.5569143439000923\n",
      "Loss after epoch #1635 is: train/0.03096357369657017 --- test/1.5527462217759447\n",
      "Loss after epoch #1636 is: train/0.030809576976482934 --- test/1.5494271493482024\n",
      "Loss after epoch #1637 is: train/0.030745826352067615 --- test/1.5435734450145147\n",
      "Loss after epoch #1638 is: train/0.030846552929052865 --- test/1.5428899541269228\n",
      "Loss after epoch #1639 is: train/0.030795930762224255 --- test/1.543741074408533\n",
      "Loss after epoch #1640 is: train/0.030747268991603023 --- test/1.5477509634647266\n",
      "Loss after epoch #1641 is: train/0.030836384204687574 --- test/1.5496463736643136\n",
      "Loss after epoch #1642 is: train/0.031075150361770967 --- test/1.5485480508499376\n",
      "Loss after epoch #1643 is: train/0.030967962567214926 --- test/1.54627455136321\n",
      "Loss after epoch #1644 is: train/0.031013802019180147 --- test/1.5524040567758017\n",
      "Loss after epoch #1645 is: train/0.030985669684189285 --- test/1.5511141400252584\n",
      "Loss after epoch #1646 is: train/0.030927200965420395 --- test/1.5512910354826037\n",
      "Loss after epoch #1647 is: train/0.030853707738964665 --- test/1.5512756763442306\n",
      "Loss after epoch #1648 is: train/0.030752619001209694 --- test/1.5494021937545561\n",
      "Loss after epoch #1649 is: train/0.03078982280110323 --- test/1.5543611151287635\n",
      "Loss after epoch #1650 is: train/0.03082824102469479 --- test/1.5521919404826734\n",
      "Loss after epoch #1651 is: train/0.030746126903685313 --- test/1.5526950348525825\n",
      "Loss after epoch #1652 is: train/0.030659125757311858 --- test/1.549612597932518\n",
      "Loss after epoch #1653 is: train/0.03063028013559386 --- test/1.5452328168556682\n",
      "Loss after epoch #1654 is: train/0.030560820275670857 --- test/1.5495931757421448\n",
      "Loss after epoch #1655 is: train/0.03071480007414487 --- test/1.5477463580497732\n",
      "Loss after epoch #1656 is: train/0.03078474271221067 --- test/1.5496160649145045\n",
      "Loss after epoch #1657 is: train/0.030670390388212062 --- test/1.5512643815949472\n",
      "Loss after epoch #1658 is: train/0.030743967441449683 --- test/1.5497167478717253\n",
      "Loss after epoch #1659 is: train/0.03060940730770545 --- test/1.5476498012472666\n",
      "Loss after epoch #1660 is: train/0.03067909214624361 --- test/1.550466182904633\n",
      "Loss after epoch #1661 is: train/0.030665451861867612 --- test/1.5499476248562367\n",
      "Loss after epoch #1662 is: train/0.030766807187253265 --- test/1.5434907514345888\n",
      "Loss after epoch #1663 is: train/0.030610547379324546 --- test/1.5394845625082123\n",
      "Loss after epoch #1664 is: train/0.030623459905209458 --- test/1.5411166266492704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1665 is: train/0.030541350406174082 --- test/1.542716555955003\n",
      "Loss after epoch #1666 is: train/0.030539370281649516 --- test/1.5425209717730788\n",
      "Loss after epoch #1667 is: train/0.03064074084067306 --- test/1.5462206098077085\n",
      "Loss after epoch #1668 is: train/0.030592709643936126 --- test/1.5460812570866553\n",
      "Loss after epoch #1669 is: train/0.030501617362503976 --- test/1.542881903770207\n",
      "Loss after epoch #1670 is: train/0.030627084982520814 --- test/1.5435857467907437\n",
      "Loss after epoch #1671 is: train/0.03069142006313738 --- test/1.5389926723826863\n",
      "Loss after epoch #1672 is: train/0.030545346078219997 --- test/1.54004311866678\n",
      "Loss after epoch #1673 is: train/0.030546517489487484 --- test/1.5410019560745198\n",
      "Loss after epoch #1674 is: train/0.03032360182950739 --- test/1.5368472186982702\n",
      "Loss after epoch #1675 is: train/0.03041958139863654 --- test/1.5421860089045074\n",
      "Loss after epoch #1676 is: train/0.0303537987118865 --- test/1.5373376066249886\n",
      "Loss after epoch #1677 is: train/0.03045142452437813 --- test/1.5423399814279695\n",
      "Loss after epoch #1678 is: train/0.030475992608803196 --- test/1.5377962013973772\n",
      "Loss after epoch #1679 is: train/0.03043679200995212 --- test/1.5397047608578025\n",
      "Loss after epoch #1680 is: train/0.03037230576984715 --- test/1.538201815888986\n",
      "Loss after epoch #1681 is: train/0.030358995815539514 --- test/1.539989015230812\n",
      "Loss after epoch #1682 is: train/0.03030930625516965 --- test/1.537515382109542\n",
      "Loss after epoch #1683 is: train/0.03031043061199949 --- test/1.5358441978450825\n",
      "Loss after epoch #1684 is: train/0.030368685286626913 --- test/1.5363746053757281\n",
      "Loss after epoch #1685 is: train/0.03030460345667057 --- test/1.5373200969665346\n",
      "Loss after epoch #1686 is: train/0.03022568392204487 --- test/1.5339721764270147\n",
      "Loss after epoch #1687 is: train/0.03020708466923023 --- test/1.5311380649836035\n",
      "Loss after epoch #1688 is: train/0.030174731330831227 --- test/1.5332445496815523\n",
      "Loss after epoch #1689 is: train/0.030210579985427415 --- test/1.5289279722184306\n",
      "Loss after epoch #1690 is: train/0.030242535391263883 --- test/1.534764484368134\n",
      "Loss after epoch #1691 is: train/0.03022206292162836 --- test/1.5324785045888865\n",
      "Loss after epoch #1692 is: train/0.030214264061322226 --- test/1.5300118670776746\n",
      "Loss after epoch #1693 is: train/0.030090885920442054 --- test/1.5289730795264078\n",
      "Loss after epoch #1694 is: train/0.03006823511551011 --- test/1.52443272324727\n",
      "Loss after epoch #1695 is: train/0.03009551439738317 --- test/1.5213233194131541\n",
      "Loss after epoch #1696 is: train/0.0301848675812132 --- test/1.5220973728288656\n",
      "Loss after epoch #1697 is: train/0.029995015280693673 --- test/1.5169757331581422\n",
      "Loss after epoch #1698 is: train/0.030049431042880523 --- test/1.5182497149631955\n",
      "Loss after epoch #1699 is: train/0.030146407442036123 --- test/1.5248508944307981\n",
      "Loss after epoch #1700 is: train/0.03021426477349385 --- test/1.5280409435717457\n",
      "Loss after epoch #1701 is: train/0.03020455025450377 --- test/1.5274307700107734\n",
      "Loss after epoch #1702 is: train/0.030193001041205814 --- test/1.527029409009212\n",
      "Loss after epoch #1703 is: train/0.030111466580712874 --- test/1.529120186704833\n",
      "Loss after epoch #1704 is: train/0.030092818699357222 --- test/1.5274347896372695\n",
      "Loss after epoch #1705 is: train/0.030140205769601816 --- test/1.5278236737153106\n",
      "Loss after epoch #1706 is: train/0.029979075397050792 --- test/1.524465832208688\n",
      "Loss after epoch #1707 is: train/0.030027653791795145 --- test/1.5221427748827796\n",
      "Loss after epoch #1708 is: train/0.029985106025891814 --- test/1.523824898996777\n",
      "Loss after epoch #1709 is: train/0.029867293264672506 --- test/1.5233082862684546\n",
      "Loss after epoch #1710 is: train/0.029946177213708534 --- test/1.5180128092069807\n",
      "Loss after epoch #1711 is: train/0.02992861903271971 --- test/1.5226826965914997\n",
      "Loss after epoch #1712 is: train/0.029879386994894227 --- test/1.5274754058252344\n",
      "Loss after epoch #1713 is: train/0.029947398041243554 --- test/1.521925238647152\n",
      "Loss after epoch #1714 is: train/0.029897204078726123 --- test/1.5175311464987926\n",
      "Loss after epoch #1715 is: train/0.029963883235578807 --- test/1.5175615239824685\n",
      "Loss after epoch #1716 is: train/0.029843266991214486 --- test/1.5176660280785974\n",
      "Loss after epoch #1717 is: train/0.02989216436888686 --- test/1.5216773753503705\n",
      "Loss after epoch #1718 is: train/0.030008689485937847 --- test/1.5172722783109576\n",
      "Loss after epoch #1719 is: train/0.029956481884629543 --- test/1.5164028222758414\n",
      "Loss after epoch #1720 is: train/0.02980607986891427 --- test/1.5082630350028015\n",
      "Loss after epoch #1721 is: train/0.029835947355219116 --- test/1.511880722248049\n",
      "Loss after epoch #1722 is: train/0.029726706134240766 --- test/1.5108136755241823\n",
      "Loss after epoch #1723 is: train/0.02960306980031667 --- test/1.507979383717665\n",
      "Loss after epoch #1724 is: train/0.029581785479735147 --- test/1.5084200630296496\n",
      "Loss after epoch #1725 is: train/0.0296680574709944 --- test/1.5095022198630728\n",
      "Loss after epoch #1726 is: train/0.029775618920460267 --- test/1.5137601006484374\n",
      "Loss after epoch #1727 is: train/0.02973940700695156 --- test/1.514404749754353\n",
      "Loss after epoch #1728 is: train/0.02971533949029909 --- test/1.5079243278434207\n",
      "Loss after epoch #1729 is: train/0.029802189558310734 --- test/1.514206051467189\n",
      "Loss after epoch #1730 is: train/0.029843866267036105 --- test/1.5131236051799177\n",
      "Loss after epoch #1731 is: train/0.02976802109308629 --- test/1.5122356751731494\n",
      "Loss after epoch #1732 is: train/0.029660775711612686 --- test/1.509411743805587\n",
      "Loss after epoch #1733 is: train/0.029649534211523096 --- test/1.5156762728891295\n",
      "Loss after epoch #1734 is: train/0.029711117004166617 --- test/1.513201092275473\n",
      "Loss after epoch #1735 is: train/0.029724566810925925 --- test/1.5117278582654397\n",
      "Loss after epoch #1736 is: train/0.02971742392460749 --- test/1.5082573668822377\n",
      "Loss after epoch #1737 is: train/0.029745433054335857 --- test/1.5132618844196588\n",
      "Loss after epoch #1738 is: train/0.029738507172173394 --- test/1.5132188555334076\n",
      "Loss after epoch #1739 is: train/0.029611715002099685 --- test/1.509617340661382\n",
      "Loss after epoch #1740 is: train/0.02965630861466056 --- test/1.513829027308891\n",
      "Loss after epoch #1741 is: train/0.02971504792916044 --- test/1.5145083411769054\n",
      "Loss after epoch #1742 is: train/0.02974618578735294 --- test/1.5120198489046786\n",
      "Loss after epoch #1743 is: train/0.029581880212227553 --- test/1.5115949182426867\n",
      "Loss after epoch #1744 is: train/0.029601912620461163 --- test/1.5081153726737602\n",
      "Loss after epoch #1745 is: train/0.029717748531397362 --- test/1.5068784253870546\n",
      "Loss after epoch #1746 is: train/0.029705317147138345 --- test/1.503979040612088\n",
      "Loss after epoch #1747 is: train/0.02963491416884916 --- test/1.5046523030937495\n",
      "Loss after epoch #1748 is: train/0.02954512298544658 --- test/1.5060152510715872\n",
      "Loss after epoch #1749 is: train/0.02957159486898298 --- test/1.500716608152177\n",
      "Loss after epoch #1750 is: train/0.029674508684723745 --- test/1.5083582605418713\n",
      "Loss after epoch #1751 is: train/0.029582689714879446 --- test/1.504942258347254\n",
      "Loss after epoch #1752 is: train/0.02954770174944823 --- test/1.5058413368325467\n",
      "Loss after epoch #1753 is: train/0.029595636901307124 --- test/1.509089884527906\n",
      "Loss after epoch #1754 is: train/0.029413675706229235 --- test/1.5063946726316357\n",
      "Loss after epoch #1755 is: train/0.029430530632424384 --- test/1.5047753069903047\n",
      "Loss after epoch #1756 is: train/0.029367254677399205 --- test/1.5045214760890888\n",
      "Loss after epoch #1757 is: train/0.029389945092462774 --- test/1.5042069403557305\n",
      "Loss after epoch #1758 is: train/0.029481274672986627 --- test/1.5056109074121802\n",
      "Loss after epoch #1759 is: train/0.029380273923917094 --- test/1.5035872568078883\n",
      "Loss after epoch #1760 is: train/0.029230160738718924 --- test/1.5008395274673425\n",
      "Loss after epoch #1761 is: train/0.02925264730305198 --- test/1.5042972429549073\n",
      "Loss after epoch #1762 is: train/0.029252204784922648 --- test/1.5054145662490919\n",
      "Loss after epoch #1763 is: train/0.02931031698745106 --- test/1.5068770735375667\n",
      "Loss after epoch #1764 is: train/0.029333549111552237 --- test/1.5060913449422582\n",
      "Loss after epoch #1765 is: train/0.02929149366704777 --- test/1.5080409048390704\n",
      "Loss after epoch #1766 is: train/0.02928482660275759 --- test/1.5059186697858495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1767 is: train/0.029282524396299116 --- test/1.508866785720204\n",
      "Loss after epoch #1768 is: train/0.029232677552366925 --- test/1.5086857291929403\n",
      "Loss after epoch #1769 is: train/0.029258836514994755 --- test/1.508028300683653\n",
      "Loss after epoch #1770 is: train/0.029251822578731324 --- test/1.5037080969493073\n",
      "Loss after epoch #1771 is: train/0.029333783435999516 --- test/1.49938514926406\n",
      "Loss after epoch #1772 is: train/0.029333296140445893 --- test/1.4974304966275693\n",
      "Loss after epoch #1773 is: train/0.029421506460175332 --- test/1.5015765344660679\n",
      "Loss after epoch #1774 is: train/0.029390000891603808 --- test/1.4996489410883842\n",
      "Loss after epoch #1775 is: train/0.02922957484912083 --- test/1.4970044371216866\n",
      "Loss after epoch #1776 is: train/0.029184448140341896 --- test/1.493205322001514\n",
      "Loss after epoch #1777 is: train/0.029062663794249693 --- test/1.4943248413866643\n",
      "Loss after epoch #1778 is: train/0.02920567096255905 --- test/1.4997642856205304\n",
      "Loss after epoch #1779 is: train/0.029302547584035627 --- test/1.4937188484603774\n",
      "Loss after epoch #1780 is: train/0.02929023959238383 --- test/1.4944312292694428\n",
      "Loss after epoch #1781 is: train/0.02931772561736222 --- test/1.4974163115678494\n",
      "Loss after epoch #1782 is: train/0.02912302410885347 --- test/1.494531420264074\n",
      "Loss after epoch #1783 is: train/0.029134210730152325 --- test/1.4882735470245883\n",
      "Loss after epoch #1784 is: train/0.0291329803711582 --- test/1.483347164232518\n",
      "Loss after epoch #1785 is: train/0.029018140908580327 --- test/1.4844882903604664\n",
      "Loss after epoch #1786 is: train/0.02905977767738785 --- test/1.4829753793035252\n",
      "Loss after epoch #1787 is: train/0.029132431818387938 --- test/1.481293481058209\n",
      "Loss after epoch #1788 is: train/0.02900052810802509 --- test/1.488102704610669\n",
      "Loss after epoch #1789 is: train/0.029098897493258835 --- test/1.4882928013455545\n",
      "Loss after epoch #1790 is: train/0.029078771102976318 --- test/1.4861804617714598\n",
      "Loss after epoch #1791 is: train/0.029076074077246328 --- test/1.4837581951529257\n",
      "Loss after epoch #1792 is: train/0.02912806652280721 --- test/1.4852557242696465\n",
      "Loss after epoch #1793 is: train/0.02914146529409431 --- test/1.484351192787825\n",
      "Loss after epoch #1794 is: train/0.029175871635374526 --- test/1.4872197943344296\n",
      "Loss after epoch #1795 is: train/0.02905365141149867 --- test/1.4873181745134225\n",
      "Loss after epoch #1796 is: train/0.029261343364861064 --- test/1.4857328058433337\n",
      "Loss after epoch #1797 is: train/0.02921099387407442 --- test/1.4846807427462467\n",
      "Loss after epoch #1798 is: train/0.029024348964799458 --- test/1.4838963922052895\n",
      "Loss after epoch #1799 is: train/0.02894942616922164 --- test/1.4824905916452562\n",
      "Loss after epoch #1800 is: train/0.028933498622597177 --- test/1.4856682855961956\n",
      "Loss after epoch #1801 is: train/0.028930201980294213 --- test/1.4875709304966223\n",
      "Loss after epoch #1802 is: train/0.028965293404012108 --- test/1.4867850274022172\n",
      "Loss after epoch #1803 is: train/0.028923178844206046 --- test/1.4869687285984945\n",
      "Loss after epoch #1804 is: train/0.028811897353737754 --- test/1.4834344279831506\n",
      "Loss after epoch #1805 is: train/0.028728044030946972 --- test/1.4844074889508065\n",
      "Loss after epoch #1806 is: train/0.02870137470366326 --- test/1.4790029064796766\n",
      "Loss after epoch #1807 is: train/0.028677463021057895 --- test/1.4807160233529921\n",
      "Loss after epoch #1808 is: train/0.028615393270073897 --- test/1.4829832757408634\n",
      "Loss after epoch #1809 is: train/0.02883185838868168 --- test/1.4821994293052883\n",
      "Loss after epoch #1810 is: train/0.028794664220365696 --- test/1.4850727708846687\n",
      "Loss after epoch #1811 is: train/0.02873609924948252 --- test/1.4838908901013412\n",
      "Loss after epoch #1812 is: train/0.028744981381141253 --- test/1.4808410368915605\n",
      "Loss after epoch #1813 is: train/0.028757656480843174 --- test/1.481022351134144\n",
      "Loss after epoch #1814 is: train/0.028725898586347078 --- test/1.4826315688287381\n",
      "Loss after epoch #1815 is: train/0.028866910389029787 --- test/1.4803304526265646\n",
      "Loss after epoch #1816 is: train/0.028864115115392473 --- test/1.4756827764893798\n",
      "Loss after epoch #1817 is: train/0.028813645089650165 --- test/1.475264419953264\n",
      "Loss after epoch #1818 is: train/0.02879559977273196 --- test/1.4795454124847922\n",
      "Loss after epoch #1819 is: train/0.028798240745025457 --- test/1.479246435993232\n",
      "Loss after epoch #1820 is: train/0.02873476462495516 --- test/1.4807641232829405\n",
      "Loss after epoch #1821 is: train/0.028694393311084348 --- test/1.478876496989565\n",
      "Loss after epoch #1822 is: train/0.028671963845146406 --- test/1.4733851073000628\n",
      "Loss after epoch #1823 is: train/0.028648172474916345 --- test/1.472281235372722\n",
      "Loss after epoch #1824 is: train/0.02872180740902764 --- test/1.4679381682822825\n",
      "Loss after epoch #1825 is: train/0.02874840563825507 --- test/1.4657602742320965\n",
      "Loss after epoch #1826 is: train/0.028661686672329646 --- test/1.4659214640874658\n",
      "Loss after epoch #1827 is: train/0.028616475376826198 --- test/1.463436424797042\n",
      "Loss after epoch #1828 is: train/0.0286490434731005 --- test/1.4600471006717801\n",
      "Loss after epoch #1829 is: train/0.02861107776947555 --- test/1.4666161260168522\n",
      "Loss after epoch #1830 is: train/0.02856823722039787 --- test/1.4662359709643769\n",
      "Loss after epoch #1831 is: train/0.02860088967540316 --- test/1.4684699823839011\n",
      "Loss after epoch #1832 is: train/0.02863233896890191 --- test/1.4701712510537517\n",
      "Loss after epoch #1833 is: train/0.028661598474619857 --- test/1.4709563171866107\n",
      "Loss after epoch #1834 is: train/0.02859407129407939 --- test/1.4732915688085657\n",
      "Loss after epoch #1835 is: train/0.028569602636768318 --- test/1.4745112354515593\n",
      "Loss after epoch #1836 is: train/0.02853205069860581 --- test/1.476293644058521\n",
      "Loss after epoch #1837 is: train/0.028604039526493517 --- test/1.4728943639842147\n",
      "Loss after epoch #1838 is: train/0.02856625971282339 --- test/1.4686263502307353\n",
      "Loss after epoch #1839 is: train/0.028546440024276525 --- test/1.4661316320664115\n",
      "Loss after epoch #1840 is: train/0.028431160602365516 --- test/1.4645260993828684\n",
      "Loss after epoch #1841 is: train/0.028389589721427716 --- test/1.4641511578815272\n",
      "Loss after epoch #1842 is: train/0.02829808053939297 --- test/1.463447030014398\n",
      "Loss after epoch #1843 is: train/0.028337390462278622 --- test/1.4579608097812207\n",
      "Loss after epoch #1844 is: train/0.028428681958657758 --- test/1.4537925764255937\n",
      "Loss after epoch #1845 is: train/0.028511699065041016 --- test/1.4555012339367392\n",
      "Loss after epoch #1846 is: train/0.028546710453018386 --- test/1.4588303684804782\n",
      "Loss after epoch #1847 is: train/0.028488305833775703 --- test/1.4545164049059707\n",
      "Loss after epoch #1848 is: train/0.028401568423617737 --- test/1.4549259919050335\n",
      "Loss after epoch #1849 is: train/0.028374982257933327 --- test/1.4530546220596248\n",
      "Loss after epoch #1850 is: train/0.028358726878885812 --- test/1.4574659222454351\n",
      "Loss after epoch #1851 is: train/0.028347133310156453 --- test/1.4574986114104167\n",
      "Loss after epoch #1852 is: train/0.028351654278002732 --- test/1.4594634768036912\n",
      "Loss after epoch #1853 is: train/0.028405790367972555 --- test/1.4591537192235908\n",
      "Loss after epoch #1854 is: train/0.028364910925177896 --- test/1.4590754103782275\n",
      "Loss after epoch #1855 is: train/0.028316120155496127 --- test/1.4613980919056497\n",
      "Loss after epoch #1856 is: train/0.028256364870687757 --- test/1.4603322327724186\n",
      "Loss after epoch #1857 is: train/0.028373467250433188 --- test/1.4641058751629763\n",
      "Loss after epoch #1858 is: train/0.02840029648900962 --- test/1.4709644608807892\n",
      "Loss after epoch #1859 is: train/0.02822074496057093 --- test/1.4649154803684739\n",
      "Loss after epoch #1860 is: train/0.028228173423686005 --- test/1.4669430091833027\n",
      "Loss after epoch #1861 is: train/0.028263769632278953 --- test/1.4671725873152452\n",
      "Loss after epoch #1862 is: train/0.028225256188385905 --- test/1.469253651314876\n",
      "Loss after epoch #1863 is: train/0.028258348141661137 --- test/1.4717148415515435\n",
      "Loss after epoch #1864 is: train/0.028229148414712885 --- test/1.47055340063464\n",
      "Loss after epoch #1865 is: train/0.0281948316889922 --- test/1.4676915761910976\n",
      "Loss after epoch #1866 is: train/0.028223319613145347 --- test/1.4664417941180254\n",
      "Loss after epoch #1867 is: train/0.028340471679831867 --- test/1.468142714228377\n",
      "Loss after epoch #1868 is: train/0.02825045412632677 --- test/1.4655401809442863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1869 is: train/0.02821452108462088 --- test/1.4591794486586984\n",
      "Loss after epoch #1870 is: train/0.028128407997288093 --- test/1.457256423447181\n",
      "Loss after epoch #1871 is: train/0.028210840227029632 --- test/1.4579556736167059\n",
      "Loss after epoch #1872 is: train/0.028158440680638653 --- test/1.4596477220002975\n",
      "Loss after epoch #1873 is: train/0.028045122857855986 --- test/1.457582491138293\n",
      "Loss after epoch #1874 is: train/0.028064035520059157 --- test/1.4590172088952067\n",
      "Loss after epoch #1875 is: train/0.027978491697081553 --- test/1.4550163099009705\n",
      "Loss after epoch #1876 is: train/0.027913684825005645 --- test/1.4546288465079233\n",
      "Loss after epoch #1877 is: train/0.02792301625502472 --- test/1.455282049907929\n",
      "Loss after epoch #1878 is: train/0.02798075119426204 --- test/1.4554636783596377\n",
      "Loss after epoch #1879 is: train/0.028090426920779676 --- test/1.4590919352649323\n",
      "Loss after epoch #1880 is: train/0.02813353305580689 --- test/1.4600334255507128\n",
      "Loss after epoch #1881 is: train/0.028211213715439335 --- test/1.4612023581182791\n",
      "Loss after epoch #1882 is: train/0.028252796150588535 --- test/1.4582500653124164\n",
      "Loss after epoch #1883 is: train/0.02829269778879551 --- test/1.4571566510677598\n",
      "Loss after epoch #1884 is: train/0.028072309202536627 --- test/1.4578135640582437\n",
      "Loss after epoch #1885 is: train/0.028027967921235596 --- test/1.4589467966009633\n",
      "Loss after epoch #1886 is: train/0.027993842488171067 --- test/1.4572686680181772\n",
      "Loss after epoch #1887 is: train/0.028049474994170417 --- test/1.4606565414765353\n",
      "Loss after epoch #1888 is: train/0.028094075137126005 --- test/1.4570560354195328\n",
      "Loss after epoch #1889 is: train/0.02805896152525515 --- test/1.4609923929258946\n",
      "Loss after epoch #1890 is: train/0.028080403324081907 --- test/1.4626655375160564\n",
      "Loss after epoch #1891 is: train/0.028038826961555872 --- test/1.45750135664426\n",
      "Loss after epoch #1892 is: train/0.02795676414628572 --- test/1.4589733020642248\n",
      "Loss after epoch #1893 is: train/0.027900552224917524 --- test/1.4551468285953562\n",
      "Loss after epoch #1894 is: train/0.027855873038301664 --- test/1.4496921866191517\n",
      "Loss after epoch #1895 is: train/0.027947738909338493 --- test/1.4523687580231366\n",
      "Loss after epoch #1896 is: train/0.028011191497426296 --- test/1.454864345035326\n",
      "Loss after epoch #1897 is: train/0.028079110551477982 --- test/1.4576714194525073\n",
      "Loss after epoch #1898 is: train/0.02800702346921104 --- test/1.4566427822992427\n",
      "Loss after epoch #1899 is: train/0.027864129762189153 --- test/1.4545407023917722\n",
      "Loss after epoch #1900 is: train/0.027856830028186248 --- test/1.451326331657972\n",
      "Loss after epoch #1901 is: train/0.027991143691313645 --- test/1.445812773177826\n",
      "Loss after epoch #1902 is: train/0.027984645529623216 --- test/1.4474127903047802\n",
      "Loss after epoch #1903 is: train/0.0279078462600415 --- test/1.4429943913943726\n",
      "Loss after epoch #1904 is: train/0.027977378874807453 --- test/1.4454749603030475\n",
      "Loss after epoch #1905 is: train/0.027985305534335123 --- test/1.4491578036177382\n",
      "Loss after epoch #1906 is: train/0.027864075766045476 --- test/1.446036493860165\n",
      "Loss after epoch #1907 is: train/0.02780227699241002 --- test/1.446189121044684\n",
      "Loss after epoch #1908 is: train/0.027789990013524443 --- test/1.439013473345108\n",
      "Loss after epoch #1909 is: train/0.027821931520687505 --- test/1.4378723528449073\n",
      "Loss after epoch #1910 is: train/0.028047848323055815 --- test/1.4352251884173914\n",
      "Loss after epoch #1911 is: train/0.028027193744342555 --- test/1.4333173816805462\n",
      "Loss after epoch #1912 is: train/0.0281396019970537 --- test/1.432755408892785\n",
      "Loss after epoch #1913 is: train/0.028133875585802312 --- test/1.431784648276399\n",
      "Loss after epoch #1914 is: train/0.028001875351535123 --- test/1.4326006739829864\n",
      "Loss after epoch #1915 is: train/0.02807719731927574 --- test/1.43140623592651\n",
      "Loss after epoch #1916 is: train/0.027979324101244783 --- test/1.43030226966489\n",
      "Loss after epoch #1917 is: train/0.027884986989050984 --- test/1.4309565529982982\n",
      "Loss after epoch #1918 is: train/0.027742985821755977 --- test/1.427893108393284\n",
      "Loss after epoch #1919 is: train/0.027712433165373283 --- test/1.4289141992564875\n",
      "Loss after epoch #1920 is: train/0.027633211213565118 --- test/1.430688641673957\n",
      "Loss after epoch #1921 is: train/0.02768628896663549 --- test/1.4309753471538054\n",
      "Loss after epoch #1922 is: train/0.027667293809648486 --- test/1.433669179890872\n",
      "Loss after epoch #1923 is: train/0.02772362368443229 --- test/1.4306014008452426\n",
      "Loss after epoch #1924 is: train/0.027733879960630536 --- test/1.4327166629662165\n",
      "Loss after epoch #1925 is: train/0.027750702205157365 --- test/1.4337353338802883\n",
      "Loss after epoch #1926 is: train/0.027913710493056125 --- test/1.4327430467694922\n",
      "Loss after epoch #1927 is: train/0.027826950581844954 --- test/1.434925603942467\n",
      "Loss after epoch #1928 is: train/0.02764734739533044 --- test/1.4361756733431121\n",
      "Loss after epoch #1929 is: train/0.02765430907842785 --- test/1.4400676145369926\n",
      "Loss after epoch #1930 is: train/0.027571882419555355 --- test/1.4394162990012456\n",
      "Loss after epoch #1931 is: train/0.027600728148456266 --- test/1.4344477846924184\n",
      "Loss after epoch #1932 is: train/0.027615435057063535 --- test/1.437495077189487\n",
      "Loss after epoch #1933 is: train/0.02772553780787753 --- test/1.4368493763260937\n",
      "Loss after epoch #1934 is: train/0.0274719132725768 --- test/1.43338583109869\n",
      "Loss after epoch #1935 is: train/0.027547325273688764 --- test/1.4304088675131836\n",
      "Loss after epoch #1936 is: train/0.027603073925766247 --- test/1.4307290310347578\n",
      "Loss after epoch #1937 is: train/0.02752830736799463 --- test/1.432114577311238\n",
      "Loss after epoch #1938 is: train/0.02763109665656083 --- test/1.4321458421576958\n",
      "Loss after epoch #1939 is: train/0.027726798116960664 --- test/1.432335142406158\n",
      "Loss after epoch #1940 is: train/0.027686271740009767 --- test/1.4308746126413676\n",
      "Loss after epoch #1941 is: train/0.027648208385210992 --- test/1.432038221413838\n",
      "Loss after epoch #1942 is: train/0.02766620050095711 --- test/1.434485241474193\n",
      "Loss after epoch #1943 is: train/0.027510744306246768 --- test/1.431138583940818\n",
      "Loss after epoch #1944 is: train/0.027440528520532588 --- test/1.4343941926903117\n",
      "Loss after epoch #1945 is: train/0.02752518870831426 --- test/1.43600650452283\n",
      "Loss after epoch #1946 is: train/0.027614187572764303 --- test/1.4347134500298586\n",
      "Loss after epoch #1947 is: train/0.027591906329290962 --- test/1.4345973053032235\n",
      "Loss after epoch #1948 is: train/0.02762754813032818 --- test/1.4322227327000518\n",
      "Loss after epoch #1949 is: train/0.027567836719088586 --- test/1.4328525575997808\n",
      "Loss after epoch #1950 is: train/0.027477687929863374 --- test/1.4330513673195693\n",
      "Loss after epoch #1951 is: train/0.027406152415551204 --- test/1.434616090879181\n",
      "Loss after epoch #1952 is: train/0.027358060373307444 --- test/1.4349969751971416\n",
      "Loss after epoch #1953 is: train/0.027477035505132325 --- test/1.4349389028307382\n",
      "Loss after epoch #1954 is: train/0.027367737993221303 --- test/1.434155567504866\n",
      "Loss after epoch #1955 is: train/0.02741910719217734 --- test/1.431759023605488\n",
      "Loss after epoch #1956 is: train/0.027552504715909774 --- test/1.4329182438310468\n",
      "Loss after epoch #1957 is: train/0.027563368275493872 --- test/1.4306425994552743\n",
      "Loss after epoch #1958 is: train/0.02751120293439466 --- test/1.4317065920333756\n",
      "Loss after epoch #1959 is: train/0.02746709003524655 --- test/1.432105028259794\n",
      "Loss after epoch #1960 is: train/0.027458224285273926 --- test/1.4279407762914733\n",
      "Loss after epoch #1961 is: train/0.027433106862792787 --- test/1.431162864423563\n",
      "Loss after epoch #1962 is: train/0.027360676935140413 --- test/1.429370197425519\n",
      "Loss after epoch #1963 is: train/0.027300103428970865 --- test/1.4280978815315646\n",
      "Loss after epoch #1964 is: train/0.02730050529821376 --- test/1.431540427028188\n",
      "Loss after epoch #1965 is: train/0.027138242440162094 --- test/1.428103383937163\n",
      "Loss after epoch #1966 is: train/0.027221403237044275 --- test/1.4306289532094905\n",
      "Loss after epoch #1967 is: train/0.02726905072864319 --- test/1.427243416718601\n",
      "Loss after epoch #1968 is: train/0.027296959748314455 --- test/1.4267843829228475\n",
      "Loss after epoch #1969 is: train/0.027245336053440126 --- test/1.4217966474458077\n",
      "Loss after epoch #1970 is: train/0.027217227243780662 --- test/1.4213825820055899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch #1971 is: train/0.027248715440389877 --- test/1.4213166974166669\n",
      "Loss after epoch #1972 is: train/0.02719175250151077 --- test/1.4237429749100177\n",
      "Loss after epoch #1973 is: train/0.027151061068423802 --- test/1.4204747151450146\n",
      "Loss after epoch #1974 is: train/0.027211640157400234 --- test/1.421996575082471\n",
      "Loss after epoch #1975 is: train/0.027269437410271904 --- test/1.4262609458189432\n",
      "Loss after epoch #1976 is: train/0.027323428690260068 --- test/1.4280188360077828\n",
      "Loss after epoch #1977 is: train/0.027265958303356614 --- test/1.425965476550472\n",
      "Loss after epoch #1978 is: train/0.027318910877636028 --- test/1.425029851380732\n",
      "Loss after epoch #1979 is: train/0.027387252399737914 --- test/1.4281945614583917\n",
      "Loss after epoch #1980 is: train/0.027254548794404053 --- test/1.4278113056851567\n",
      "Loss after epoch #1981 is: train/0.027367608677937628 --- test/1.4286617207647971\n",
      "Loss after epoch #1982 is: train/0.02740230274167325 --- test/1.4289845666153218\n",
      "Loss after epoch #1983 is: train/0.02738904594182069 --- test/1.430550820590389\n",
      "Loss after epoch #1984 is: train/0.0272550885438627 --- test/1.426603055968194\n",
      "Loss after epoch #1985 is: train/0.027233320196498045 --- test/1.4251898927286242\n",
      "Loss after epoch #1986 is: train/0.02723198464541777 --- test/1.4219688660258836\n",
      "Loss after epoch #1987 is: train/0.027204941622245255 --- test/1.4236237513906942\n",
      "Loss after epoch #1988 is: train/0.02717184050841106 --- test/1.423924069411963\n",
      "Loss after epoch #1989 is: train/0.027186330004069757 --- test/1.4280330678097555\n",
      "Loss after epoch #1990 is: train/0.027302320587739 --- test/1.428948594282859\n",
      "Loss after epoch #1991 is: train/0.027434001967798865 --- test/1.4280593565702713\n",
      "Loss after epoch #1992 is: train/0.027401833134417224 --- test/1.4284866180485634\n",
      "Loss after epoch #1993 is: train/0.02730240943921398 --- test/1.4212467991859654\n",
      "Loss after epoch #1994 is: train/0.02725150664890038 --- test/1.4209256472507334\n",
      "Loss after epoch #1995 is: train/0.027243964765644963 --- test/1.4243448253706497\n",
      "Loss after epoch #1996 is: train/0.027284094698873145 --- test/1.4288395846239754\n",
      "Loss after epoch #1997 is: train/0.027208457640288525 --- test/1.4283889498005298\n",
      "Loss after epoch #1998 is: train/0.027199095535302323 --- test/1.4273846447870826\n",
      "Loss after epoch #1999 is: train/0.027119624042219865 --- test/1.4230726402664988\n",
      "Loss after epoch #2000 is: train/0.02708571931267564 --- test/1.4251828818432115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ea2e492610>"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "row_features = reduced_features.to_numpy()\n",
    "sgd_model = initialize(n_rows, n_cols)\n",
    "# sgd_model = initialize(n_rows, n_cols, row_features)\n",
    "predictions, train_MSEs, test_MSEs = train_sgd(sgd_model, num_epochs=2000, batch_size=10000)\n",
    "\n",
    "plt.scatter(np.array(range(len(train_MSEs))), np.array(train_MSEs), s=2, c='b', marker='o')\n",
    "plt.scatter(np.array(range(len(test_MSEs))), np.array(test_MSEs), s=2, c='r', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1952"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(test_MSEs) #optimal number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4204747151450146"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_MSEs[np.argmin(test_MSEs)] #best test MSE achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1918366981868844"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(test_MSEs[np.argmin(test_MSEs)]) #average train error at the optimal number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1647757902982832"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(train_MSEs[np.argmin(test_MSEs)]) #best average test error achieved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
