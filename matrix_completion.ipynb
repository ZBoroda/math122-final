{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful packages\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data as a dataframe with pandas\n",
    "user_history = pd.read_csv(\"user_history.csv\")\n",
    "user_history_without_user_ID = user_history.drop(['USER ID'],axis=1)\n",
    "user_history_index = user_history.set_index('USER ID')\n",
    "user_ratings = pd.read_csv(\"user_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the divition\n",
    "train = pd.read_csv(\"train_rating.csv\")\n",
    "test = pd.read_csv(\"test_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER ID</th>\n",
       "      <th>alpine kimono</th>\n",
       "      <th>sweden kansas</th>\n",
       "      <th>student icon</th>\n",
       "      <th>supreme ivan</th>\n",
       "      <th>albert charlie</th>\n",
       "      <th>heavy trapeze</th>\n",
       "      <th>fabric tokyo</th>\n",
       "      <th>brother robin</th>\n",
       "      <th>tiger catalog</th>\n",
       "      <th>...</th>\n",
       "      <th>cigar lagoon</th>\n",
       "      <th>equal comedy</th>\n",
       "      <th>bombay podium</th>\n",
       "      <th>helena robot</th>\n",
       "      <th>prodigy rhino</th>\n",
       "      <th>jumbo gray</th>\n",
       "      <th>radius wizard</th>\n",
       "      <th>fame quiz</th>\n",
       "      <th>bazaar complex</th>\n",
       "      <th>glass slogan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100950</td>\n",
       "      <td>2.192897</td>\n",
       "      <td>0.361397</td>\n",
       "      <td>0.531663</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>0.694338</td>\n",
       "      <td>1.250662</td>\n",
       "      <td>1.483259</td>\n",
       "      <td>1.799682</td>\n",
       "      <td>0.664616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309236</td>\n",
       "      <td>2.622033</td>\n",
       "      <td>1.242243</td>\n",
       "      <td>2.947560</td>\n",
       "      <td>0.693319</td>\n",
       "      <td>0.625303</td>\n",
       "      <td>2.352295</td>\n",
       "      <td>1.548417</td>\n",
       "      <td>1.517709</td>\n",
       "      <td>1.508219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100956</td>\n",
       "      <td>2.776597</td>\n",
       "      <td>0.788821</td>\n",
       "      <td>1.187149</td>\n",
       "      <td>0.473049</td>\n",
       "      <td>2.245112</td>\n",
       "      <td>1.998881</td>\n",
       "      <td>0.072812</td>\n",
       "      <td>1.441970</td>\n",
       "      <td>2.264368</td>\n",
       "      <td>...</td>\n",
       "      <td>2.304845</td>\n",
       "      <td>1.498307</td>\n",
       "      <td>0.319484</td>\n",
       "      <td>0.089212</td>\n",
       "      <td>3.157167</td>\n",
       "      <td>2.789594</td>\n",
       "      <td>1.003377</td>\n",
       "      <td>1.141516</td>\n",
       "      <td>2.011509</td>\n",
       "      <td>0.377898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100962</td>\n",
       "      <td>0.281717</td>\n",
       "      <td>5.046727</td>\n",
       "      <td>4.407484</td>\n",
       "      <td>2.138591</td>\n",
       "      <td>1.075562</td>\n",
       "      <td>0.385842</td>\n",
       "      <td>0.626482</td>\n",
       "      <td>0.026648</td>\n",
       "      <td>1.949374</td>\n",
       "      <td>...</td>\n",
       "      <td>1.463952</td>\n",
       "      <td>0.601814</td>\n",
       "      <td>1.983130</td>\n",
       "      <td>2.364877</td>\n",
       "      <td>0.429133</td>\n",
       "      <td>2.758070</td>\n",
       "      <td>0.563619</td>\n",
       "      <td>0.271453</td>\n",
       "      <td>0.579626</td>\n",
       "      <td>1.785609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100969</td>\n",
       "      <td>0.943147</td>\n",
       "      <td>1.165713</td>\n",
       "      <td>2.016138</td>\n",
       "      <td>1.236626</td>\n",
       "      <td>0.973435</td>\n",
       "      <td>2.514205</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>1.091282</td>\n",
       "      <td>1.320748</td>\n",
       "      <td>...</td>\n",
       "      <td>1.428416</td>\n",
       "      <td>3.791742</td>\n",
       "      <td>1.102070</td>\n",
       "      <td>3.250911</td>\n",
       "      <td>1.209403</td>\n",
       "      <td>0.246261</td>\n",
       "      <td>0.558631</td>\n",
       "      <td>1.163652</td>\n",
       "      <td>1.922758</td>\n",
       "      <td>1.008040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100974</td>\n",
       "      <td>0.485729</td>\n",
       "      <td>4.633607</td>\n",
       "      <td>4.120416</td>\n",
       "      <td>1.497073</td>\n",
       "      <td>1.463875</td>\n",
       "      <td>0.867737</td>\n",
       "      <td>0.877514</td>\n",
       "      <td>0.019603</td>\n",
       "      <td>1.751483</td>\n",
       "      <td>...</td>\n",
       "      <td>1.179954</td>\n",
       "      <td>1.031741</td>\n",
       "      <td>1.935182</td>\n",
       "      <td>2.012611</td>\n",
       "      <td>0.553173</td>\n",
       "      <td>2.614605</td>\n",
       "      <td>0.313479</td>\n",
       "      <td>0.143246</td>\n",
       "      <td>0.914407</td>\n",
       "      <td>2.011048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>125614</td>\n",
       "      <td>0.200730</td>\n",
       "      <td>3.819307</td>\n",
       "      <td>3.793920</td>\n",
       "      <td>1.352770</td>\n",
       "      <td>1.616516</td>\n",
       "      <td>0.138227</td>\n",
       "      <td>0.193954</td>\n",
       "      <td>0.770098</td>\n",
       "      <td>1.724992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985254</td>\n",
       "      <td>0.508837</td>\n",
       "      <td>2.033624</td>\n",
       "      <td>2.783638</td>\n",
       "      <td>0.360851</td>\n",
       "      <td>1.787872</td>\n",
       "      <td>0.749357</td>\n",
       "      <td>0.205445</td>\n",
       "      <td>0.945106</td>\n",
       "      <td>1.155107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>125622</td>\n",
       "      <td>1.139876</td>\n",
       "      <td>0.361305</td>\n",
       "      <td>1.420300</td>\n",
       "      <td>1.024918</td>\n",
       "      <td>2.313955</td>\n",
       "      <td>1.638803</td>\n",
       "      <td>1.570336</td>\n",
       "      <td>0.233047</td>\n",
       "      <td>1.521743</td>\n",
       "      <td>...</td>\n",
       "      <td>3.606324</td>\n",
       "      <td>1.670899</td>\n",
       "      <td>1.127929</td>\n",
       "      <td>1.888831</td>\n",
       "      <td>4.198021</td>\n",
       "      <td>2.934536</td>\n",
       "      <td>0.501603</td>\n",
       "      <td>0.923039</td>\n",
       "      <td>0.737211</td>\n",
       "      <td>0.675006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>125629</td>\n",
       "      <td>2.423679</td>\n",
       "      <td>0.555591</td>\n",
       "      <td>1.244828</td>\n",
       "      <td>0.100391</td>\n",
       "      <td>1.098254</td>\n",
       "      <td>2.027846</td>\n",
       "      <td>1.485342</td>\n",
       "      <td>0.912006</td>\n",
       "      <td>0.304603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050602</td>\n",
       "      <td>2.439248</td>\n",
       "      <td>1.352124</td>\n",
       "      <td>2.926793</td>\n",
       "      <td>0.254287</td>\n",
       "      <td>0.488679</td>\n",
       "      <td>2.181696</td>\n",
       "      <td>1.367859</td>\n",
       "      <td>1.410247</td>\n",
       "      <td>0.549376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>125636</td>\n",
       "      <td>0.096291</td>\n",
       "      <td>3.028528</td>\n",
       "      <td>2.554836</td>\n",
       "      <td>0.404308</td>\n",
       "      <td>1.401119</td>\n",
       "      <td>0.760813</td>\n",
       "      <td>0.356988</td>\n",
       "      <td>1.873021</td>\n",
       "      <td>1.977173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609813</td>\n",
       "      <td>0.242120</td>\n",
       "      <td>1.061562</td>\n",
       "      <td>2.616126</td>\n",
       "      <td>0.091176</td>\n",
       "      <td>1.162114</td>\n",
       "      <td>1.218459</td>\n",
       "      <td>1.067140</td>\n",
       "      <td>0.441260</td>\n",
       "      <td>0.666655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>125645</td>\n",
       "      <td>0.595734</td>\n",
       "      <td>2.072896</td>\n",
       "      <td>1.997112</td>\n",
       "      <td>0.553371</td>\n",
       "      <td>1.674531</td>\n",
       "      <td>0.381437</td>\n",
       "      <td>0.149846</td>\n",
       "      <td>2.993711</td>\n",
       "      <td>1.455024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.825080</td>\n",
       "      <td>0.370440</td>\n",
       "      <td>0.773802</td>\n",
       "      <td>3.314752</td>\n",
       "      <td>0.146642</td>\n",
       "      <td>1.096581</td>\n",
       "      <td>2.257115</td>\n",
       "      <td>1.034738</td>\n",
       "      <td>1.001442</td>\n",
       "      <td>0.267665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4500 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      USER ID  alpine kimono  sweden kansas  student icon  supreme ivan  \\\n",
       "0      100950       2.192897       0.361397      0.531663      0.016105   \n",
       "1      100956       2.776597       0.788821      1.187149      0.473049   \n",
       "2      100962       0.281717       5.046727      4.407484      2.138591   \n",
       "3      100969       0.943147       1.165713      2.016138      1.236626   \n",
       "4      100974       0.485729       4.633607      4.120416      1.497073   \n",
       "...       ...            ...            ...           ...           ...   \n",
       "4495   125614       0.200730       3.819307      3.793920      1.352770   \n",
       "4496   125622       1.139876       0.361305      1.420300      1.024918   \n",
       "4497   125629       2.423679       0.555591      1.244828      0.100391   \n",
       "4498   125636       0.096291       3.028528      2.554836      0.404308   \n",
       "4499   125645       0.595734       2.072896      1.997112      0.553371   \n",
       "\n",
       "      albert charlie  heavy trapeze  fabric tokyo  brother robin  \\\n",
       "0           0.694338       1.250662      1.483259       1.799682   \n",
       "1           2.245112       1.998881      0.072812       1.441970   \n",
       "2           1.075562       0.385842      0.626482       0.026648   \n",
       "3           0.973435       2.514205      0.022476       1.091282   \n",
       "4           1.463875       0.867737      0.877514       0.019603   \n",
       "...              ...            ...           ...            ...   \n",
       "4495        1.616516       0.138227      0.193954       0.770098   \n",
       "4496        2.313955       1.638803      1.570336       0.233047   \n",
       "4497        1.098254       2.027846      1.485342       0.912006   \n",
       "4498        1.401119       0.760813      0.356988       1.873021   \n",
       "4499        1.674531       0.381437      0.149846       2.993711   \n",
       "\n",
       "      tiger catalog  ...  cigar lagoon  equal comedy  bombay podium  \\\n",
       "0          0.664616  ...      0.309236      2.622033       1.242243   \n",
       "1          2.264368  ...      2.304845      1.498307       0.319484   \n",
       "2          1.949374  ...      1.463952      0.601814       1.983130   \n",
       "3          1.320748  ...      1.428416      3.791742       1.102070   \n",
       "4          1.751483  ...      1.179954      1.031741       1.935182   \n",
       "...             ...  ...           ...           ...            ...   \n",
       "4495       1.724992  ...      0.985254      0.508837       2.033624   \n",
       "4496       1.521743  ...      3.606324      1.670899       1.127929   \n",
       "4497       0.304603  ...      0.050602      2.439248       1.352124   \n",
       "4498       1.977173  ...      0.609813      0.242120       1.061562   \n",
       "4499       1.455024  ...      0.825080      0.370440       0.773802   \n",
       "\n",
       "      helena robot  prodigy rhino  jumbo gray  radius wizard  fame quiz  \\\n",
       "0         2.947560       0.693319    0.625303       2.352295   1.548417   \n",
       "1         0.089212       3.157167    2.789594       1.003377   1.141516   \n",
       "2         2.364877       0.429133    2.758070       0.563619   0.271453   \n",
       "3         3.250911       1.209403    0.246261       0.558631   1.163652   \n",
       "4         2.012611       0.553173    2.614605       0.313479   0.143246   \n",
       "...            ...            ...         ...            ...        ...   \n",
       "4495      2.783638       0.360851    1.787872       0.749357   0.205445   \n",
       "4496      1.888831       4.198021    2.934536       0.501603   0.923039   \n",
       "4497      2.926793       0.254287    0.488679       2.181696   1.367859   \n",
       "4498      2.616126       0.091176    1.162114       1.218459   1.067140   \n",
       "4499      3.314752       0.146642    1.096581       2.257115   1.034738   \n",
       "\n",
       "      bazaar complex  glass slogan  \n",
       "0           1.517709      1.508219  \n",
       "1           2.011509      0.377898  \n",
       "2           0.579626      1.785609  \n",
       "3           1.922758      1.008040  \n",
       "4           0.914407      2.011048  \n",
       "...              ...           ...  \n",
       "4495        0.945106      1.155107  \n",
       "4496        0.737211      0.675006  \n",
       "4497        1.410247      0.549376  \n",
       "4498        0.441260      0.666655  \n",
       "4499        1.001442      0.267665  \n",
       "\n",
       "[4500 rows x 101 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpine kimono</th>\n",
       "      <th>sweden kansas</th>\n",
       "      <th>student icon</th>\n",
       "      <th>supreme ivan</th>\n",
       "      <th>albert charlie</th>\n",
       "      <th>heavy trapeze</th>\n",
       "      <th>fabric tokyo</th>\n",
       "      <th>brother robin</th>\n",
       "      <th>tiger catalog</th>\n",
       "      <th>explain alex</th>\n",
       "      <th>...</th>\n",
       "      <th>cigar lagoon</th>\n",
       "      <th>equal comedy</th>\n",
       "      <th>bombay podium</th>\n",
       "      <th>helena robot</th>\n",
       "      <th>prodigy rhino</th>\n",
       "      <th>jumbo gray</th>\n",
       "      <th>radius wizard</th>\n",
       "      <th>fame quiz</th>\n",
       "      <th>bazaar complex</th>\n",
       "      <th>glass slogan</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USER ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100950</th>\n",
       "      <td>2.192897</td>\n",
       "      <td>0.361397</td>\n",
       "      <td>0.531663</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>0.694338</td>\n",
       "      <td>1.250662</td>\n",
       "      <td>1.483259</td>\n",
       "      <td>1.799682</td>\n",
       "      <td>0.664616</td>\n",
       "      <td>0.165158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309236</td>\n",
       "      <td>2.622033</td>\n",
       "      <td>1.242243</td>\n",
       "      <td>2.947560</td>\n",
       "      <td>0.693319</td>\n",
       "      <td>0.625303</td>\n",
       "      <td>2.352295</td>\n",
       "      <td>1.548417</td>\n",
       "      <td>1.517709</td>\n",
       "      <td>1.508219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100956</th>\n",
       "      <td>2.776597</td>\n",
       "      <td>0.788821</td>\n",
       "      <td>1.187149</td>\n",
       "      <td>0.473049</td>\n",
       "      <td>2.245112</td>\n",
       "      <td>1.998881</td>\n",
       "      <td>0.072812</td>\n",
       "      <td>1.441970</td>\n",
       "      <td>2.264368</td>\n",
       "      <td>2.518723</td>\n",
       "      <td>...</td>\n",
       "      <td>2.304845</td>\n",
       "      <td>1.498307</td>\n",
       "      <td>0.319484</td>\n",
       "      <td>0.089212</td>\n",
       "      <td>3.157167</td>\n",
       "      <td>2.789594</td>\n",
       "      <td>1.003377</td>\n",
       "      <td>1.141516</td>\n",
       "      <td>2.011509</td>\n",
       "      <td>0.377898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100962</th>\n",
       "      <td>0.281717</td>\n",
       "      <td>5.046727</td>\n",
       "      <td>4.407484</td>\n",
       "      <td>2.138591</td>\n",
       "      <td>1.075562</td>\n",
       "      <td>0.385842</td>\n",
       "      <td>0.626482</td>\n",
       "      <td>0.026648</td>\n",
       "      <td>1.949374</td>\n",
       "      <td>0.062942</td>\n",
       "      <td>...</td>\n",
       "      <td>1.463952</td>\n",
       "      <td>0.601814</td>\n",
       "      <td>1.983130</td>\n",
       "      <td>2.364877</td>\n",
       "      <td>0.429133</td>\n",
       "      <td>2.758070</td>\n",
       "      <td>0.563619</td>\n",
       "      <td>0.271453</td>\n",
       "      <td>0.579626</td>\n",
       "      <td>1.785609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100969</th>\n",
       "      <td>0.943147</td>\n",
       "      <td>1.165713</td>\n",
       "      <td>2.016138</td>\n",
       "      <td>1.236626</td>\n",
       "      <td>0.973435</td>\n",
       "      <td>2.514205</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>1.091282</td>\n",
       "      <td>1.320748</td>\n",
       "      <td>0.525940</td>\n",
       "      <td>...</td>\n",
       "      <td>1.428416</td>\n",
       "      <td>3.791742</td>\n",
       "      <td>1.102070</td>\n",
       "      <td>3.250911</td>\n",
       "      <td>1.209403</td>\n",
       "      <td>0.246261</td>\n",
       "      <td>0.558631</td>\n",
       "      <td>1.163652</td>\n",
       "      <td>1.922758</td>\n",
       "      <td>1.008040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100974</th>\n",
       "      <td>0.485729</td>\n",
       "      <td>4.633607</td>\n",
       "      <td>4.120416</td>\n",
       "      <td>1.497073</td>\n",
       "      <td>1.463875</td>\n",
       "      <td>0.867737</td>\n",
       "      <td>0.877514</td>\n",
       "      <td>0.019603</td>\n",
       "      <td>1.751483</td>\n",
       "      <td>0.297013</td>\n",
       "      <td>...</td>\n",
       "      <td>1.179954</td>\n",
       "      <td>1.031741</td>\n",
       "      <td>1.935182</td>\n",
       "      <td>2.012611</td>\n",
       "      <td>0.553173</td>\n",
       "      <td>2.614605</td>\n",
       "      <td>0.313479</td>\n",
       "      <td>0.143246</td>\n",
       "      <td>0.914407</td>\n",
       "      <td>2.011048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125614</th>\n",
       "      <td>0.200730</td>\n",
       "      <td>3.819307</td>\n",
       "      <td>3.793920</td>\n",
       "      <td>1.352770</td>\n",
       "      <td>1.616516</td>\n",
       "      <td>0.138227</td>\n",
       "      <td>0.193954</td>\n",
       "      <td>0.770098</td>\n",
       "      <td>1.724992</td>\n",
       "      <td>0.550699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985254</td>\n",
       "      <td>0.508837</td>\n",
       "      <td>2.033624</td>\n",
       "      <td>2.783638</td>\n",
       "      <td>0.360851</td>\n",
       "      <td>1.787872</td>\n",
       "      <td>0.749357</td>\n",
       "      <td>0.205445</td>\n",
       "      <td>0.945106</td>\n",
       "      <td>1.155107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125622</th>\n",
       "      <td>1.139876</td>\n",
       "      <td>0.361305</td>\n",
       "      <td>1.420300</td>\n",
       "      <td>1.024918</td>\n",
       "      <td>2.313955</td>\n",
       "      <td>1.638803</td>\n",
       "      <td>1.570336</td>\n",
       "      <td>0.233047</td>\n",
       "      <td>1.521743</td>\n",
       "      <td>0.799919</td>\n",
       "      <td>...</td>\n",
       "      <td>3.606324</td>\n",
       "      <td>1.670899</td>\n",
       "      <td>1.127929</td>\n",
       "      <td>1.888831</td>\n",
       "      <td>4.198021</td>\n",
       "      <td>2.934536</td>\n",
       "      <td>0.501603</td>\n",
       "      <td>0.923039</td>\n",
       "      <td>0.737211</td>\n",
       "      <td>0.675006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125629</th>\n",
       "      <td>2.423679</td>\n",
       "      <td>0.555591</td>\n",
       "      <td>1.244828</td>\n",
       "      <td>0.100391</td>\n",
       "      <td>1.098254</td>\n",
       "      <td>2.027846</td>\n",
       "      <td>1.485342</td>\n",
       "      <td>0.912006</td>\n",
       "      <td>0.304603</td>\n",
       "      <td>0.219560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050602</td>\n",
       "      <td>2.439248</td>\n",
       "      <td>1.352124</td>\n",
       "      <td>2.926793</td>\n",
       "      <td>0.254287</td>\n",
       "      <td>0.488679</td>\n",
       "      <td>2.181696</td>\n",
       "      <td>1.367859</td>\n",
       "      <td>1.410247</td>\n",
       "      <td>0.549376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125636</th>\n",
       "      <td>0.096291</td>\n",
       "      <td>3.028528</td>\n",
       "      <td>2.554836</td>\n",
       "      <td>0.404308</td>\n",
       "      <td>1.401119</td>\n",
       "      <td>0.760813</td>\n",
       "      <td>0.356988</td>\n",
       "      <td>1.873021</td>\n",
       "      <td>1.977173</td>\n",
       "      <td>0.865512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609813</td>\n",
       "      <td>0.242120</td>\n",
       "      <td>1.061562</td>\n",
       "      <td>2.616126</td>\n",
       "      <td>0.091176</td>\n",
       "      <td>1.162114</td>\n",
       "      <td>1.218459</td>\n",
       "      <td>1.067140</td>\n",
       "      <td>0.441260</td>\n",
       "      <td>0.666655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125645</th>\n",
       "      <td>0.595734</td>\n",
       "      <td>2.072896</td>\n",
       "      <td>1.997112</td>\n",
       "      <td>0.553371</td>\n",
       "      <td>1.674531</td>\n",
       "      <td>0.381437</td>\n",
       "      <td>0.149846</td>\n",
       "      <td>2.993711</td>\n",
       "      <td>1.455024</td>\n",
       "      <td>2.076679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.825080</td>\n",
       "      <td>0.370440</td>\n",
       "      <td>0.773802</td>\n",
       "      <td>3.314752</td>\n",
       "      <td>0.146642</td>\n",
       "      <td>1.096581</td>\n",
       "      <td>2.257115</td>\n",
       "      <td>1.034738</td>\n",
       "      <td>1.001442</td>\n",
       "      <td>0.267665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4500 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         alpine kimono  sweden kansas  student icon  supreme ivan  \\\n",
       "USER ID                                                             \n",
       "100950        2.192897       0.361397      0.531663      0.016105   \n",
       "100956        2.776597       0.788821      1.187149      0.473049   \n",
       "100962        0.281717       5.046727      4.407484      2.138591   \n",
       "100969        0.943147       1.165713      2.016138      1.236626   \n",
       "100974        0.485729       4.633607      4.120416      1.497073   \n",
       "...                ...            ...           ...           ...   \n",
       "125614        0.200730       3.819307      3.793920      1.352770   \n",
       "125622        1.139876       0.361305      1.420300      1.024918   \n",
       "125629        2.423679       0.555591      1.244828      0.100391   \n",
       "125636        0.096291       3.028528      2.554836      0.404308   \n",
       "125645        0.595734       2.072896      1.997112      0.553371   \n",
       "\n",
       "         albert charlie  heavy trapeze  fabric tokyo  brother robin  \\\n",
       "USER ID                                                               \n",
       "100950         0.694338       1.250662      1.483259       1.799682   \n",
       "100956         2.245112       1.998881      0.072812       1.441970   \n",
       "100962         1.075562       0.385842      0.626482       0.026648   \n",
       "100969         0.973435       2.514205      0.022476       1.091282   \n",
       "100974         1.463875       0.867737      0.877514       0.019603   \n",
       "...                 ...            ...           ...            ...   \n",
       "125614         1.616516       0.138227      0.193954       0.770098   \n",
       "125622         2.313955       1.638803      1.570336       0.233047   \n",
       "125629         1.098254       2.027846      1.485342       0.912006   \n",
       "125636         1.401119       0.760813      0.356988       1.873021   \n",
       "125645         1.674531       0.381437      0.149846       2.993711   \n",
       "\n",
       "         tiger catalog  explain alex  ...  cigar lagoon  equal comedy  \\\n",
       "USER ID                               ...                               \n",
       "100950        0.664616      0.165158  ...      0.309236      2.622033   \n",
       "100956        2.264368      2.518723  ...      2.304845      1.498307   \n",
       "100962        1.949374      0.062942  ...      1.463952      0.601814   \n",
       "100969        1.320748      0.525940  ...      1.428416      3.791742   \n",
       "100974        1.751483      0.297013  ...      1.179954      1.031741   \n",
       "...                ...           ...  ...           ...           ...   \n",
       "125614        1.724992      0.550699  ...      0.985254      0.508837   \n",
       "125622        1.521743      0.799919  ...      3.606324      1.670899   \n",
       "125629        0.304603      0.219560  ...      0.050602      2.439248   \n",
       "125636        1.977173      0.865512  ...      0.609813      0.242120   \n",
       "125645        1.455024      2.076679  ...      0.825080      0.370440   \n",
       "\n",
       "         bombay podium  helena robot  prodigy rhino  jumbo gray  \\\n",
       "USER ID                                                           \n",
       "100950        1.242243      2.947560       0.693319    0.625303   \n",
       "100956        0.319484      0.089212       3.157167    2.789594   \n",
       "100962        1.983130      2.364877       0.429133    2.758070   \n",
       "100969        1.102070      3.250911       1.209403    0.246261   \n",
       "100974        1.935182      2.012611       0.553173    2.614605   \n",
       "...                ...           ...            ...         ...   \n",
       "125614        2.033624      2.783638       0.360851    1.787872   \n",
       "125622        1.127929      1.888831       4.198021    2.934536   \n",
       "125629        1.352124      2.926793       0.254287    0.488679   \n",
       "125636        1.061562      2.616126       0.091176    1.162114   \n",
       "125645        0.773802      3.314752       0.146642    1.096581   \n",
       "\n",
       "         radius wizard  fame quiz  bazaar complex  glass slogan  \n",
       "USER ID                                                          \n",
       "100950        2.352295   1.548417        1.517709      1.508219  \n",
       "100956        1.003377   1.141516        2.011509      0.377898  \n",
       "100962        0.563619   0.271453        0.579626      1.785609  \n",
       "100969        0.558631   1.163652        1.922758      1.008040  \n",
       "100974        0.313479   0.143246        0.914407      2.011048  \n",
       "...                ...        ...             ...           ...  \n",
       "125614        0.749357   0.205445        0.945106      1.155107  \n",
       "125622        0.501603   0.923039        0.737211      0.675006  \n",
       "125629        2.181696   1.367859        1.410247      0.549376  \n",
       "125636        1.218459   1.067140        0.441260      0.666655  \n",
       "125645        2.257115   1.034738        1.001442      0.267665  \n",
       "\n",
       "[4500 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_history_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.9971665788086455', '2.701128327489425']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "def matrixComplete(train, test, user_history = pd.DataFrame({'A' : []})):\n",
    "    # user_history \n",
    "    # user_history need to be PCA before enter the method\n",
    "    train_ratings = train['RATING']\n",
    "    train_users = train['USER ID']\n",
    "    train_movies = train['PRODUCT']\n",
    "    \n",
    "    test_ratings = test['RATING']\n",
    "    test_users = test['USER ID']\n",
    "    test_movies = test['PRODUCT']\n",
    "    \n",
    "    m = len(train_ratings) # the size of the training set\n",
    "    n_users = train_users.unique().shape[0] # the largest index, plus 1\n",
    "    n_movies = train_movies.unique().shape[0]\n",
    "    \n",
    "    movie_index = pd.unique(train_movies)\n",
    "    user_index = pd.unique(train_users).sort\n",
    "    # user_history as rows\n",
    "    sgd_model = initialize(n_users, n_movies, movie_index, user_index, user_history)\n",
    "    # user_history appended\n",
    "    #new = user_history.melt('USER ID', var_name='PRODUCT', value_name='RATING')\n",
    "    #train_ratings = train_ratings.concat(new['RATING'], axis = 1)\n",
    "    #train_users = train_users.concat(new['USER ID'], axis = 1)\n",
    "    #train_movies = train_movies.concat(new['PRODUCT'], axis = 1)\n",
    "    #sgd_model = initialize(n_users, n_movies)\n",
    "    \n",
    "    # epoch is flexible\n",
    "    epoch = 10\n",
    "    return train_sgd(sgd_model, epoch, m, train_users, train_movies, train_ratings, \n",
    "                     test_users, test_movies, test_ratings, movie_index, user_index, n_movies)\n",
    "\n",
    "def initialize(n_users, n_movies, movie_index, user_index, rows = pd.DataFrame({'A' : []})):\n",
    "    \"\"\"Initalize a random model, and normalize it so that it has sensible mean and variance\"\"\"\n",
    "    # (The normalization helps make sure we start out at a reasonable parameter scale, which speeds up training)\n",
    "    if (rows.empty):\n",
    "        # k is flexible\n",
    "        k = 10\n",
    "        rows = pd.DataFrame(data = np.random.normal(size=(n_users, k)), index = user_index)\n",
    "    else:\n",
    "        k = rows.shape[1]\n",
    "    \n",
    "    columns = pd.DataFrame(data = np.random.normal(size=(n_movies, k)), index = movie_index)\n",
    "    raw_predictions = predict((rows, columns))\n",
    "    \n",
    "    s = np.sqrt(2*raw_predictions.std()) # We want to start out with roughly unit variance\n",
    "    b = np.sqrt((3.5 - raw_predictions.mean()/s)/k) #We want to start out with average rating 3.5\n",
    "    rows /= s\n",
    "    rows += b\n",
    "    columns /= s\n",
    "    columns += b\n",
    "    \n",
    "    return (rows, columns)\n",
    "\n",
    "def predict(model):\n",
    "    \"\"\"The model's predictions for all user/movie pairs\"\"\"\n",
    "    rows, columns = model\n",
    "    re_rows = rows.to_numpy()\n",
    "    re_columns = columns.to_numpy()\n",
    "    return re_rows @ re_columns.T\n",
    "\n",
    "def single_example_step(model, user, movie, rating):\n",
    "    \"\"\"Update the model using the gradient at a single training example\"\"\"\n",
    "    rows, columns = model\n",
    "    residual = np.dot(rows.loc[user], columns.loc[movie]) - rating\n",
    "    grad_users = 2 * residual * columns.loc[movie] # the gradient for the rows matrix\n",
    "    grad_movies = 2 * residual * rows.loc[user] # the gradient for the columns matrix\n",
    "    rows.loc[user] -= learning_rate*grad_users\n",
    "    columns.loc[movie] -= learning_rate*grad_movies\n",
    "\n",
    "def train_sgd(model, epochs, m, train_users, train_movies, train_ratings,\n",
    "             test_users, test_movies, test_ratings, movie_index, user_index, n_movies):\n",
    "    \"\"\"Train the model for a number of epochs via SGD (batch size=1)\"\"\"\n",
    "    rows, columns = model\n",
    "    # It's good practice to shuffle your data before doing batch gradient descent,\n",
    "    # so that each mini-batch peforms like a random sample from the dataset\n",
    "    shuffle = np.random.permutation(m) \n",
    "    shuffled_users = train_users[shuffle]\n",
    "    shuffled_movies = train_movies[shuffle]\n",
    "    shuffled_ratings = train_ratings[shuffle]\n",
    "    for epoch in range(epochs):\n",
    "        for user, movie, rating in zip(shuffled_users, shuffled_movies, shuffled_ratings):\n",
    "            # update the model using the gradient at a single example\n",
    "            single_example_step(model, user, movie, rating)\n",
    "        # after each Epoch, we'll evaluate our model\n",
    "        predicted = predict(model)\n",
    "        print(predicted)\n",
    "        t_pred = pd.DataFrame(predicted.transpose(), index=movie_index).transpose()\n",
    "        t_pred['USER ID'] = pd.DataFrame(user_index)\n",
    "        \n",
    "        print(unindex_pred)\n",
    "        melt_pred = unindex_pred.melt('USER ID', var_name='PRODUCT', value_name='p_ratings')\n",
    "        merge_pred_train = train_ratings.to_frame().merge(melt_pred, on = ['USER ID', 'PRODUCT'])\n",
    "        merge_pred_test = test_ratings.to_frame().merge(melt_pred, on = ['USER ID', 'PRODUCT'])\n",
    "        \n",
    "        train_loss = np.mean((merge_pred_train['RATING'] - merge_pred_train['p_ratings'])**2)\n",
    "        test_loss = np.mean((merge_pred_test['RATING'] - merge_pred_test['p_ratings'])**2)\n",
    "        print(\"Loss after epoch #{} is: train/{} --- test/{}\".format(epoch+1, train_loss, test_loss))\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-0ef43cd57ee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomplete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrixComplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_history_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-6f29d5fe80f3>\u001b[0m in \u001b[0;36mmatrixComplete\u001b[0;34m(train, test, user_history)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# epoch is flexible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     return train_sgd(sgd_model, epoch, m, train_users, train_movies, train_ratings, \n\u001b[0m\u001b[1;32m     32\u001b[0m                      test_users, test_movies, test_ratings, movie_index, user_index, n_movies)\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-6f29d5fe80f3>\u001b[0m in \u001b[0;36mtrain_sgd\u001b[0;34m(model, epochs, m, train_users, train_movies, train_ratings, test_users, test_movies, test_ratings, movie_index, user_index, n_movies)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mt_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmovie_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mt_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USER ID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munindex_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataFrame constructor not properly called!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "complete = matrixComplete(train, test, user_history_index)\n",
    "complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
